{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (1000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.3.1生成数据集\n",
    "from mxnet import autograd,nd\n",
    "\n",
    "num_inputs= 2\n",
    "num_examples= 1000\n",
    "true_w = [2,-3.4]\n",
    "true_b = 4.2\n",
    "features = nd.random.normal(scale=1,shape=(num_examples,num_inputs))\n",
    "#features[:,0]截取所有行，0列的元素，获得一个一维行向量,即1x1000\n",
    "labels = true_w[0] *features[:,0]+true_w[1] * features[:,1]+true_b \n",
    "labels +=nd.random.normal(scale=0.01,shape=labels.shape)\n",
    "features.shape,labels.shape# features为1000x2,labels为1x1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3.2 读取数据 \n",
    "from mxnet.gluon import data as gdata\n",
    "\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = gdata.ArrayDataset(features,labels)\n",
    "# 随机读取batc_size大小的数据样本，shuffle代表随机,data_iter为一个迭代器\n",
    "data_iter =gdata.DataLoader(dataset,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.7265826   1.1108662 ]\n",
      " [ 0.70998806  0.03483093]\n",
      " [-1.2080863   1.8140212 ]\n",
      " [-0.19679789 -0.13781768]\n",
      " [-0.3216947  -0.83062977]\n",
      " [ 0.277957    1.1219468 ]\n",
      " [ 0.80678344 -0.5540094 ]\n",
      " [ 0.7201801   0.9385755 ]\n",
      " [ 0.23789974  0.41007295]\n",
      " [-0.0446528   1.1711494 ]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[-1.0402609   5.4816027  -4.3862033   4.2766104   6.375893    0.9473662\n",
      "  7.703524    2.4503956   3.2905803   0.12103113]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for X,y in data_iter:\n",
    "    print(X,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential0_ (\n",
      "\n",
      ")\n",
      "sequential0_ (\n",
      "  Parameter dense0_weight (shape=(1, 0), dtype=float32)\n",
      "  Parameter dense0_bias (shape=(1,), dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3.3.3 定义模型\n",
    "# nn是netural network的缩写，nn模块中定义了大量神经网路的层\n",
    "from mxnet.gluon import nn\n",
    "# 模型变量net，它是⼀个Sequential实例。在Gluon中，Sequential实例可以看作是⼀个串联各个层的容器。\n",
    "# 在构造模型时，我们在该容器中依次添加层。\n",
    "net = nn.Sequential()\n",
    "# 线性回归的输出层⼜叫全连接层。\n",
    "# 在Gluon中，全连接层是⼀个Dense实例。我们定义该层输出个数为1。 \n",
    "# add函数向模型变量net中添加全连接层\n",
    "net.add(nn.Dense(1))\n",
    "# 在Gluon中我们⽆须指定每⼀层输⼊的形状，例如线性回归的输⼊个数。\n",
    "# 当模型得到数据时，例如后⾯执⾏net(X)时，模型将⾃动推断出每⼀层的输⼊个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3.4 初始化模型参数 \n",
    "# init是initializer的缩写形式,init模块提供了模型参数初始化的各种⽅法\n",
    "from mxnet import init\n",
    "# init.Normal(sigma=0.01)指定权重参数w每个元素将在初始化时随机采样于均值为0、标准差为0.01的正态分布。\n",
    "# 偏差参数b默认会初始化为零。 net中有模型参数w，b\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3.5 定义损失函数 \n",
    "# 在Gluon中，loss模块定义了各种损失函数。\n",
    "from mxnet.gluon import loss as gloss\n",
    "loss =gloss.L2Loss() # 平方损失又称L2范数损失 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3.6 定义优化算法\n",
    "# 在导⼊Gluon后，我们创建⼀个Trainer实例，并指定学习率为0.03的小批量随机梯度下降（sgd）为优化算法。\n",
    "from mxnet import gluon\n",
    "# net.collect_params()可以获取net实例所有层的全部参数，sgd算法将迭代这些参数,学习率为0.03\n",
    "trainer =gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':0.03})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,loss 0.035142\n",
      "epoch 2,loss 0.000126\n",
      "epoch 3,loss 0.000048\n"
     ]
    }
   ],
   "source": [
    "# 3.3.7训练模型\n",
    "num_epochs =3 \n",
    "for epoch in range(1,num_epochs+1):\n",
    "    for X,y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X),y)# 小批量样本的loss，net(X)为模型预测值，y为真实值\n",
    "        l.backward()# 对l求模型参数w,b的梯度\n",
    "        # 在step函数中指明批量⼤小，从而对批量中样本梯度求平均\n",
    "        trainer.step(batch_size)# 模型参数用sgd算法进行迭代\n",
    "    l=loss(net(features),labels)# 所有样本的loss\n",
    "    print('epoch %d,loss %f' %(epoch,l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4],\n",
       " \n",
       " [[ 1.9996562 -3.3999028]]\n",
       " <NDArray 1x2 @cpu(0)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = net[0]\n",
    "true_w,dense.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2,\n",
       " \n",
       " [4.200024]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b,dense.bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[0.02962869 0.02212788]]\n",
       " <NDArray 1x2 @cpu(0)>,\n",
       " \n",
       " [0.0259296]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.weight.grad(),dense.bias.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Parameter in module mxnet.gluon.parameter object:\n",
      "\n",
      "class Parameter(builtins.object)\n",
      " |  A Container holding parameters (weights) of Blocks.\n",
      " |  \n",
      " |  :py:class:`Parameter` holds a copy of the parameter on each :py:class:`Context` after\n",
      " |  it is initialized with ``Parameter.initialize(...)``. If :py:attr:`grad_req` is\n",
      " |  not ``'null'``, it will also hold a gradient array on each :py:class:`Context`::\n",
      " |  \n",
      " |      ctx = mx.gpu(0)\n",
      " |      x = mx.nd.zeros((16, 100), ctx=ctx)\n",
      " |      w = mx.gluon.Parameter('fc_weight', shape=(64, 100), init=mx.init.Xavier())\n",
      " |      b = mx.gluon.Parameter('fc_bias', shape=(64,), init=mx.init.Zero())\n",
      " |      w.initialize(ctx=ctx)\n",
      " |      b.initialize(ctx=ctx)\n",
      " |      out = mx.nd.FullyConnected(x, w.data(ctx), b.data(ctx), num_hidden=64)\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  name : str\n",
      " |      Name of this parameter.\n",
      " |  grad_req : {'write', 'add', 'null'}, default 'write'\n",
      " |      Specifies how to update gradient to grad arrays.\n",
      " |  \n",
      " |      - ``'write'`` means everytime gradient is written to grad :py:class:`NDArray`.\n",
      " |      - ``'add'`` means everytime gradient is added to the grad :py:class:`NDArray`. You need\n",
      " |        to manually call ``zero_grad()`` to clear the gradient buffer before each\n",
      " |        iteration when using this option.\n",
      " |      - 'null' means gradient is not requested for this parameter. gradient arrays\n",
      " |        will not be allocated.\n",
      " |  shape : int or tuple of int, default None\n",
      " |      Shape of this parameter. By default shape is not specified. Parameter with\n",
      " |      unknown shape can be used for :py:class:`Symbol` API, but ``init`` will throw an error\n",
      " |      when using :py:class:`NDArray` API.\n",
      " |  dtype : numpy.dtype or str, default 'float32'\n",
      " |      Data type of this parameter. For example, ``numpy.float32`` or ``'float32'``.\n",
      " |  lr_mult : float, default 1.0\n",
      " |      Learning rate multiplier. Learning rate will be multiplied by lr_mult\n",
      " |      when updating this parameter with optimizer.\n",
      " |  wd_mult : float, default 1.0\n",
      " |      Weight decay multiplier (L2 regularizer coefficient). Works similar to lr_mult.\n",
      " |  init : Initializer, default None\n",
      " |      Initializer of this parameter. Will use the global initializer by default.\n",
      " |  stype: {'default', 'row_sparse', 'csr'}, defaults to 'default'.\n",
      " |      The storage type of the parameter.\n",
      " |  grad_stype: {'default', 'row_sparse', 'csr'}, defaults to 'default'.\n",
      " |      The storage type of the parameter's gradient.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  grad_req : {'write', 'add', 'null'}\n",
      " |      This can be set before or after initialization. Setting ``grad_req`` to ``'null'``\n",
      " |      with ``x.grad_req = 'null'`` saves memory and computation when you don't\n",
      " |      need gradient w.r.t x.\n",
      " |  lr_mult : float\n",
      " |      Local learning rate multiplier for this Parameter. The actual learning rate\n",
      " |      is calculated with ``learning_rate * lr_mult``. You can set it with\n",
      " |      ``param.lr_mult = 2.0``\n",
      " |  wd_mult : float\n",
      " |      Local weight decay multiplier for this Parameter.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, name, grad_req='write', shape=None, dtype=<class 'numpy.float32'>, lr_mult=1.0, wd_mult=1.0, init=None, allow_deferred_init=False, differentiable=True, stype='default', grad_stype='default')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  cast(self, dtype)\n",
      " |      Cast data and gradient of this Parameter to a new data type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or numpy.dtype\n",
      " |          The new data type.\n",
      " |  \n",
      " |  data(self, ctx=None)\n",
      " |      Returns a copy of this parameter on one context. Must have been\n",
      " |      initialized on this context before. For sparse parameters, use\n",
      " |      :py:meth:`Parameter.row_sparse_data` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context\n",
      " |          Desired context.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      NDArray on ctx\n",
      " |  \n",
      " |  grad(self, ctx=None)\n",
      " |      Returns a gradient buffer for this parameter on one context.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context\n",
      " |          Desired context.\n",
      " |  \n",
      " |  initialize(self, init=None, ctx=None, default_init=<mxnet.initializer.Uniform object at 0x0000021CD9C2C080>, force_reinit=False)\n",
      " |      Initializes parameter and gradient arrays. Only used for :py:class:`NDArray` API.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      init : Initializer\n",
      " |          The initializer to use. Overrides :py:meth:`Parameter.init` and default_init.\n",
      " |      ctx : Context or list of Context, defaults to :py:meth:`context.current_context()`.\n",
      " |          Initialize Parameter on given context. If ctx is a list of Context, a\n",
      " |          copy will be made for each context.\n",
      " |      \n",
      " |          .. note::\n",
      " |              Copies are independent arrays. User is responsible for keeping\n",
      " |              their values consistent when updating.\n",
      " |              Normally :py:class:`gluon.Trainer` does this for you.\n",
      " |      \n",
      " |      default_init : Initializer\n",
      " |          Default initializer is used when both :py:func:`init`\n",
      " |          and :py:meth:`Parameter.init` are ``None``.\n",
      " |      force_reinit : bool, default False\n",
      " |          Whether to force re-initialization if parameter is already initialized.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> weight = mx.gluon.Parameter('weight', shape=(2, 2))\n",
      " |      >>> weight.initialize(ctx=mx.cpu(0))\n",
      " |      >>> weight.data()\n",
      " |      [[-0.01068833  0.01729892]\n",
      " |       [ 0.02042518 -0.01618656]]\n",
      " |      <NDArray 2x2 @cpu(0)>\n",
      " |      >>> weight.grad()\n",
      " |      [[ 0.  0.]\n",
      " |       [ 0.  0.]]\n",
      " |      <NDArray 2x2 @cpu(0)>\n",
      " |      >>> weight.initialize(ctx=[mx.gpu(0), mx.gpu(1)])\n",
      " |      >>> weight.data(mx.gpu(0))\n",
      " |      [[-0.00873779 -0.02834515]\n",
      " |       [ 0.05484822 -0.06206018]]\n",
      " |      <NDArray 2x2 @gpu(0)>\n",
      " |      >>> weight.data(mx.gpu(1))\n",
      " |      [[-0.00873779 -0.02834515]\n",
      " |       [ 0.05484822 -0.06206018]]\n",
      " |      <NDArray 2x2 @gpu(1)>\n",
      " |  \n",
      " |  list_ctx(self)\n",
      " |      Returns a list of contexts this parameter is initialized on.\n",
      " |  \n",
      " |  list_data(self)\n",
      " |      Returns copies of this parameter on all contexts, in the same order\n",
      " |      as creation. For sparse parameters, use :py:meth:`Parameter.list_row_sparse_data`\n",
      " |      instead.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of NDArrays\n",
      " |  \n",
      " |  list_grad(self)\n",
      " |      Returns gradient buffers on all contexts, in the same order\n",
      " |      as :py:meth:`values`.\n",
      " |  \n",
      " |  list_row_sparse_data(self, row_id)\n",
      " |      Returns copies of the 'row_sparse' parameter on all contexts, in the same order\n",
      " |      as creation. The copy only retains rows whose ids occur in provided row ids.\n",
      " |      The parameter must have been initialized before.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      row_id: NDArray\n",
      " |          Row ids to retain for the 'row_sparse' parameter.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of NDArrays\n",
      " |  \n",
      " |  reset_ctx(self, ctx)\n",
      " |      Re-assign Parameter to other contexts.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context or list of Context, default ``context.current_context()``.\n",
      " |          Assign Parameter to given context. If ctx is a list of Context, a\n",
      " |          copy will be made for each context.\n",
      " |  \n",
      " |  row_sparse_data(self, row_id)\n",
      " |      Returns a copy of the 'row_sparse' parameter on the same context as row_id's.\n",
      " |      The copy only retains rows whose ids occur in provided row ids.\n",
      " |      The parameter must have been initialized on this context before.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      row_id: NDArray\n",
      " |          Row ids to retain for the 'row_sparse' parameter.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      NDArray on row_id's context\n",
      " |  \n",
      " |  set_data(self, data)\n",
      " |      Sets this parameter's value on all contexts.\n",
      " |  \n",
      " |  var(self)\n",
      " |      Returns a symbol representing this parameter.\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradient buffer on all contexts to 0. No action is taken if\n",
      " |      parameter is uninitialized or doesn't require gradient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  dtype\n",
      " |      The type of the parameter.\n",
      " |      \n",
      " |      Setting the dtype value is equivalent to casting the value of the parameter\n",
      " |  \n",
      " |  grad_req\n",
      " |  \n",
      " |  shape\n",
      " |      The shape of the parameter.\n",
      " |      \n",
      " |      By default, an unknown dimension size is 0. However, when the NumPy semantic\n",
      " |      is turned on, unknown dimension size is -1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dense.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
