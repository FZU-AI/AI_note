# 第二章 预备知识#
**2.1环境搭建参考：**
[https://discuss.gluon.ai/t/topic/13576](https://discuss.gluon.ai/t/topic/13576)

**2.2数据操作：**

在MXNet中，NDArray是一个类，也是存储和变换数据的主要工具。
向量和矩阵都是特殊的张量。

**1.创建**

(1) x=nd.arange(n)：创建一个长度为n的一维(行向量)数组x，即[0,1,2,...,n-1]，即形状为1xn。

arange(start,end,step)：创建一个范围从start到end（不包括end）的一维数组，步长为step，step默认1。

arange()被创建在CPU使用的内存上，返回了一个NDArray实例。

(2) x.shape：获取数组x的形状（如二维数组(矩阵)的形状为（n，m），n为行数，m为列数）。 

(3) x.size：获取数组x的元素总数。

(4) x.reshape((n，m))：将数组x的形状改为（n，m），n为行数，m为列数）,除了形状改变之外，x中的元素保持不变。

x.reshape((3, 4))也可写成x.reshape((-1, 4))或x.reshape((3, -1))。由于x的元素个数是已知的，这里的-1是能够通过元素个数和其他维度的大小推断出来的。 

(5) nd.zeros((2, 3, 4))：创建一个各元素为0，形状为(2,3,4)的张量（即多维数组）。

(6) nd.ones((3, 4))：创建各元素为1，形状为(3,4)的张量。 

(7) nd.array([[2,3], [1, 2]])：通过list创建张量，并初始化。

(8) nd.random.normal(loc=0,scale=1, shape=(3, 4)) ：创建一个形状为（3，4）的张量，每个元素都随机采样于均值（loc）为0、标准差（sacle）为1的正态分布(normal)。 loc的默认值为0，scale的默认值为1。

(9) X.zeros_like() == nd.zeros_like(X)：创建和X形状相同且元素为0的NDArray

  X.ones_like() == nd.ones_like(X)：创建和X形状相同且元素为1的NDArray


**2.运算**

(1) +-*/：按元素对张量进行计算(对形状相同的NDArray进行计算，形状不同的NDArray可能会触发广播机制)

+-*/运算符全名函数分别为elemwise_add()、elemwise_sub()、elemwise_mul()、elemwise_div()，使用运算符全名函数可以避免为X + Y开了临时内存来存储计算结果

X+Y == nd.elemwise_add(X,Y)

(2) X.exp() == nd.exp(X)：按元素对张量做指数运算

(3) nd.dot(X, Y.T)：将X与Y的转置(Y.T)做矩阵乘法。X为n行m列，Y为m行n列。注意与X*Y的区别。

(4) nd.concat(X, Y, dim=0)：按行连结两个矩阵

  nd.concat(X, Y, dim=1)：按列连结两个矩阵,默认dim=1

    x = [[1,1],[2,2]]
	y = [[3,3],[4,4],[5,5]]
	z = [[6,6],[7,7],[8,8]]
	concat(x,y,z,dim=0) = [[ 1.,  1.],
                           [ 2.,  2.],
                           [ 3.,  3.],
                           [ 4.,  4.],
                           [ 5.,  5.],
                           [ 6.,  6.],
                           [ 7.,  7.],
                           [ 8.,  8.]]
	concat(y,z,dim=1) = [[ 3.,  3.,  6.,  6.],
                         [ 4.,  4.,  7.,  7.],
                         [ 5.,  5.,  8.,  8.]]
(5) X==Y：按元素对张量进行判断是否相等，如果X和Y在相同位置的条件判断为真（值相等），那么新的NDArray在相同位置的值为1；反之为0。

(6) X.sum()：对X中的所有元素求和得到只有⼀个元素的NDArray。 

(7) X.norm()：获得张量X的L2范数(只有⼀个元素的NDArray)，即所有元素的平方和再开根号。
（Lp范数为所有元素的p次方的和再开p次根号）。

范数（Norm）是一个函数，其赋予某个向量空间（或矩阵）中的每个向量以长度或大小。

L-0范数：用来统计向量中非零元素的个数。

L-1范数：向量中所有元素的绝对值之和。可用于优化中去除没有取值的信息，
又称稀疏规则算子。

L-2范数：典型应用——欧式距离。可用于优化正则化项，避免过拟合。

L-∞范数：计算向量中的最大值。

(8) X.asscalar()：将单元素的NDArray变成成标量

(9) X.exp()、X.sum()、X.norm()、X.zeros_like()等可以分别改写为nd.exp(X)、nd.sum(X)、nd. norm(X)、nd.zeros_like(X)等。

(10) nd.power(x,n)：对x中的所有元素做n次方的运算 

**3.广播机制**

对两个形状不同的NDArray按元素运算时，可能会触发⼴播（broadcasting）机制：先适当复制元素使这两个NDArray形状相同后再按元素运算。 

**4.索引**

(1) X[1:3,:3] ：截取1，2行和0，1，2列的所有元素（依据左闭右开的原则）

(2) X[:,2]：截取所有行，2列的元素，获得一个一维行向量

(3) X[1:3,:]=12 ：截取1，2行的所有元素，并重新赋值为12

(4) X[1, 2] = 9 ：获取第一1行第2列的元素，并重新赋值为9

**5.运算的内存开销**

以上操作都会新开内存来存储运算结果。即使像X = X + Y这样的运算，我们也会新开内存来存储X+Y的结果，然后将X指向新内存。

id(X)：获得张量X的内存地址

我们可以通过索引将指定结果存储到特定内存，如Z[:] = X + Y ，我们把X + Y的结果通过[:]写进Z对应的内存中。 

如果X的值在之后的程序中不会复用，我们也可以⽤X[:] = X + Y或者X += Y来减少运算的内存开销(X+Y的结果存储在X的内存上，不会新开内存)。 

但是我们还是为X + Y开了临时内存来存储计算结果，再复制到对应的内存。为避免临时内存的开销，我们可以使用运算符全名函数中的out参数， 如nd.elemwise_add(X, Y, out=Z) 

**6.NDArray和NumPy相互变换**

D=nd.array(P) ：将NumPy实例P变换成NDArray实例D

D.asnumpy() ：将NDArray实例D变换成NumPy实例。 

**练习**

1.运行本节中的代码。将本节中条件判断式X == Y改为X < Y或X > Y，看看能够得到什么样的NDArray。

	逐个元素进行判断，若判断为真，那么新的NDArray在相同位置的值为1；反之为0。

2.将广播机制中按元素运算的两个NDArray替换成其他形状，结果是否和预期一样？ 



**2.3自动求梯度**

**1.梯度：**
函数f:R<sup>n</sup>(n维的实数向量集合)→R(实数集合)的输⼊是⼀个n维列向量x = [x1,x2,...,xn]<sup>⊤</sup>，输出是标量。如y=2x<sup>⊤</sup>x，y为标量。

函数f(x)有关x的梯度是一个由n个偏导数组成的列向量：

 ∇<sub>x</sub>f(x) =[∂f(x)/∂x<sub>1</sub>, ∂f(x)/∂x<sub>2</sub> ,..., ∂f(x)/∂x<sub>n</sub> ]<sup>⊤</sup>

**2.使用 MXNet 自动求梯度的步骤：**
    
1.导入autograd库

    from mxnet import autograd,nd
    # x为n维列向量
    x = nd.arange(4).reshape((4,1))

2.调用`attach_grad()`函数来申请存储梯度所需要的内存,标记需要求梯度的参数x(即要对x求梯度)

    x.attach_grad()

3.调用autograd.record()记录求梯度的计算，为了减少计算和内存开销，默认情况下 MXNet 不会记录（可选）。调用record(),默认情况下autograd还会将运行模式从预测模式转为训练模式。

	# x的形状为（4,1）, y是一个标量
	# 定义前向传播的函数,保存函数便于计算梯度
	with autograd.record():
		y = 2 * nd.dot(x.T,x)

4.调用backward()函数自动求梯度,对y求关于x的梯度。如果y不是一个标量，MXNet将默认先对y中元素求和得到新的变量(标量)，再求该变量有关x的梯度。即当y不是标量时等价于y.sum().backward()。

	# 调用反向传播
	y.backward()

	#  x的梯度,函数y = 2(x^⊤)x关于x的梯度应为4x
	x.grad
    ----
    output:
    [[ 0.]
     [ 4.]
     [ 8.]
     [12.]]
    <NDArray 4x1 @cpu(0)>

**3.训练模式和预测模式**

MXNet的运行模式包括训练模式和预测模式。调用record函数后，MXNet会记录并计算梯度。此外，默认情况下autograd还会将运行模式从预测模式转为训练模式。可以通过调用autograd.is_training()函数来判断是否处于训练模式。 