{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7.1 VGG块\n",
    "import d2lzh as d2l\n",
    "from mxnet import gluon,init,nd\n",
    "from mxnet.gluon import nn\n",
    "# VGG块连续使用数个相同的填充为1、窗口形状为 3×3 的卷积层后接上一个步幅为2、窗口形状为 2×2 的最大池化层。\n",
    "def vgg_block(num_convs,num_channels):\n",
    "    #num_convs为卷积层的数量 num_channels为输出通道数\n",
    "    blk=nn.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        # 卷积层保持输⼊的⾼和宽不变(n-3+2+1)\n",
    "        blk.add(nn.Conv2D(num_channels,kernel_size=3,padding=1,activation='relu'))\n",
    "    # 池化层则对高和宽减半(n-2+2)/2\n",
    "    blk.add(nn.MaxPool2D(pool_size=2,strides=2))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7.2 VGG⽹络 \n",
    "# 与AlexNet和LeNet⼀样，VGG⽹络由卷积层模块后接全连接层模块构成。\n",
    "# 卷积层模块串联数个vgg_block，其超参数由变量conv_arch定义。该变量指定了每个VGG块⾥卷积层个数和输出通道数。\n",
    "conv_arch=((1,64),(1,128),(2,256),(2,512),(2,512))\n",
    "# VGG-11\n",
    "def vgg(conv_arch):\n",
    "    net=nn.Sequential()\n",
    "    # 卷积层部分\n",
    "    for (num_convs,num_channels) in conv_arch:\n",
    "        net.add(vgg_block(num_convs,num_channels))\n",
    "    # 全连接层部分\n",
    "    net.add(nn.Dense(4096,activation='relu'),nn.Dropout(0.5),\n",
    "           nn.Dense(4096,activation='relu'),nn.Dropout(0.5),\n",
    "           nn.Dense(10))\n",
    "    return net\n",
    "\n",
    "net=vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential44 output shape:\t (1, 64, 112, 112)\n",
      "sequential45 output shape:\t (1, 128, 56, 56)\n",
      "sequential46 output shape:\t (1, 256, 28, 28)\n",
      "sequential47 output shape:\t (1, 512, 14, 14)\n",
      "sequential48 output shape:\t (1, 512, 7, 7)\n",
      "dense21 output shape:\t (1, 4096)\n",
      "dropout14 output shape:\t (1, 4096)\n",
      "dense22 output shape:\t (1, 4096)\n",
      "dropout15 output shape:\t (1, 4096)\n",
      "dense23 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "net.initialize()\n",
    "X=nd.random.uniform(shape=(1,1,224,224))\n",
    "for blk in net:\n",
    "    X=blk(X)\n",
    "    print(blk.name,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7.3 获取数据和训练模型 \n",
    "ratio=4\n",
    "small_conv_arch=[(pair[0],pair[1]//ratio) for pair in conv_arch]\n",
    "net=vgg(small_conv_arch)# 构造⼀个通道数更小的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n"
     ]
    }
   ],
   "source": [
    "# 与AlexNet相比使用了稍大些的学习率，模型训练过程与AlexNet类似。\n",
    "import mxnet as mx\n",
    "lr,num_epochs,batch_size,ctx=0.05,5,128,mx.cpu()\n",
    "net.initialize(ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size,resize=224)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.train_ch5??\n",
    "d2l.load_data_fashion_mnist??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
