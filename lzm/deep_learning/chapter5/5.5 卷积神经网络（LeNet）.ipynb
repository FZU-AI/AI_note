{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5.1 LeNet模型\n",
    "import d2lzh as d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd,gluon,init,nd\n",
    "from mxnet.gluon import loss as gloss,nn\n",
    "import time\n",
    "\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Conv2D(channels=6,kernel_size=5,activation='sigmoid'),# 卷积层\n",
    "       nn.MaxPool2D(pool_size=2,strides=2),# 最大池化层\n",
    "       nn.Conv2D(channels=6,kernel_size=5,activation='sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2,strides=2),\n",
    "       # Dense会默认将(批量⼤⼩, 通道, ⾼, 宽)形状的输⼊转换成(批量⼤⼩, 通道*⾼*宽)形状的输入\n",
    "       nn.Dense(120,activation='sigmoid'),# 全连接层\n",
    "       nn.Dense(84,activation='sigmoid'),\n",
    "       nn.Dense(10)# 输出层\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 output shape:\t (1, 6, 24, 24)\n",
      "pool0 output shape:\t (1, 6, 12, 12)\n",
      "conv1 output shape:\t (1, 6, 8, 8)\n",
      "pool1 output shape:\t (1, 6, 4, 4)\n",
      "dense0 output shape:\t (1, 120)\n",
      "dense1 output shape:\t (1, 84)\n",
      "dense2 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X=nd.random.uniform(shape=(1,1,28,28))\n",
    "net.initialize()\n",
    "# 逐层进⾏前向计算来查看每个层的输出形状。\n",
    "# 卷积层由于使⽤⾼和宽均为5的卷积核，从而将⾼和宽分别减小4，而池化层则将⾼和宽减半，但通道数则从1增加到16。\n",
    "# 全连接层则逐层减少输出个数，直到变成图像的类别数10。 \n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.name,'output shape:\\t',X.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5.2 获取数据和训练模型 \n",
    "batch_size=256\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试在gpu(0)上 创建NDArray，如果成功则使⽤gpu(0)，否则仍然使⽤CPU。 \n",
    "# 本函数已保存在d2lzh包中⽅便以后使⽤ \n",
    "def try_gpu():\n",
    "    try:\n",
    "        ctx=mx.gpu()\n",
    "        _=nd.zeros((1,),ctx=ctx)# 尝试在gpu上创建NDArray\n",
    "    except mx.base.MXNetError:\n",
    "            ctx =mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh包中⽅便以后使⽤。\n",
    "def evaluate_accuracy(data_iter,net,ctx):\n",
    "    acc_sum,n=nd.array([0],ctx=ctx),0\n",
    "    for X,y in data_iter:\n",
    "        # 如果ctx代表GPU及相应的显存，将数据复制到显存上\n",
    "        # 如果ctx代表CPU及内存，则目标变量和源变量共享源变量的内存\n",
    "        X,y=X.as_in_context(ctx),y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum+=( net(X).argmax(axis=1)==y ).sum()# argmax(axis=1)返回每行最大元素的索引\n",
    "        n+=y.size\n",
    "    return acc_sum.asscalar()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh包中⽅便以后使⽤\n",
    "def train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs):\n",
    "    print('training on',ctx)\n",
    "    loss=gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range (num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X,y=X.as_in_context(ctx),y.as_in_context(ctx)# 将数据复制到相应的设备上\n",
    "            with autograd.record():\n",
    "                y_hat=net(X)\n",
    "                l=loss(y_hat,y).sum() #小批量样品的损失函数和\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y=y.astype('float32')\n",
    "            train_l_sum+=l.asscalar()# 所有样品的损失函数的和\n",
    "            train_acc_sum+=( y_hat.argmax(axis=1)==y ).sum().asscalar()# 所有样品预测准确的个数\n",
    "            n+=y.size# 样本数\n",
    "        test_acc=evaluate_accuracy(test_iter,net,ctx)\n",
    "        print('epoch %d,loss %.4f,train acc %.3f,test acc %.3f,time %.lf sec'\n",
    "              %(epoch+1,train_l_sum/n,train_acc_sum/n,test_acc,time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1,loss 2.2773,train acc 0.133,test acc 0.372,time 6 sec\n",
      "epoch 2,loss 1.1670,train acc 0.540,test acc 0.671,time 5 sec\n",
      "epoch 3,loss 0.8433,train acc 0.673,test acc 0.713,time 5 sec\n",
      "epoch 4,loss 0.7180,train acc 0.721,test acc 0.730,time 5 sec\n",
      "epoch 5,loss 0.6530,train acc 0.744,test acc 0.765,time 5 sec\n"
     ]
    }
   ],
   "source": [
    "ctx=mx.gpu()\n",
    "lr,num_epochs=0.9,5\n",
    "# 重新将模型参数初始化到设备变量ctx之上，并使⽤Xavier随机初始化。\n",
    "# 损失函数和训练算 法则依然使⽤交叉熵损失函数和小批量随机梯度下降。 \n",
    "net.initialize(force_reinit=True,ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.name??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MaxPool2D??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.evaluate_accuracy??\n",
    "autograd.record??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `gluon.Optimize` not found.\n"
     ]
    }
   ],
   "source": [
    "d2l.train_ch3??\n",
    "gluon.Trainer??\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
