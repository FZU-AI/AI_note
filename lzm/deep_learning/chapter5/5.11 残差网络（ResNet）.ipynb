{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.11.1 残差块\n",
    "# 残差块里首先有2个有相同输出通道数的 3×3 卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。\n",
    "# 然后我们将输入跳过这2个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而可以相加。\n",
    "# 如果想改变通道数，就需要引入一个额外的 1×1 卷积层来将输入变换成需要的形状后再做相加运算。\n",
    "# 残差块的实现如下。它可以设定输出通道数、是否使用额外的 1×1 卷积层来修改通道数以及卷积层的步幅。\n",
    "import d2lzh as d2l\n",
    "from mxnet import gluon,init,nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "# 本类已保存在d2lzh包中方便以后使用\n",
    "class Residual(nn.Block):\n",
    "    def __init__(self,num_channels,use_1x1conv=False,strides=1,**kwargs):\n",
    "        super(Residual,self).__init__(**kwargs)\n",
    "        self.conv1=nn.Conv2D(num_channels,kernel_size=3,padding=1,strides=strides)# 两个3x3卷积层\n",
    "        self.conv2=nn.Conv2D(num_channels,kernel_size=3,padding=1)\n",
    "        if use_1x1conv:#是否使用1x1卷积层来修改通道数以及卷积层的步幅。\n",
    "            self.conv3=nn.Conv2D(num_channels,kernel_size=1,strides=strides)\n",
    "        else:\n",
    "            self.conv3=None\n",
    "        self.bn1=nn.BatchNorm()# 两个批量归一化层\n",
    "        self.bn2=nn.BatchNorm()\n",
    "            \n",
    "    def forward(self,X):\n",
    "        Y=nd.relu(self.bn1(self.conv1(X)))# 加权运算1(卷积和批量归一化)和激活函数\n",
    "        Y=self.bn2(self.conv2(Y))# 加权运算2(卷积和批量归一化)，得到f(x)-x\n",
    "        if self.conv3:\n",
    "            X=self.conv3(X)\n",
    "        return nd.relu(Y+X)# Y+X即f(x),在应用激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 6, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输入和输出形状一致的情况。\n",
    "blk=Residual(3)\n",
    "blk.initialize()\n",
    "X=nd.random.uniform(shape=(4,3,6,6))\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6, 3, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在增加输出通道数的同时减半输出的高和宽。\n",
    "blk=Residual(6,use_1x1conv=True,strides=2)\n",
    "blk.initialize()\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.11.2 ResNet模型\n",
    "# ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的 7×7 卷积层后接步幅为2的 3×3 的最大池化层。\n",
    "# 不同之处在于ResNet每个卷积层后增加的批量归一化层。\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Conv2D(64,kernel_size=7,strides=2,padding=3),\n",
    "       nn.BatchNorm(),nn.Activation('relu'),# 批量归一化发生在卷积计算之后、应用激活函数之前。\n",
    "       nn.MaxPool2D(pool_size=3,strides=2,padding=1)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。\n",
    "# 第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。\n",
    "# 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\n",
    "# 下面我们来实现这个模块。注意，这里对第一个模块做了特别处理。\n",
    "def resnet_block(num_channels,num_residuals,first_block=False):\n",
    "    blk=nn.Sequential()\n",
    "    for i in range(num_residuals):# 残差块数\n",
    "        if i==0 and not first_block:# 除第一个残差模块的第一个残差块将上一个模块的通道数翻倍，并将高和宽减半。\n",
    "            blk.add(Residual(num_channels,use_1x1conv=True,strides=2))\n",
    "        else:\n",
    "            blk.add(Residual(num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个模块使用2个残差块。\n",
    "net.add(resnet_block(64,2,first_block=True),# 第一个残差模块不用减半\n",
    "       resnet_block(128,2),\n",
    "       resnet_block(256,2),\n",
    "       resnet_block(512,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后，与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。\n",
    "net.add(nn.GlobalAvgPool2D(),nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv5 output shape:\t (1, 64, 112, 112)\n",
      "batchnorm4 output shape:\t (1, 64, 112, 112)\n",
      "relu0 output shape:\t (1, 64, 112, 112)\n",
      "pool0 output shape:\t (1, 64, 56, 56)\n",
      "sequential1 output shape:\t (1, 64, 56, 56)\n",
      "sequential2 output shape:\t (1, 128, 28, 28)\n",
      "sequential3 output shape:\t (1, 256, 14, 14)\n",
      "sequential4 output shape:\t (1, 512, 7, 7)\n",
      "pool1 output shape:\t (1, 512, 1, 1)\n",
      "dense0 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# 输入形状在ResNet不同模块之间的变化。\n",
    "X=nd.random.uniform(shape=(1,1,224,224))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.name,'output shape:\\t',X.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
