{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.10.2 从零开始实现 \n",
    "# 通过NDArray来实现批量归一化层。\n",
    "import d2lzh as d2l\n",
    "from mxnet import autograd,gluon,init,nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "def batch_norm(X,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "    # 通过autograd来判断当前模式是训练模式还是预测模式\n",
    "    if not autograd.is_training():\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat=(X-moving_mean)/nd.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4)# 判断输入X的维数是否为二维或者四维\n",
    "        if len(X.shape)==2:#二维即(样本，特征)\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean=X.mean(axis=0)# axis=0对每一列的元素求平均\n",
    "            var=((X-mean)**2).mean(axis=0)\n",
    "        else:# 四维(样本，通道，高，宽)\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean=X.mean(axis=(0,2,3),keepdims=True)\n",
    "            var=((X-mean)**2).mean(axis=(0,2,3),keepdims=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat=(X-mean)/nd.sqrt(var+eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean=momentum*moving_mean+(1.0-momentum)*mean\n",
    "        moving_var=momentum*moving_var+(1.0-momentum)*var\n",
    "    Y=gamma*X_hat+beta # 拉伸和偏移\n",
    "    return Y,moving_mean,moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义一个BatchNorm层。它保存参与求梯度和迭代的拉伸参数gamma和偏移参数beta，\n",
    "# 同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。\n",
    "class BatchNorm(nn.Block):\n",
    "    def __init__(self,num_features,num_dims,**kwargs):\n",
    "        super(BatchNorm,self).__init__(**kwargs)\n",
    "        if num_dims==2:#全连接层\n",
    "            # (1,num_features)为gamma(权重)的形状，则输入X的形状为(样本数,1)\n",
    "            shape=(1,num_features)# num_features参数对于全连接层来说应为输出个数\n",
    "        else:#卷积层\n",
    "            shape=(1,num_features,1,1)# num_features参数对于卷积层来说则为输出通道数。\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma=self.params.get('gamma',shape=shape,init=init.One())# 相当于权重\n",
    "        self.beta=self.params.get('beta',shape=shape,init=init.Zero())# 相当于偏差\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean=nd.zeros(shape)\n",
    "        self.moving_var=nd.zeros(shape)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.context!=X.context:\n",
    "            self.moving_mean=self.moving_mean.copyto(X.context)\n",
    "            self.moving_var=self.moving_var.copyto(X.context)\n",
    "        # 保存更新过的moving_mean和moving_var\n",
    "        Y,self.moving_mean,self.moving_var=batch_norm(X,self.gamma.data(),self.beta.data(),\n",
    "                                    self.moving_mean,self.moving_var,eps=1e-5,momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用批量归一化层的LeNet\n",
    "# 在所有的卷积层或全连接层之后、激活层之前加入批量归一化层。\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Conv2D(6,kernel_size=5),\n",
    "       BatchNorm(6,num_dims=4),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2,strides=2),\n",
    "       nn.Conv2D(16,kernel_size=5),\n",
    "       BatchNorm(16,num_dims=4),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2,strides=2),\n",
    "       nn.Dense(120),\n",
    "       BatchNorm(120,num_dims=2),\n",
    "       nn.Activation('sigmoid'), \n",
    "       nn.Dense(84),\n",
    "       BatchNorm(84,num_dims=2),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.Dense(10) \n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 0.6344, train acc 0.774, test acc 0.828, time 62.5 sec\n",
      "epoch 2, loss 0.3898, train acc 0.858, test acc 0.824, time 61.4 sec\n",
      "epoch 3, loss 0.3478, train acc 0.875, test acc 0.871, time 61.1 sec\n",
      "epoch 4, loss 0.3200, train acc 0.886, test acc 0.873, time 60.8 sec\n",
      "epoch 5, loss 0.3002, train acc 0.892, test acc 0.860, time 61.7 sec\n"
     ]
    }
   ],
   "source": [
    "# 训练修改后的模型。\n",
    "import mxnet as mx\n",
    "lr,num_epochs,batch_size,ctx=1.0,5,256,mx.cpu()\n",
    "net.initialize(ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1.8226874 1.8819977 2.0167785 1.3154042 1.6759734 1.0088483]\n",
       " <NDArray 6 @cpu(0)>,\n",
       " \n",
       " [-1.9086581   0.5927755  -0.3861179  -1.1667049   0.28886405 -0.68728215]\n",
       " <NDArray 6 @cpu(0)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].gamma.data().reshape((-1,)),net[1].beta.data().reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.10.3 简洁实现 \n",
    "# Gluon中nn模块定义的BatchNorm类使用起来更加简单。\n",
    "# 它不需要指定自己定义的BatchNorm类中所需的num_features和num_dims参数值。\n",
    "# 在Gluon中，这些参数值都将通过延后初始化而自动获取。\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Conv2D(6,kernel_size=5),\n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2,strides=2),\n",
    "       nn.Conv2D(16,kernel_size=5),\n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2,strides=2),\n",
    "       nn.Dense(120),\n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation('sigmoid'), \n",
    "       nn.Dense(84),\n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.Dense(10)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 0.6309, train acc 0.779, test acc 0.809, time 25.5 sec\n",
      "epoch 2, loss 0.3890, train acc 0.858, test acc 0.861, time 25.0 sec\n",
      "epoch 3, loss 0.3464, train acc 0.874, test acc 0.841, time 25.7 sec\n",
      "epoch 4, loss 0.3215, train acc 0.882, test acc 0.880, time 25.4 sec\n",
      "epoch 5, loss 0.3042, train acc 0.889, test acc 0.872, time 25.4 sec\n"
     ]
    }
   ],
   "source": [
    "net.initialize(ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=nd.arange(24).reshape(2,2,2,2)\n",
    "assert len(X.shape) in (2,4)\n",
    "X.mean(axis=(0,2,3),keepdims=True)\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "\n",
    "net[0].weight\n",
    "x=nd.arange(10).reshape(5,1)# x\n",
    "w=nd.arange(10).reshape(1,-1)# w\n",
    "x*w,x,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reshape(2,1,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.mean??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init.Zero??\n",
    "init.One??\n",
    "nn.Activation??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
