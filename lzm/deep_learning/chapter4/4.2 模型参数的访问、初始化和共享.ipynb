{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init,nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Dense(256,activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()# 使⽤默认初始化⽅式,为[0.07,-0.07]的均匀分布(init.Uniform(scale=0.7))\n",
    "\n",
    "X=nd.random.uniform(shape=(2,20))\n",
    "Y=net(X) # 前向计算 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dense0_ (\n",
       "   Parameter dense0_weight (shape=(256, 20), dtype=float32)\n",
       "   Parameter dense0_bias (shape=(256,), dtype=float32)\n",
       " ),\n",
       " mxnet.gluon.parameter.ParameterDict)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.2.1 访问模型参数 \n",
    "# 可以通过Block类的params属性来访问该层包含的所有参数。\n",
    "net[0].params,type(net[0].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter dense0_weight (shape=(256, 20), dtype=float32),\n",
       " Parameter dense0_weight (shape=(256, 20), dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].params['dense0_weight'],net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.06700657 -0.00369488  0.0418822  ... -0.05517294 -0.01194733\n",
       "  -0.00369594]\n",
       " [-0.03296221 -0.04391347  0.03839272 ...  0.05636378  0.02545484\n",
       "  -0.007007  ]\n",
       " [-0.0196689   0.01582889 -0.00881553 ...  0.01509629 -0.01908049\n",
       "  -0.02449339]\n",
       " ...\n",
       " [ 0.00010955  0.0439323  -0.04911506 ...  0.06975312  0.0449558\n",
       "  -0.03283203]\n",
       " [ 0.04106557  0.05671307 -0.00066976 ...  0.06387014 -0.01292654\n",
       "   0.00974177]\n",
       " [ 0.00297424 -0.0281784  -0.06881659 ... -0.04047417  0.00457048\n",
       "   0.05696651]]\n",
       "<NDArray 256x20 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gluon⾥参数类型为Parameter类，它包含参数和梯度的数值，可以分别通过data函数和grad函数来访问。\n",
    "net[0].weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " ...\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]]\n",
       "<NDArray 256x20 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 权重梯度的形状和权重的形状⼀样。\n",
    "net[0].weight.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "<NDArray 10 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sequential0_ (\n",
       "   Parameter dense0_weight (shape=(256, 20), dtype=float32)\n",
       "   Parameter dense0_bias (shape=(256,), dtype=float32)\n",
       "   Parameter dense1_weight (shape=(10, 256), dtype=float32)\n",
       "   Parameter dense1_bias (shape=(10,), dtype=float32)\n",
       " ),\n",
       " mxnet.gluon.parameter.ParameterDict)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使⽤collect_params函数来获取net变量所有嵌套（例如通过add函数嵌套）的层所包含的所有参数。\n",
    "# 它返回的同样是⼀个由参数名称到参数实例的字典。 \n",
    "net.collect_params(),type(net.collect_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential0_ (\n",
       "  Parameter dense0_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense1_weight (shape=(10, 256), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect_params可以通过正则表达式来匹配参数名，从而筛选需要的参数。\n",
    "net.collect_params('.*weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.00195949 -0.0173764   0.00047347  0.00145809  0.00326049  0.00457878\n",
       " -0.00894258  0.00493839 -0.00904343 -0.01214079  0.02156406  0.01093822\n",
       "  0.01827143 -0.0104467   0.01006219  0.0051742  -0.00806932  0.01376901\n",
       "  0.00205885  0.00994352]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.2.2 初始化模型参数 \n",
    "# init.Normal(sigma=0.01)将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并依然将偏差参数清零。\n",
    "# ⾮⾸次对模型初始化需要指定force_reinit为真 ,代表强制重新初始化，默认False\n",
    "net.initialize(init=init.Normal(sigma=0.01),force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init.Constant使⽤常数来初始化权重参数。\n",
    "net.initialize(init=init.Constant(1),force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.00512482 -0.06579044 -0.10849719 -0.09586414  0.06394844  0.06029618\n",
       " -0.03065033 -0.01086642  0.01929168  0.1003869  -0.09339568 -0.08703034\n",
       " -0.10472868 -0.09879824 -0.00352201 -0.11063069 -0.04257748  0.06548801\n",
       "  0.12987629 -0.13846186]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net[0].weight为Parameter类\n",
    "# 只对某个特定参数进⾏初始化。调⽤Parameter类的initialize函数，它 与Block类提供的initialize函数的使⽤⽅法⼀致。\n",
    "net[0].weight.initialize(init=init.Xavier(),force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init dense0_weight (256, 20)\n",
      "Init dense1_weight (10, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[-5.3659673  7.5773945  8.986376  -0.         8.827555   0.\n",
       "  5.9840508 -0.         0.         0.         7.4857597 -0.\n",
       " -0.         6.8910007  6.9788704 -6.1131554  0.         5.4665203\n",
       " -9.735263   9.485172 ]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.2.3 ⾃定义初始化方法\n",
    "# init.Uniform、Normal、Xavier均为Initialize的子类\n",
    "class MyInit(init.Initializer):\n",
    "    def _init_weight(self,name,data):\n",
    "        print('Init',name,data.shape)\n",
    "        data[:]=nd.random.uniform(low=-10,high=10,shape=data.shape)\n",
    "        data*=data.abs()>=5\n",
    "        \n",
    "net.initialize(MyInit(),force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-4.3659673  8.5773945  9.986376   1.         9.827555   1.\n",
       "  6.9840508  1.         1.         1.         8.48576    1.\n",
       "  1.         7.8910007  7.9788704 -5.1131554  1.         6.4665203\n",
       " -8.735263  10.485172 ]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过Parameter类的set_data函数来直接改写模型参数。\n",
    "net[0].weight.set_data(net[0].weight.data()+1)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 8x20 @cpu(0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.2.4 共享模型参数 \n",
    "# 在构造第三隐藏层时通过params来指定它使⽤第⼆隐藏层的参数。因此模型的第二隐藏层（shared变量）和第三隐藏层共享模型参数。\n",
    "# 因为模型参数⾥包含了 梯度，所以在反向传播计算时，第⼆隐藏层和第三隐藏层的梯度都会被累加在shared.params.grad()⾥。 \n",
    "net=nn.Sequential()\n",
    "shared=nn.Dense(8,activation='relu')\n",
    "net.add(nn.Dense(8,activation='relu'),\n",
    "       shared,\n",
    "       nn.Dense(8,activation='relu',params=shared.params),\n",
    "       nn.Dense(10))\n",
    "net.initialize()\n",
    "X=nd.random.uniform(shape=(2,20))\n",
    "net(X)\n",
    "\n",
    "net[1].weight.data()[0] == net[2].weight.data()[0]\n",
    "net[0].weight.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter dense6_weight (shape=(1, 0), dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 练习1.尝试在net.initialize()后、net(X)前访问模型参数，观察模型参数的形状。\n",
    "# net.initialize()后、net(X)前访问模型参数发现模型参数的shape为(1,0)，\n",
    "# 可见权重w实际的初始化发生在第一个正向传播过程中(即net(X)后，net.initialize()并未初始化)\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Dense(1))\n",
    "net[0].weight\n",
    "net.initialize()\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "<NDArray 8x8 @cpu(0)>\n",
      "\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "<NDArray 8x8 @cpu(0)>\n",
      "\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "<NDArray 8x8 @cpu(0)>\n",
      "\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "<NDArray 8x8 @cpu(0)>\n",
      "\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "<NDArray 8x8 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "# 练习2.构造⼀个含共享参数层的多层感知机并训练。在训练过程中，观察每⼀层的模型参数和梯 度。 \n",
    "import d2lzh as d2l\n",
    "from mxnet import gluon,init,autograd\n",
    "from mxnet.gluon import loss as gloss,nn\n",
    "\n",
    "net=nn.Sequential()\n",
    "shared=nn.Dense(8,activation='relu')\n",
    "net.add(nn.Dense(8,activation='relu'),\n",
    "        shared,\n",
    "        nn.Dense(8,activation='relu',params=shared.params),\n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "batch_size,num_epochs=256,5\n",
    "loss=gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':0.5})\n",
    "for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            if trainer is None:\n",
    "                sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = d2l.evaluate_accuracy(test_iter, net)\n",
    "        print(net[1].weight.data()==net[2].weight.data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet\n",
    "\n",
    "\n",
    "mxnet.init??\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net.initialize??\n",
    "init.Constant??\n",
    "mxnet.gluon.parameter.Parameter.initialize??\n",
    "net.initialize??\n",
    "\n",
    "init.Xavier??\n",
    "init.Initializer??\n",
    "mxnet.gluon.parameter.Parameter??\n",
    "\n",
    "nn.HybridBlock??\n",
    "nn.Dense??\n",
    "gluon.Trainer??\n",
    "init.Uniform??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
