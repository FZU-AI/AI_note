{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.1 继承Block类来构造模型 \n",
    "# Block类是nn模块⾥提供的⼀个模型构造类，是所有神经网络层和模型的基类。我们可以继承它来定义我们想要的模型。\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "# 定义的MLP类重载了Block类的__init__函数和forward函数。\n",
    "# 它们分别⽤于创建模型参数和定义前向计算。前向计算也即正向传播。 \n",
    "class MLP(nn.Block):\n",
    "    # 声明带有模型参数的层，这⾥声明了两个全连接层 \n",
    "    def __init__(self,**kwargs):\n",
    "        # 调用MLP的父类Block的构造函数来进行必要的初始化(如_children)。这样在构造实例时还可以指定其他函数参数\n",
    "        super(MLP,self).__init__(**kwargs)\n",
    "        self.hidden=nn.Dense(256,activation='relu')# 隐藏层 \n",
    "        self.output=nn.Dense(10)# 输出层 \n",
    "    # 定义模型的前向计算，即如何根据输⼊x计算返回所需要的模型输出     \n",
    "    def forward(self,x):\n",
    "        return self.output(self.hidden(x))\n",
    "    # 以上的MLP类中⽆须定义反向传播函数。系统将通过⾃动求梯度而⾃动⽣成反向传播所需的backward函数。     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.09543004  0.04614332 -0.00286655 -0.07790346 -0.05130241  0.02942038\n",
       "   0.08696645 -0.0190793  -0.04122177  0.05088576]\n",
       " [ 0.0769287   0.03099706  0.00856576 -0.044672   -0.06926838  0.09132431\n",
       "   0.06786592 -0.06187843 -0.03436674  0.04234696]]\n",
       "<NDArray 2x10 @cpu(0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uniform默认为[0,1)的均匀分布\n",
    "X=nd.random.uniform(shape=(2,20))\n",
    "net=MLP()\n",
    "net.initialize()\n",
    "# net(X)会调⽤MLP继承⾃Block类的__call__函数，这个函数将调⽤MLP类定义的forward函数来完成前向计算。 \n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.2 Sequential类继承⾃Block类 \n",
    "# 实现⼀个与Sequential类有相同功能的MySequential类。\n",
    "class MySequential(nn.Block):\n",
    "    # 调用MySequential的父类Block的构造函数来进行必要的初始化。\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MySequential,self).__init__(**kwargs)\n",
    "        \n",
    "    def add(self,block):\n",
    "        # block是⼀个Block⼦类实例，假设它有⼀个独⼀⽆⼆的名字。我们将它保存在Block类的 \n",
    "        # 成员变量_children⾥，其类型是OrderedDict(字典)。当MySequential实例调⽤ \n",
    "        # initialize函数时，系统会⾃动对_children⾥所有成员初始化\n",
    "        self._children[block.name]=block\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # OrderedDict保证会按照成员添加时的顺序遍历成员 \n",
    "        # values()返回OrderedDict的值(即层)\n",
    "        for block in self._children.values():\n",
    "            x=block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00362229  0.00633331  0.03201145 -0.01369375  0.10336448 -0.0350802\n",
       "  -0.00032165 -0.01676024  0.06978628  0.01303309]\n",
       " [ 0.03871717  0.02608212  0.03544958 -0.02521311  0.11005436 -0.01430663\n",
       "  -0.03052467 -0.03852826  0.06321152  0.0038594 ]]\n",
       "<NDArray 2x10 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=MySequential()\n",
    "net.add(nn.Dense(256,activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.3 构造复杂的模型\n",
    "class FancyMLP(nn.Block):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(FancyMLP,self).__init__(**kwargs)\n",
    "        # 使⽤get_constant创建的随机权重参数不会在训练中被迭代（即常数参数）\n",
    "        # params为其参数字典(ParameterDict)，get_constant检索参数字典，查找'rand_weight',\n",
    "        # 若未找到，则在参数字典中创建键值对(key='rand_weight',value为Constant类型)\n",
    "        self.rand_weight=self.params.get_constant('rand_weight',nd.random.uniform(shape=(20,20)))\n",
    "        self.dense=nn.Dense(20,activation='relu')\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.dense(x)\n",
    "        # 使⽤创建的常数参数，以及NDArray的relu函数和dot函数\n",
    "        x=nd.relu( nd.dot(x,self.rand_weight.data())+1 )\n",
    "        # 复用全连接层。等价于两个全连接层共享参数 \n",
    "        x=self.dense(x)\n",
    "        # 控制流，这⾥我们需要调⽤asscalar函数来返回标量进⾏⽐较\n",
    "        while x.norm().asscalar()>1:\n",
    "            x/=2\n",
    "        if x.norm().asscalar()<0.8:\n",
    "            x*=10\n",
    "        return x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fancymlp7_ (\n",
       "  Constant fancymlp7_rand_weight (shape=(20, 20), dtype=<class 'numpy.float32'>)\n",
       "  Parameter dense27_weight (shape=(20, 0), dtype=float32)\n",
       "  Parameter dense27_bias (shape=(20,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=FancyMLP()\n",
    "net.initialize()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Block.collect_params of NestMLP(\n",
       "  (net): Sequential(\n",
       "    (0): Dense(20 -> 64, Activation(relu))\n",
       "    (1): Dense(64 -> 32, Activation(relu))\n",
       "  )\n",
       "  (dense): Dense(32 -> 16, Activation(relu))\n",
       ")>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为FancyMLP和Sequential类都是Block类的⼦类，所以我们可以嵌套调⽤它们。 \n",
    "class NestMLP(nn.Block):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(NestMLP,self).__init__(**kwargs)\n",
    "        self.net=nn.Sequential()\n",
    "        self.net.add(nn.Dense(64,activation='relu'),\n",
    "                    nn.Dense(32,activation='relu'))\n",
    "        self.dense=nn.Dense(16,activation='relu')\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.dense(self.net(x))\n",
    "    \n",
    "net=nn.Sequential()\n",
    "net.add(NestMLP(),nn.Dense(20),FancyMLP())\n",
    "\n",
    "net.initialize()\n",
    "net(X)\n",
    "net[0].collect_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "super??\n",
    "nd.random.uniform??\n",
    "\n",
    "import mxnet\n",
    "mxnet.initializer.Uniform??\n",
    "\n",
    "\n",
    "\n",
    "net??\n",
    "nn.Block??\n",
    "\n",
    "net.forward??\n",
    "\n",
    "\n",
    "nn.Dense??\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net.__call__??\n",
    "net._children??\n",
    "\n",
    "\n",
    "\n",
    "net._params??\n",
    "net.params.get_constant??\n",
    "nd.concat??\n",
    "\n",
    "net.initialize??\n",
    "net.collect_params??\n",
    "net.initialize??\n",
    "\n",
    "nn.Sequential??\n",
    "\n",
    "net.initialize??\n",
    "mxnet.initializer.Uniform??\n",
    "\n",
    "nn.Block??\n",
    "nn.Sequential??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
