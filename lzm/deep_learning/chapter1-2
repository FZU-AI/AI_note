# 第一章 #
通俗来说，**机器学习**是一门讨论各式各样的适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。

**深度学习**是指机器学习中的一类函数，它们的形式通常为多层神经网络。

绝大多数神经网络都包含以下的**核心原则**：

1.交替使用线性处理单元与非线性处理单元，它们经常被称为“层”。
 
2.使用链式法则（即反向传播）来更新网络的参数（权值和偏置）。 

**深度学习的优点：**

1.优秀的容量控制方法，如丢弃法（即dropout），可避免大型网络的训练进入过拟合（过拟合即模型在训练集上的预测表现很好，而在测试集上的预测表现不好）。这是靠在整个网络中注入噪声而达到的，如训练时随机将权重替换为随机的数字。 

2.注意力机制使用了一个可学习的指针结构来构建出一个精妙的解决方法。也就是说，与其在像机器翻译这样的任务中记忆整个句子，不如记忆指向翻译的中间状态的指针。

3.模型允许重复修改深度网络的内部状态，这样就能模拟出推理链条上的各个步骤，就好像处理器在计算过程中修改内存一样。 

4.生成对抗网络的关键将采样部分替换成了任意的含有可微分参数的算法。这些参数将被训练到使辨别器不能再分辨真实的和生成的样本。生成对抗网络可使用任意算法来生成输出。

5.并行计算的能力

设计可扩展算法的最大瓶颈在于深度学习优化算法的**核心**：随机梯度下降需要相对更小的批量。

**框架：**

第一代框架：Caffe、Torch和Theano

第二代框架：TensorFlow（经常是以高层API Keras的形式被使用）、CNTK、Caffe2和ApacheMXNet。

第三代框架，即命令式深度学习框架，是由用类似NumPy的语法来定义模型的Chainer所开创的。这样的思想后来被PyTorch和MXNet的Gluon  API采用。

**深度学习的应用：**
1.智能语音助手
2.物体识别
3.博弈
4.自动驾驶汽车

**机器学习和深度学习的关系：**

**机器学习**研究如何使计算机系统利用经验改善性能。

**深度学习**是具有多级表示的表征学习方法（作为机器学习的一类，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出）。在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。

深度学习基于神经网络模型可以逐级表示越来越抽象的概念或模式。以图像为例，它的输入是一堆原始像素值。深度学习模型中，图像可以逐级表示为特定位置和角度的边缘、由边缘组合得出的花纹、由多种花纹进一步汇合得到的特定部位的模式等。最终，模型能够较容易根据更高级的表示完成给定的任务，如识别图像中的物体。

**深度学习的特点：**

1.端到端的训练。即并不是将单独调试的部分拼凑起来组成一个系统，而是将整个系统组建好之后一起训练。

2.从含参数统计模型转向完全无参数的模型。当数据非常稀缺时，我们需要通过简化对现实的假设来得到实用的模型；当数据充足时，我们就可以用能更好地拟合现实的无参数模型来替代这些含参数模型。

3.相对其它经典的机器学习方法而言，深度学习的不同在于：对非最优解的包容、对非凸非线性优化的使用

# 第二章 #
**2.1环境搭建参考：**
[https://discuss.gluon.ai/t/topic/13576](https://discuss.gluon.ai/t/topic/13576)

**2.2数据操作：**

在MXNet中，NDArray是一个类，也是存储和变换数据的主要工具。
向量和矩阵都是特殊的张量。

**1.创建**

(1) x=nd.arange(n)：创建一个长度为n的一维(行向量)数组x，即[0,1,2,...,n-1]。

arange(start,end,step)：创建一个范围从start到end（不包括end）的一维数组，步长为step，step默认1。

arange()被创建在CPU使用的内存上，返回了一个NDArray实例。

(2) x.shape：获取数组x的形状（如二维数组(矩阵)的形状为（n，m），n为行数，m为列数）。 

(3) x.size：获取数组x的元素总数。

(4) x.reshape((n，m))：将数组x的形状改为（n，m），n为行数，m为列数）,除了形状改变之外，x中的元素保持不变。

x.reshape((3, 4))也可写成x.reshape((-1, 4))或x.reshape((3, -1))。由于x的元素个数是已知的，这里的-1是能够通过元素个数和其他维度的大小推断出来的。 

(5) nd.zeros((2, 3, 4))：创建一个各元素为0，形状为(2,3,4)的张量（即多维数组）。

(6) nd.ones((3, 4))：创建各元素为1，形状为(3,4)的张量。 

(7) nd.array([[2,3], [1, 2]])：通过list创建张量，并初始化。

(8) nd.random.normal(0, 1, shape=(3, 4)) ：创建一个形状为（3，4）的张量，每个元素都随机采样于均值为0、标准差为1的正态分布。 

(9) X.zeros_like() == nd.zeros_like(X)：创建和X形状相同且元素为0的NDArray

  X.ones_like() == nd.ones_like(X)：创建和X形状相同且元素为1的NDArray


**2.运算**

(1) +-*/：按元素对张量进行计算(对形状相同的NDArray进行计算，形状不同的NDArray可能会触发广播机制)

+-*/运算符全名函数分别为elemwise_add()、elemwise_sub()、elemwise_mul()、elemwise_div()，使用运算符全名函数可以避免为X + Y开了临时内存来存储计算结果

X+Y == nd.elemwise_add(X,Y)

(2) X.exp() == nd.exp(X)：按元素对张量做指数运算

(3) nd.dot(X, Y.T)：将X与Y的转置(Y.T)做矩阵乘法。X为n行m列，Y为m行n列。注意与X*Y的区别。

(4) nd.concat(X, Y, dim=0)：按行连结两个矩阵

  nd.concat(X, Y, dim=1)：按列连结两个矩阵
	 
(5) X==Y：按元素对张量进行判断是否相等，如果X和Y在相同位置的条件判断为真（值相等），那么新的NDArray在相同位置的值为1；反之为0。

(6) X.sum()：对X中的所有元素求和得到只有⼀个元素的NDArray。 

(7) X.norm()：获得张量X的L2范数(只有⼀个元素的NDArray)，即所有元素的平方和再开根号。
（Lp范数为所有元素的p次方的和再开p次根号）。

范数（Norm）是一个函数，其赋予某个向量空间（或矩阵）中的每个向量以长度或大小。

L-0范数：用来统计向量中非零元素的个数。

L-1范数：向量中所有元素的绝对值之和。可用于优化中去除没有取值的信息，
又称稀疏规则算子。

L-2范数：典型应用——欧式距离。可用于优化正则化项，避免过拟合。

L-∞范数：计算向量中的最大值。

(8) X.asscalar()：将单元素的NDArray变成成标量

(9) X.exp()、X.sum()、X.norm()、X.zeros_like()等可以分别改写为nd.exp(X)、nd.sum(X)、nd. norm(X)、nd.zeros_like(X)等。 

**3.广播机制**

对两个形状不同的NDArray按元素运算时，可能会触发⼴播（broadcasting）机制：先适当复制元素使这两个NDArray形状相同后再按元素运算。 

**4.索引**

(1) X[1:3,:3] ：截取1，2行和0，1，2列的所有元素（依据左闭右开的原则）

(2) X[1:3,:]=12 ：截取1，2行的所有元素，并重新赋值为12

(3) X[1, 2] = 9 ：获取第一1行第2列的元素，并重新赋值为9

**5.运算的内存开销**

以上操作都会新开内存来存储运算结果。即使像X = X + Y这样的运算，我们也会新开内存来存储X+Y的结果，然后将X指向新内存。

id(X)：获得张量X的内存地址

我们可以通过索引将指定结果存储到特定内存，如Z[:] = X + Y ，我们把X + Y的结果通过[:]写进Z对应的内存中。 

如果X的值在之后的程序中不会复用，我们也可以⽤X[:] = X + Y或者X += Y来减少运算的内存开销(X+Y的结果存储在X的内存上，不会新开内存)。 

但是我们还是为X + Y开了临时内存来存储计算结果，再复制到对应的内存。为避免临时内存的开销，我们可以使⽤运算符全名函数中的out参数， 如nd.elemwise_add(X, Y, out=Z) 

**6.NDArray和NumPy相互变换**

D=nd.array(P) ：将NumPy实例P变换成NDArray实例D

D.asnumpy() ：将NDArray实例D变换成NumPy实例。 

**练习**

1.运行本节中的代码。将本节中条件判断式X == Y改为X < Y或X > Y，看看能够得到什么样的NDArray。

逐个元素进行判断，若判断为真，那么新的NDArray在相同位置的值为1；反之为0。

2.将广播机制中按元素运算的两个NDArray替换成其他形状，结果是否和预期⼀样？ 



**2.3自动求梯度**

**1.梯度：**
函数f:R^n(n维的实数向量集合)→R(实数集合)的输⼊是⼀个n维列向量x = [x1,x2,...,xn]^⊤，输出是标量。如y=2(x^⊤)x，y为标量。

函数f(x)有关x的梯度是一个由n个偏导数组成的列向量：

 ∇xf(x) =[∂f(x)/∂x1 , ∂f(x)/∂x2 ,..., ∂f(x)/∂xn ]^⊤

**2.使用 MXNet 自动求梯度的步骤：**
    
    #1. 导入autograd库
    from mxnet import autograd,nd
    #x为n维列向量
    x = nd.arange(4).reshape((4,1))
    #2. 调用attach_grad()函数来申请存储梯度所需要的内存,标记需要求梯度的参数x
    x.attach_grad()
    #3. 调用autograd.record()记录求梯度的计算
	#   为了减少计算和内存开销，默认情况下 MXNet 不会记录（可选）
	#   调用record(),默认情况下autograd还会将运行模式从预测模式转为训练模式
	#   x的形状为（4,1）, y是一个标量
	with autograd.record():
		y = 2 * nd.dot(x.T,x)
	#4. 调用backward()函数自动求梯度,对y求关于x的梯度
	#   如果y不是一个标量，MXNet将默认先对y中元素求和得到新的变量(标量)，再求该变量有关x的梯度
	y.backward()
	#  x的梯度,函数y = 2(x^⊤)x关于x的梯度应为4x
	x.grad
    ----
    output:
    [[ 0.]
     [ 4.]
     [ 8.]
     [12.]]
    <NDArray 4x1 @cpu(0)>

**3.训练模式和预测模式**

MXNet的运行模式包括训练模式和预测模式。调用record函数后，MXNet会记录并计算梯度。此外，默认情况下autograd还会将运⾏模式从预测模式转为训练模式。可以通过调用autograd.is_training()函数来判断是否处于训练模式。 

