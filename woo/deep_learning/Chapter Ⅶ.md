# 第七章

### 7.1  优化与深度学习

在深度学习中，我们通常会定义一个损失函数，有了损失函数，就可以用优化算法试图将其最小化，在优化中，损失函数通常被称作优化问题的目标函数。



#### 7.1.1  优化与深度学习的关系

虽然优化为深度学习提供了最小化损失函数的方法，但本质上，优化与深度学习的目标是有区别的。由于优化算法的目标函数通常是一个基于训练数据集的损失函数，优化的目标在于降低训练误差。而深度学习的目标在于降低泛化误差，因此要特别注意过拟合。本章只关注目标函数，而不关注模型的泛化误差。



#### 7.1.2  优化在深度学习中的挑战

优化在深度学习中会有很多挑战，比如局部最小值和鞍点。

局部最小值：对于目标函数f(x)，如果在x0上的值比在x0邻近的其他点的值更小，那么f(x0)可能是局部最小值，也有可能是全局最小值。求一个优化问题的数值解最优解时，由于目标函数有关解的梯度为0，最终求得的数值解可能只是令目标函数局部最小化而非全局最小化。在高数中的理解可以类别为一个极值。

鞍点：目标函数有关解的梯度为0，也可能是拐点。



### 7.2  梯度下降和随机梯度下降

#### 7.2.1  一维梯度下降

下面以f(x)=x^2 为例看一看梯度下降是如何工作的。

```
#导入所需要的包
%matplotlib inline
import d2lzh as d2l
import math
from mxnet import nd
import numpy as np

#传入梯度下降参数eta为η，对x迭代10次
def gd(eta):
    x=10
    results=[x]
    for i in range(10):
        x -= eta * 2 * x	#x^2的导数为2*x
        print('x:',x)
        results.append(x)    
    return results

#设η为0.2
res=gd(0.2)
```

#### 7.2.2  学习率⭐

上述梯度下降算法中的η叫做学习率，需要人工设定。过小的学习率会导致x更新缓慢从而需要更多次迭代才能得到较好的解。但使用过大的学习率会使一阶泰勒公式失效，这时无法保证迭代x会降低f(x)的值。

#### 7.2.3  多维度梯度下降

了解一维梯度下降后，再考虑一种更广义的情况：目标函数的输入为向量，输出为标量。假设输入为d维向量，目标函数有关x的梯度就是一个由d个偏导数组成的向量。

#### 7.2.4  随机梯度下降

在深度学习中，目标函数通常是训练数据集中有关各个样本的损失函数的平均，如果使用梯度下降，每次自变量迭代的计算开销为O(n)，它随着n线性增长。因此当数据量很大时，梯度每次迭代的计算开销很高。

### 7.3  小批量随机梯度下降

在之前的梯度下降中，使用整个数据集来计算梯度；而随机梯度下降在每次迭代中只随机采样一个样本计算梯度。小批量随机梯度下降是在每轮迭代中随机选取多个样本来组成一个小批量，如果样本数量为1，则变成随机梯度下降。



### 7.4  动量法

#### 7.4.1  梯度下降的问题

在梯度下降中学习率对目标函数的影响极大，容易造成无解或者解的速度极慢的情况。动量法就是为了解决上述问题。

#### 7.4.2  动量法

设时间步的自变量为xt，学习率为ηt，动量法对每次迭代的步骤做如下修改：
$$
vt ← γ * vt−1 + ηt * gt, \\xt ← xt−1 − vt,
$$
其中，动量超参数γ满⾜0 ≤ γ < 1。当γ = 0时，动量法等价于小批量随机梯度下降。 

通过指数加权移动平均理解动量法：γ有不同取值时，yt可以当作对最近的n个时间步的xt值的加权平均，而且离当前时间步t越近的xt的值获得的权重更大（越接近1）。γ=0.95时，取最近的20个时间步；γ=0.9时，取最近的10个时间步。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决于当前梯度，还取决于过去各个梯度。

 

### 7.5  AdaGrad算法

在之前的算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。在“动量法”一节中，x1和x2梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散，但这会导致自变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均使得自变量的更新方向更加一致。AdaGrad则根据每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。使用AdaGrad算法时，自变量中每个元素的学习率在迭代过程中一直在降低或不变。



### 7.6  RMSProp算法

在AdaGrad算法中，自变量中每个元素的学习率在迭代过程中一直在降低或不变。因此，当学习率在迭代早期降得较快且当前解依然不佳时，可能较难找到一个有用的解。为此RMSProp算法AdaGrad算法做了一点小小的修改，RMSProp算法和AdaGrad算法的不同在于，RMSProp算法使⽤了小批量随机梯度按元素平 ⽅的指数加权移动平均来调整学习率。 
$$
st ← γ*st−1 + (1−γ)gt ⊙gt
$$


### 7.7  AdaDelta算法 

除了RMSProp算法以外，另⼀个常⽤优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能 较难找到有⽤解的问题做了改进。



### 7.8  Adam算法

Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均，使⽤了偏差修正。



















