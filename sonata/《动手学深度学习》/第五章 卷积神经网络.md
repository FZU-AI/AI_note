#批量归一化
通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，**训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化**。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。

批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，。**批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路**。

##全连接层的批量归一化
计算过程：
某个批量x而言， 首先求得均值（各样本的x和 / n）及方差（各样本x - 均值 的平方 / n）。再根据均值与方差，对x标准化。学习一组新参数，得到批量归一化输出。
变化过程：
数据集的分布变化如下图。
<img src = "https://img2018.cnblogs.com/blog/1813159/202002/1813159-20200225194758733-1505385923.png" style="zoom:60%">
标准化后的w与b如下图。
<img src = "https://img2018.cnblogs.com/blog/1813159/202002/1813159-20200225195022536-738111419.png" style="zoom:60%">

特点：
1. 对于测试集和数据集用相同的 $\boldsymbol{\mu} \quad and \quad \boldsymbol{\sigma}_\mathcal{B}^2$.
2. 归一化的目的是让训练变得更简单，速度更快，步长更大。
3. 值得注意的是，可学习的拉伸和偏移参数保留了不对$\hat{\boldsymbol{x}}^{(i)}$做批量归一化的可能：此时只需学出$\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}$和$\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}$。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。

##对卷积层做批量归一化
批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且**每个通道都拥有独立的拉伸和偏移参数，并均为标量**。设小批量中有$m$个样本。在单个通道上，假设卷积计算输出的高和宽分别为$p$和$q$。我们需要对该通道中$m \times p \times q$个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中$m \times p \times q$个元素的均值和方差。
我们在所有的卷积层或全连接层之后、激活层之前加入批量归一化层。
```python
net = nn.Sequential(
            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size
            BatchNorm(6, num_dims=4),
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2), # kernel_size, stride
            #第二层
            nn.Conv2d(6, 16, 5),
            BatchNorm(16, num_dims=4),
            nn.Sigmoid(),
            #以下...
```
##小结
* 在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。
* 对全连接层和卷积层做批量归一化的方法稍有不同。
* 批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的。

#残差网络
##ResNet模型
假设某个神经网络的输出是x，期望输出是H(x)，如果直接把输入x传到输出作为初始结果，那么此时需要学习的目标就是$F(x) = H(x) - x$，也就是下面这个残差网络。
左边是一个普通的网络，右边是一个ResNet的残差学习单元，ResNet相当于将学习目标改变了，不再是学习一个完整的输出$H(x)$，而是学习输出和输入的差别$F(x) = H(x) - x$，即残差。
<img src="https://img2018.cnblogs.com/blog/1813159/202002/1813159-20200225195728486-741102622.png" style="zoom:60%">
ResNet沿用了VGG全$3\times 3$卷积层的设计。残差块里首先有2个有相同输出通道数的$3\times 3$卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这**两个**卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求**两个卷积层的输出与输入形状一样**，从而可以相加。如果想改变通道数，就需要引入一个额外的$1\times 1$卷积层来将输入变换成需要的形状后再做相加运算。
ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的$7\times 7$卷积层后接步幅为2的$3\times 3$的最大池化层。不同之处在于**ResNet每个卷积层后增加的批量归一化层**。
GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。