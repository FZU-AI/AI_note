#               **第3章**

## **3.1 单层神经网络之线性回归**

#### 一、线性回归的用途

###### 线性回归的输出值是一个连续值，适用于解决回归问题。如预测房屋价格、气温、销售额等问题；与之相反的有分类问题，此类问题的输出值是一个离散值，如图像分类、邮件识别等，使用softmax回归解决此类问题

#### 二、线性回归的要素

##### 1.模型

设房屋的⾯积为*x*1，房龄为*x*2，售出价格为*y*。我们需要建⽴基于输⼊*x*1和*x*2来计算输出*y*的表达

式，也就是模型（model）。顾名思义，线性回归假设输出与各个输⼊之间是线性关系：

​											y*ˆ = *x*1*w*1 + *x*2*w*2 + *b,*

其中*w*1和*w*2是权重，*b*是偏差，且均为标量。它们是线性回归模型的参数。模型输出*y*ˆ是线性回归对真实价格*y*的预测或估计。我们通常允许它们之间有⼀定误差。

##### 2.训练集

通常收集⼀系列的真实数据，例如多栋房屋的真实售出价格和它们对应的⾯积和房龄。我们希望在这个数据上⾯寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语⾥，该数据集被称为**训练数据集或训练集**，⼀栋房屋被称为⼀个样本（sample），其真实售出价格叫作（label），⽤来预测标签的两个因素叫作特征（feature）：特征⽤来表征样本的特点。

##### 3.损失函数

在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取⼀个⾮负数作为

误差，且数值越小表⽰误差越小。⼀个常⽤的选择是平⽅函数。

##### 4.优化算法

当模型和损失函数形式较为简单时，上⾯的误差最小化问题的解可以直接⽤公式表达出来。这类解叫作**解析解**。然而，⼤多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作**数值解**。

在求数值解的优化算法中，**小批量随机梯度下降**在深度学习中被⼴泛使⽤。

它的算法很简单：先选取⼀组模型参数的初始值，如随机选取；接下来对参数进⾏多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量（mini-batch）*B*，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积作为模型参数在本次迭代的减小量。

模型的每个参数将作如下迭代：

![image-20200728115417202](http://qd2yf2je3.bkt.clouddn.com/20200728115421.png)

在上式中，*|B|*代表每个小批量中的样本个数（批量⼤小，batch size），*η*称作学习率并取正数。需要强调的是，这⾥的批量⼤小和学习率的值是⼈为设定的，并不是通过模型训练学出的，因此叫作超参数。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。

##### 模型预测

模型训练完成后，我们将模型参数*w*1*, w*2*, b*在优化算法停⽌时的值分别记作w1^,w2^,b^。注意，这⾥

我们得到的并不⼀定是最小化损失函数的最优解w1*,w2*,b*,而是对最优解的⼀个近似。

#### 三、线性回归与神经网络的联系

![image-20200728161129274](http://qd2yf2je3.bkt.clouddn.com/20200728172725.png)



在图3.1所⽰的神经⽹络中，输⼊分别为*x*1和*x*2，因此输⼊层的输⼊个数为2。输⼊个数也叫特征

数或特征向量维度。图3.1中⽹络的输出为*o*，输出层的输出个数为1。需要注意的是，我们直接将

图3.1中神经⽹络的输出*o*作为线性回归的输出，即*y*ˆ = *o*。由于输⼊层并不涉及计算，按照惯例，

图3.1所示的神经⽹络的层数为1。所以，线性回归是⼀个单层神经网络。输出层中负责计算*o*的

单元⼜叫神经元。在线性回归中，*o*的计算依赖于*x*1和*x*2。也就是说，输出层中的神经元和输⼊

层中各个输⼊完全连接。因此，这⾥的输出层⼜叫全连接层或稠密层。

#### 四、矢量计算表达式

矢量计算指：设有一个数学表达式

![image-20200728163029445](http://qd2yf2je3.bkt.clouddn.com/20200728163031.png)

用矩阵相加相乘来得到等价的表达式

![image-20200728163131580](http://qd2yf2je3.bkt.clouddn.com/20200728163133.png)

有矢量表达式![image-20200728163233019](http://qd2yf2je3.bkt.clouddn.com/20200728163235.png)

###### 设有两个向量a,b

```
from mxnet import nd
from time import time

a = nd.ones(shape=1000)
b = nd.ones(shape=1000)		//一行1000列的行向量
```

###### ①两个向量按元素逐一做标量加法

```
start=time()
c = nd.zeros(shape=1000)
for i in range(1000):
    c[i] = a[i] + b[i]
   time() - start
```

```
0.6056520938873291
```

###### ②两个向量直接做矢量加法

```
start = time() 
d = a + b
time() - start
```

```
0.0009953975677490234
```