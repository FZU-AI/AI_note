# 第一章 简介

*神经网络的核心原则：*

​	交替使用线性处理单元与非线性梳理单元，他们经常称为“层”

​	使用链式法则（即反向传播）来更新网络参数

*深度学习的定义:*

表征学习是指找出表示数据的合适方式，已将输入变化为正确的输出，书中讨论的深度学习是具有多级表示的表征学习方法。在每一级，深度学习通过简单的函数讲该级的表示变黄为更高级的表示，其可以看做许多简单函数复合的函数。



# 第二章 预备知识

## 配置环境

由于ubuntu中文件下载速度慢，在配置环境时使用的vmware的共享文件夹，其文件夹路径  /mnt/hgfs

​	为了方便使用，创建了到home目录的软连接

```
cd /mnt/hgfs
ln -s /mnt/hgfs/share/ ~/share
```



重启后，设置的共享文件夹找不到了。是由于共享文件夹权限的的原因。原来以为是vmware-tools版本问题，但是通过` vmware-hgfsclient `命令查到共享文件夹仍在，通过https://blog.csdn.net/anlz729/article/details/106826215中的方法更改了共享文件夹的权限，恢复正常。

最终解决方案：

在/root/.bashrc 中添加命令 vmhgfs-fuse .host:/ /mnt/hgfs

每次开机以root用户登陆，就可以直接访问共享文件。

虚拟机装不上CUDA,好像是VMware的原因，虚拟机无法安装CUDA,就使用Windows系统。

在windows中安装cuda时，win10一直提示“此应用无法再你的电脑上运行”，与64位操作系统不匹配，在重装系统后还是出现相同的错误。发现是下载的安装包出现了问题，应该是文件损坏导致无法安装，下载后的文件只有42b，从百度云下载了正常的cuda才完成了安装。

## 数据操作

MXNet中的ndarray模块导入` from mxnet improt nd `

`x =  nd.arrange(num)`  		<!--创建行向量,创建从0开始的num个连续整数-->

` x.shape `									 <!--获得x的事例形状-->

` x.size `									   <!--返回x的元素总数-->

` x.reshape（m，n）`				  <!--将x该为m行n列的矩阵-->

` x.reshape（m，-1）`				<!--n=x.size/m-->

`nd.zeros((2,3,2))`		        <!--创建形状为（2,3,2）的张量，值为0-->

`nd.oness((*,*,*,...))`		<!--与zeros类似，值为1-->

`nd.array([[2,1,4,3],[1,2,3,4],[4,3,2,1]])`			 <!--制定矩阵的每个值得创建方法-->

`nd.random.normal(0,1,shape=(3,4))`							 <!--随机生成元素的值，每个元素都随机采样于均值为0，标准差为1的正态分布，第三个参数为矩阵的维度-->

`nd.random.normal(0,1,(3,4))`											<!--与上一个函数的作用相同，shape可以省略-->

NDArray支持大量的运算符

注“矩阵的‘*’和matlab中的点乘类似，为对应位置的元素相乘，而非矩阵的乘法

`x.exp()`											<!--每个元素变为以e为底的指数的值-->

`dot(x,y,T)`									  <!--矩阵乘法，第三个参数T表示y要转置，可不要该参数-->

`nd.concat(X, Y, dim=0), nd.concat(X, Y, dim=1)`    <!--将x和y拼接，当dim=0时，行拼接，即行数增加，列数不变，要求两个矩阵的列数相同，dim为1时为列拼接-->

`x==y`												   <!--可以判断两个矩阵对应位置上的值是否相等，相等返回1，不等放回0，矩阵形式-->

`x.sum()`											 <!--求x中所有元素的和-->

Y.exp()、X.sum()、X.norm()等分别改写为nd.exp(Y)、nd.sum(X)、nd.
norm(X)等

`x.norm().asscalar()`				   <!--norm为求L2范数的函数,asscalar()函数将向量转换为python标量-->

**⼴播机制** 维度不匹配的两个矩阵A和B，运算时会被复制成最大的维度，再计算



**索引** NDArray的索引从0开始逐⼀递增，索引截取范围左闭右开，如 X[1:3]为x的1,2行，等价于 X[1:3,:]

X[1:3,1:3]为x的1,2行，1,2,列的部分

 X[a, b]为按索引范围矩阵中的具体位置的值



**运算的内存开销 ** 对于每个运算操作新开内存来存储运算结果，如x+y，x-y等  且y=x+y,y的地址发生改变  id(y)

对于`Z[:] = X + Y`z使用了索引，z的地址不会发生改变，但是系统还是会开辟临时的内存来存储计算结果，再复制到z对于的内存，使用云算法全面函数中的out参数，可以避免这个临时的内存开销，如

`nd.elemwise_add(X, Y, out=Z)`



**NDArray和NumPy相互变换** 通过array函数和asnumpy函数令数据在NDArray和NumPy格式之间相互变换

` a=nd.array(P)`			<!--将numyp实例转换为NDArray实例-->

` D.asnumpy()`				<!--将NDArray实例转换为numyp实例-->

## 自动求梯度

1.需要导入autograd包 ` from mxnet import autograd, nd`

2.`x.attach_grad()` 申请存储梯度所需要的内存

3.' with autograd.record():
			 y = 2 * nd.dot(x.T, x)'   调⽤record函数来要求MXNet记录与求梯度有关的计算

4.` y.backward()` backward函数用于⾃动求梯度

5.`x.grad` 计算x的梯度并输出

在调⽤record函数后，MXNet会记录并计算梯度。此外，默认情况下autograd还
会将运⾏模式从预测模式转为训练模式。默认没调用record函数时，运行模式为预测模式

 **对Python控制流求梯度** MXNet的⼀个便利之处是包含了Python的控制流（如条件和循环控制）

```
def f(a):
	b = a * 2
	while b.norm().asscalar() < 1000:
		b = b * 2
	if b.sum().asscalar() > 0:
		c = b
	else:
		c = 100 * b
	return c
with autograd.record():
	c = f(a)
c.backward()
a.grad
```

 定义了一个函数f(a)，函数内带了while和if的控制,其输出一定是x*a,向量，任然可以输出其梯度

**查阅⽂档** 

dir可以查询一个模块中的所有成员和属性 如`print(dir(nd.random))`,通常可以忽略由__开头和结尾的函数

help函数可以查找特定函数和类的使⽤ ` help(nd.ones_like)`