# 第四章 深度学习计算

# 模型构造

**继承Block类来构造模型** Block让模型的构造更灵活，它是nn模块里土工的一个模型构造类，通过继承它来构造需要的模型。使用Block来构造多层感知器，需要重载Block类中的\_\_init\_\_函数(创建模型参数)和forward函数(前向计算)。

```python
class MLP(nn.Block):
    def __init__(self,**kwargs):
        #初始化模型
        super(MLP,self).__init__(**kwargs)
        self.hideen = nn.Dense(256,activation='relu')
        self.output = nn.Dense(10)

    def forward(self,x):
        #定义前向计算
        return self.output(self.hideen(x))
    #反向传播通过自动求梯度来生成所需要的backward函数
```

**Sequential** 通过add函数来添加网络层，`self._children[block.name] = block`,在前向计算时通过迭代器来进行计算。`for block in self._children.values():x = block(x)`

通过继承Block类，可以构造更复杂的模型，如下，构造一个常数参数，并多次调用相同的层的模型。

```python
class FancyMLP(nn.Block):
    #通过继承Block构造复杂的模型
    def __init__(self,**kwargs):
        super(FancyMLP,self).__init__(**kwargs)
        #使⽤get_constant创建的随机权重参数不会在训练中被迭代（即常数参数）
        self.rand_weight = self.params.get_constant('rand_weight',nd.random.uniform(shape=(20,20)))
        self.dense = nn.Dense(20,activation='relu')

    def forward(self,x):
        #在前向计算中使⽤创建的常数参数,并通过控制流多次调⽤相同的层
        x=self.dense(x)
        x=nd.relu(nd.dot(x,self.rand_weight.data())+1)
        #复用全连接层，等价于两个全连接层共享参数
        x = self.dense(x)
        while x.norm().asscalar()>1:
            x/=2
        if x.norm().asscalar()<0.8:
            x*=10
        return x.sum()
```

对于Block类的子类，可以通过nn.Sequential()嵌套调用如

`net = nn.Sequential()
net.add(NestMLP(), nn.Dense(20), FancyMLP())`

## 模型参数的访问、初始化和共享

**参数的访问** Block类可以通过params属性来访问该层包含的所有参数，访问net中某一层的参数可以通过索引的方式来进行，索引0表示最先添加的层，参数的存储是使用参数名(weight,bias)称映射到参数实例的字典。

` net[0].params, type(net[0].params)` 查看第0层的模型参数(有哪些参数，和参数维度)和参数数据类型

` net[0].params['dense0_weight'], net[0].weight`这两个是等价的

`net[0].weight.data()`可以访问参数的具体值

` net[0].weight.grad()`访问参数的梯度

`net.collect_params()`获取net变量的所有嵌套层中的参数，也是一个参数名称到参数实例的字典

` net.collect_params('.*weight')`通过正则表达式来匹配参数名来筛选参数

**参数的初始化**  `net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)`对于初始化过的模型参数，再次初始化,通过制定force_reinit为真

` net.initialize(init=init.Constant(1), force_reinit=True)`用常数1来初始化模型参数

` net[0].weight.initialize(init=init.Xavier(), force_reinit=True)`仅对模型中的第0层的weight参数进行初始化，Xavier()表示随机初始化方法

**自定义的初始化方法** 通过继承init.Initializer,通常只需要实现 _init_weight这个函数。例，构造一个一半为[−10,−5]和[5,10]两个区间⾥均匀分布的随机数，另一半为0的初始化函数。如下

```python
class MyInit(init.Initializer):
	def _init_weight(self, name, data):
		print('Init', name, data.shape)
		data[:] = nd.random.uniform(low=-10, high=10, shape=data.shape)
		data *= data.abs() >= 5
```

` net[0].weight.set_data(net[0].weight.data() + 1)`通过Parameter类的set_data函数可以直接改写模型参数。

**共享模型参数** 在进行前向计算时，希望两个隐藏层共享一组模型参数时，在构造Dense时，可以通过params关键字来进行模型参数共享，在进行方向传播时，共享模型参数的层梯度会被累加，共同作用。如下:

```python
net = nn.Sequential()
shared = nn.Dense(8, activation='relu')
net.add(nn.Dense(8, activation='relu'),
	shared,
	nn.Dense(8, activation='relu', params=shared.params))#在构造Dense时，使用之前某层的参数
```

**模型参数的延后初始化** 在进行模型构造的时候，我们并没有指明出模型输入的维度，所以在执行net.initialize()时，模型无法确定参数的维度，并不会立马进行初始化，而会在第一次做前向计算net(X)时，模型才可以得知参数的维度，来进行模型参数的初始化。它试模型的创建变得更简单，只需要定义每个输出层的大小，而不用人工计算每个输入层的大小。不过这也导致了在进行前向计算之前无法直接操作模型参数。

**避免延后初始化** 1.对已经初始化过的模型参数再次执行net.initialize()时，由于模型参数的维度已知，并不会延后初始化，会立即执行。

2.在创建层时，指定了它的输入个数，通过in_units关键之，也会立即执行初始化函数。

```python
net = nn.Sequential()
net.add(nn.Dense(256, in_units=20, activation='relu'))
net.add(nn.Dense(10, in_units=256))
net.initialize()
```

## 自定义层

深度学习的⼀个魅⼒在于神经⽹络中各式各样的层，因此我们需要自定义层。定义一个自定义层与之前用Block类构造模型类似，需要重新定义\_\_init\_\_(self,**kwargs)函数和forward()函数。一个不含模型参数的自定义层如下:

```python
class CenteredLayer(nn.Block):
    def __init__(self, **kwargs):
        super(CenteredLayer, self).__init__(**kwargs)
    def forward(self, x):
        return x - x.mean()
```

**定义函模型参数的自定义层** 通过Parameter类和ParameterDict类，利⽤Block类⾃带的ParameterDict类型的成员变量params，它是由参数名字(字符串)映射到Parameter类型的模型参数字典。听过get函数从ParameterDict创建Parameter实例。

```python
#创建Parameter实例
params = gluon.ParameterDict()
params.get('param2', shape=(2, 3))
#自定义层
class MyDense(nn.Block):
# units为该层的输出个数，in_units为该层的输⼊个数
    def __init__(self, units, in_units, **kwargs):
        super(MyDense, self).__init__(**kwargs)
        self.weight = self.params.get('weight', shape=(in_units, units))
        self.bias = self.params.get('bias', shape=(units,))
    def forward(self, x):
        linear = nd.dot(x, self.weight.data()) + self.bias.data()
        return nd.relu(linear)
```

## 读取和存储

在训练模型时，经常需要把训练好的模型部署到不同的设备上，这就需要将训练好的模型参数存储在硬盘上，以备后续读取使用。

**NDAyyar的读写** 

`x = nd.ones(3)
nd.save('x', x)`					通过save()将x存储名为'x'的NDArray

` x2 = nd.load('x')`				通过load()加载名为'x'的NDArray

` y = nd.zeros(4)
nd.save('xy', [x, y])
x2, y2 = nd.load('xy')`			存储一列NDArray

` mydict = {'x': x, 'y': y}
nd.save('mydict', mydict)
mydict2 = nd.load('mydict')` 存储和加载从字符串映射到NDArray的字典

**Gluoon模型参数的读写** 

Gluon的Block类提供了save_parameters()函数和load_parameters()函数来读写模型参数。对弈一个MLP()实例化的模型net，对其训练的模型参数的存储和加载如下:

```python
filename = 'mlp.params'			
net.save_parameters(filename)		#将参数存储名为mlp.params的文件
net2 = MLP()
net2.load_parameters(filename)		#不适用随机初始化模型参数，通过加载模型来对参数进行初始化
```

## GPU计算

**NDArray的设备选择**

` x.context`返回NDArray所在的设备

` a = nd.array([1, 2, 3], ctx=mx.gpu())`在存储时选择GPU设备

处理在创建时制定设备，还通过copyto函数和as_in_context函数在设备之间传输数据,当源变量和目标变量的设备一直时，as_in_context函数使⽬标变量和源变量共享源变量的内存或显存，而而copyto函数总是为⽬标变量开新的内存或显存。

`y = x.copyto(mx.gpu())`

`z = x.as_in_context(mx.gpu())`将x,z复制到gpu(0)上

**Gluon的设备选择** 

Gluon的模型在初始化时同样可以通过ctx参数指定设备。

`net.initialize(ctx=mx.gpu())` 在模型初始化函数中加入ctx参数选择设备

