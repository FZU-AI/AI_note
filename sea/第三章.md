---
typora-copy-images-to: ./
---

# 第三章

## 线性回归

**输出**  线性回归输出是一个连续值，而softmax回归适合用于分类问题

**模型**  基于输入的特征x来计算输出y，如有两个特征输入,其模型如下
$$
\hat{y} = x_1 w_1 + x_2 w_2 + b
$$
其中w 为权重，b为偏差，且均为标量。输出是对正式价格y的估计

**损失函数**  用于衡量真实值与预测值之间的误差，通常会选却一个非负数作为误差，一个常用的选择是平方函数。
$$
ℓ ^{(i)} (w_1 ,w_2 ,b) =\frac{1}{2}(\hat{y}^ {(i)} − y^{(i)})^2
$$
使用训练集中的所有样本误差的平均值来衡量模型预测的质量，即
$$
ℓ  (w_1 ,w_2 ,b) =\frac{1}{n}\sum_1^nℓ ^{(i)}(w_1,w_2,b)=\frac{1}{n}\sum_1^n\frac{1}{2}(x_1^{(i)}w_1+x_2^{2}+b-y^{(i)})^2
$$
**目标**  找到一组w和b是的训练样本的平均损失最小

**优化算法** 误差最小化问题的解可以直接⽤公式表达时，叫做解析解，线性回归和平方误差属于这个范畴。然后大多数深度学习模型没有解析解，只能通过优化算法的有限次迭代模型参数来降低算是函数的值，此就叫做数值解

**小批量随机梯度下降** 在数值解的优化算法中，通常采用小批量随机梯度下降。由固定数目的训练样本组层小批量，选取一组模型参数的初始值，可随机选取，然后进行迭代，每次迭代都可能降低损失函数的值。每次迭代求小批量中的平均损失有关参数的倒数，在乘上步长(学习效率)作为每各参数迭代的减小量。

 其中批量大小和学习率时人为设定的，叫作超参数。
$$
w_1 ← w_1 −\frac{η}{|B|}\sum_{i∈B}\frac{∂ℓ^{(i)} (w_1 ,w_2 ,b)}
{∂w_1}
= w_1 −\frac{η}{|B|}\sum_{i∈B}x^{(i)}_1(x^{(i)}_1w_1 + x^{(i)}_2w_2 + b − y^{(i)})
$$
**模型预测** 模型训练完成后，得到的参数并不一定可以最小化损失函数，而是对最优解的一个近似。然后就可以使用模型来进行预测，输入对应得特征，会有对应得输出。

### 线性回归的表示方法

线性回归是一个单层神经网络。当输出层中的神经元和输入层中的各个输入完全连接，这类的输出层叫全连接层或稠密层。其代码实现如下:

```python
from mxnet import nd,autograd
from IPython import display
from matplotlib import pyplot as plt
import random
from time import time
num_inputs = 2          #特征维度
num_examples = 1000     #1000个样本
true_w = [2,-3.4]       #真实权值
true_b=4.2              #真实偏差
features = nd.random.normal(scale=1,shape=(num_examples,num_inputs))  #随机生成特征值
labels = true_w[0]*features[:,0]+true_w[1]*features[:,1]+true_b		#计算特征值对应的输出
labels+=nd.random.normal(scale=0.01,shape=num_examples)			#给标签加上误差，期望为0
'''
def use_svg_display(): #用矢量图显示
    display.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5,2.5)): #设置图的尺寸
    use_svg_display
    plt.rcParams['figure.figsize'] = figsize

set_figsize()
plt.scatter(features[:,1].asnumpy(),labels.asnumpy(),1)
plt.show()
'''
def data_iter(batch_size,features,labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)  #将样本打乱
    for i in range(0,num_examples,batch_size):
        #每次返回batch_size个样本及标签
        j = nd.array(indices[i:min(i+batch_size,num_examples)])
        yield features.take(j),labels.take(j)
        #yield 保持简洁性的同时获得了 iterable 的效果,与iterable类似，逐个放回下一个值，并且内存占用为常数，take函数根据索引放回对应元素
batch_size = 10

#权重初始化
w = nd.random.normal(scale=0.01,shape=(num_inputs,1))
b=nd.zeros(shape=1)
w.attach_grad()
b.attach_grad()

def linreg(x,w,b):      #定义模型
    return nd.dot(x,w)+b

def squared_loss(y_hat,y):  #定义损失函数
    return (y_hat-y.reshape(y_hat.shape))**2/2

def sgd(params,lr,batch_size):      #定义优化算法
    #params为为权重[[w1,w2],b]，lr为学习效率，batch_size为批量大小
    for param in params:
        param[:]=param-lr*param.grad/batch_size

#模型训练
lr = 0.03
num_epochs=3    #迭代次数
net=linreg      #网络模型
loss = squared_loss #损失函数
for epoch in range(num_epochs):
    for x,y in data_iter(batch_size,features,labels):
        with autograd.record():
            l = loss(net(x,w,b),y)
        l.backward()
        sgd([w,b],lr,batch_size)
    train_l = loss(net(features,w,b),labels)
    print('epoch %d,loss %f'%(epoch+1,train_l.mean().asnumpy()))
    print (true_w,w)
    print (true_b,b)
```

### 线性回归的简洁实现 

使用MXNet提供的Gluon借口，可以更方便的实现线性回归的训练

需要导入mxnet中的init、gluon模块；

mxnet.gluon 中的data、nn、loss模块

```python
from mxnet import autograd,nd,gluon
from mxnet.gluon import data as gdata #导入数据模块
from mxnet.gluon import nn #导入神经网络模块
from mxnet import init
from mxnet.gluon import loss as gloss

num_inputs=2
num_examples = 1000
true_w = [2,-3.4]
true_b = 4.2
features = nd.random.normal(scale=1,shape=(num_examples,num_inputs))
labels = true_w[0]*features[:,0]+true_w[1]*features[:,1]+true_b
labels += nd.random.normal(scale = 0.01,shape=labels.shape)
batch_size = 10
#1.读取数据,由于data常用做变量名，因此使用gdata代替
dataset = gdata.ArrayDataset(features,labels)
data_iter = gdata.DataLoader(dataset,batch_size,shuffle=True)   #数据的迭代，分为小批量

#2.定义模型，nn是神经网络的缩写，其定义了大量神经网络的层
net = nn.Sequential()       #串联各个网络层的容器
net.add(nn.Dense(1))        #输出层个数为1的全连接层，无需定义每一层的输入形状，其会自动推断

#3.初始化模型参数
net.initialize(init.Normal(sigma=0.01)) #制定初始权重随机为均值为0，标准差为0.01的正态分布

#4.定义损失函数
loss = gloss.L2Loss()       #损失函数为L2范数损失

#5.定义优化算法
trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':0.03})
#net.collect_params()用于获取模型中的参数，'sgd'表示小批量随机梯度下降算法,第三个参数表示学习效率为0.03

num_epochs = 3
for epoch in range(1,num_epochs+1):
    for X,y in data_iter:
        with autograd.record():
            l=loss(net(X),y)            #l为一维数组
        l.backward()                    #由于l为一维数组，l.backward()等价于l.sum().backward()
        trainer.step(batch_size)        #Trainer实例的step函数来迭代模型参数,指明批量⼤小，从对梯度求平均
    l = loss(net(features),labels)
    print('epoch %d,loss:%f' %(epoch,l.mean().asnumpy()))
dense = net[0]      
print(true_w,dense.weight.data())
print(true_b, dense.bias.data())
#运行时找不到weight和bias,原因pylint是静态检查，在用第三方库的时候有些成员只有在运行代码的时候才会被建立，它就找不到成员，在设置（settings.json）里添加"python.linting.pylintArgs": ["--generate-members"]
```

## softmax回归

应用于离散值预测问题，softmax回归的输出单元从⼀个变成了多个，softmax回归的输出值个数等于标签⾥的类别数，softmax回归的输出层也是⼀个全连接层。

![](.\picture\3_9.png)

softmax输出有两个问题。一是输出层的输出值的范围不确定，很难直观判断。二是真实标签是离散值，离散值与不确定范围的误差难以衡量。使用softmax运算符可以解决这个问题
$$
\hat{y}_1,\hat{y}_2,\hat{y}_3=softmax(o_1,o_2,o_3)\\

\hat{y}_i=\frac{exp(o_i)}{\sum_{j=1}^3exp(o_j)}
$$
在上⾯的图像分类问题中，假
设softmax回归的权重和偏差参数分别为
$$
W=\left[
\matrix{
w_{11}&w_{12}&w_{13}\\
w_{21}&w_{22}&w_{23}\\
w_{31}&w_{32}&w_{33}\\
w_{41}&w_{42}&w_{43}\\
}
\right],b=\left[\matrix {b_1&b_2&b_3}\right]
$$
softmax回归对样本i分类的⽮量计算表达式为
$$
o^{(i)} = x^{(i)}W + b\\
\hat{y}^{(i)} = softmax(o^{(i)}).
$$
**交叉熵损失函数** 

真实标签也可以⽤类别分布表达：对于样本i，我们构造向量y (i) ，使其第y (i) (样本i类别的离散数值)个元素为1，其余为0。对于训练，并不需要预测概率完全等于标签概率，我们只需要我们需要的预测值比其他类别的预测值大即可。例如预测值为0.6时，就能输出正确的类别，而平方损失函数过于严格，要求对应类别预测值要为1。因此使用交叉熵损失函数，其值关心对于正确类别的预测概率，应为只要其值足够大，就可以确保正确分类。交叉熵如下:
$$
H(y^{(i)},\hat{y}^{(i)})=-\sum_{j=1}^qy_j^{(i)}\log \hat{y}_j^{(i)}
$$
遇到⼀个样本有多个标签时，例如图像⾥含有不⽌⼀个物体时,交叉熵同样只关⼼对图像中出现的物体类别的预测概率。交叉熵损失函数定义如下:
$$
ℓ (\theta)=\frac{1}{n}\sum_{i=1}^nH(y^{(i)},\hat{y}^{(i)})
$$
**模型预测及评价** 训练好softmax回归模型后，给定任⼀样本特征，就可以预测每个输出类别的概率。如果它与真实类别(标签)⼀致，说明这次预测是正确的,使⽤准确率(accuracy)来评价模型的表现。

### 图像分类数据集

使用Fashion-MNIST数据集，其获取可以通过`gdata.vision.FashionMNIST(train=True)`

'`gdata.vision.transforms.ToTensor() `将图像或成批图像转换为张量NDArray,还将图像通道从最后⼀维移到最前⼀维来⽅便之

`load_data_fashion_mnist(batch_size)`,返回train_iter和test_iter两个变量

### softmax的简单实现

`X.sum(axis=0,keepdims=True)` 对同一行(axis=1)和同一列(axis=0)求和，结果保留在行和列这两个维度

```python
import d2lzh as d2l
from mxnet import gluon, init
from mxnet.gluon import loss as gloss, nn

#1.获取数据集,设置批量大小
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

#2.定义模型，并初始化权值,有10个输出的全连接层
net = nn.Sequential()
net.add(nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))

#3.定义交叉熵损失函数
loss = gloss.SoftmaxCrossEntropyLoss()

#4.定义优化算法
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})

#5.训练模型
num_epochs = 5
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,
None, trainer)
```



## 多层感知机

多层感知器相比单层神经网络增加了一个或多个隐藏层，位于输入层和输出层之间。![](.\picture\3_8.png)

给定一个小批量样本X，感知器的输出为O，对于单隐藏层的感知器模型为
$$
H=XW_h+b_h\\
O=HW_o+b_o
$$
将两个式子联立可以得到
$$
O=(XW_h+b_h)W_o+b_o=XW_hW_o+b_hW_o+b_o
$$
可以看出，它依然等价于一个单层神经网络，其中输出层权重参数为W\_{h}W\_{o},偏差为b\_{h}W\_{o}+b\_{o},由此可以发现再添加更多的隐藏层，<u>_以上设计只能与单层神经网络等价_</u>

**激活函数** 上述问题的原因是全连接层知识对数据做反射变换，而多个反射变换的叠加仍然是一个仿射变换。解决方法是引入非线性变换，例如对隐藏变量使⽤按元素运算的⾮线性函数（激活函数）进⾏变换，然后再作为下⼀个全连接层的输⼊

ReLU函数，其只保留正数元素，并将负数元素清零,当输入为正数时导数为1，负数时导数为0
$$
ReLU = max(x,0)
$$
sigmoid函数，将元素的值变化到0和1之间,当输⼊为0时，sigmoid函数的导数达到最⼤值0.25；当输⼊越偏
离0时，sigmoid函数的导数越接近0,导数关于y轴对称
$$
sigmod(x)=\frac{1}{1+exp(-x)}
$$
tanh函数，将元素的值变化到-1和1之间,形状与sigmoid函数类似，输入为0时导数最大，为1，输入越偏离0时导数越接近0
$$
tanh(x)=\frac{1-exp(-2x)}{1+exp(-2x)}
$$
因此层感知机就是含有⾄少⼀个隐藏层的由全连接层组成的神经⽹络，且每个隐藏层的输出通过激
活函数进⾏变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。其模型如下
$$
H=\phi(XW_h+b_h)\\
O=HW_o+b_o
$$
其中 ϕ表示激活函数，在分类问题中可以对输出o做softmax运算，使用softmax的交叉熵损失函数。

### 多层感知机的实现

```python
import d2lzh as d2l
from mxnet import gluon, init
from mxnet.gluon import loss as gloss, nn

#1.定义模型并初始化，一层个输出有256个的隐藏层，使用ReLU作为激活函数
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'),nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))

#2.读取数据
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
#3.定义损失函数
loss = gloss.SoftmaxCrossEntropyLoss()
#4.定义优化函数
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})
#5.模型训练
num_epochs = 5
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,None, trainer)
```

## 模型选择、欠拟合、过拟合

训练误差指的是在训练集上的误差，泛化误差值得是在测试集上的误差。

在机器学习中，需要评估若干候选模型，并进行模型选择。这就需要验证数据集来进行选择。

**验证数据集** 通常测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。因此不应该只依赖测试集来进行模型选择，需要预留一部分在训练集合测试集以外的数据来进行模型选择。这部分数据就叫验证数据集。

**K折交叉验证** 把原始训练数据分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个字数据集验证模型，并使用其他K-1个字数据集来训练模型。取K次训练误差和验证误差分别求平均。

**欠拟合和过拟合** 欠拟合指的是无法得到较低的训练误差，过拟合是指训练误差远低于测试误差。在实践中，应尽量应对欠拟合和过拟合。有两个重点因素:模型复杂度和训练数据集大小

**模型复杂度** 对于高阶多项式函数模型，模型函数的选择空间更大，所以其复杂度也更高，容易在训练集上给出更低的误差，而模型复杂度过低，很容易出现欠拟合。选择复杂度合适的模型是解决过拟合和欠拟合的一个方法。

![1](.\picture\3_1.png)

**数据集大小** 影响拟合程度的另一个因素是数据集的大小，如果训练集中的样本较少，特别是比模型参数数量更少时容易发生过拟合，此外泛化误差不会随着数据集的增加而增大，因此在计算资源运行的范围内，训练数据集大一些较好，特别是模型复杂时，如层数较多的深度学习模型。

#### 多项式函数

以下显示了三阶多项式函数的正常拟合，欠拟合，过拟合的在测试集和训练集上的每次迭代的损失函数的损失值图像。

```python
import d2lzh as d2l
from mxnet import autograd, gluon, nd
from mxnet.gluon import data as gdata, loss as gloss, nn

#使用模型 y = 1.2x − 3.4x 2 + 5.6x 3 + 5 + ϵ来生成数据,100个训练集和测试集数据
n_train,n_test,true_w,true_b=100,100,[1.2,-3.4,5.6],5
features = nd.random.normal(shape=(n_train+n_test,1))
#计算各个阶数的特征
poly_features = nd.concat(features, nd.power(features, 2),nd.power(features, 3))
labels = (true_w[0] * poly_features[:, 0] + true_w[1] * poly_features[:, 1]+ true_w[2] * poly_features[:, 2] + true_b)
labels += nd.random.normal(scale=0.1, shape=labels.shape)   #生成误差

 #制图函数,x_vals, y_vals为第一组输入的x和y;x_label, y_label为坐标轴标签;x2_vals, y2_vals为另一组x,y，legend为两组数据的名称
def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,legend=None, figsize=(3.5, 2.5)):
    d2l.set_figsize(figsize)
    d2l.plt.xlabel(x_label)
    d2l.plt.ylabel(y_label)
    d2l.plt.semilogy(x_vals, y_vals)
    if x2_vals and y2_vals:
        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=':')
        d2l.plt.legend(legend)
    d2l.plt.show()

#超参数及损失函数
num_epochs, loss = 100, gloss.L2Loss()

def fit_and_plot(train_features, test_features, train_labels, test_labels):
    #定义模型及初始化
    net = nn.Sequential()
    net.add(nn.Dense(1))
    net.initialize()
    #小批量数据
    batch_size = min(10, train_labels.shape[0])
    train_iter = gdata.DataLoader(gdata.ArrayDataset(train_features, train_labels), batch_size, shuffle=True)
    #优化算法
    trainer = gluon.Trainer(net.collect_params(), 'sgd',{'learning_rate': 0.01})
    train_ls, test_ls = [], []      #损失函数值的记录
    for _ in range(num_epochs):
        for X, y in train_iter:
            with autograd.record():
                l = loss(net(X), y)
            l.backward()
            trainer.step(batch_size)
        train_ls.append(loss(net(train_features),train_labels).mean().asscalar())
        test_ls.append(loss(net(test_features),test_labels).mean().asscalar())
    print('final epoch: train loss', train_ls[-1], 'test loss', test_ls[-1])
    semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',range(1, num_epochs + 1), test_ls, ['train', 'test'])
    print('weight:', net[0].weight.data().asnumpy(),'\nbias:', net[0].bias.data().asnumpy())

#正常拟合
fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :],labels[:n_train], labels[n_train:])
#欠拟合
fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train],labels[n_train:])
#训练样本不足时(4个参数，两个训练样本)的过拟合
fit_and_plot(pol![3_2](D:\Backup\我的文档\My Pictures\Screenshots\3_2.png)y_features[0:2, :], poly_features[n_train:, :], labels[0:2],labels[n_train:])
```

​																						正常拟合

![3_2](.\picture\3_2.png)

​																						欠拟合

![3_3](.\picture\3_3.png)

​																						过拟合

![3_4](.\picture\3_4.png)

## 权重衰减

通过权重衰减是用来应对过拟合的，它等价于L2范数正则化，正则化通过为模型损失函数添加惩罚项试学出的模型参数值较小。

L2范数正则化在损失函数的基础上添加L2范数的惩罚项，从而得到训练所需要的最小化函数。惩罚项试模型权重参数的每个原始的平方和与一个正的常数的乘积。以线性回归的损失函数为例，其损失函数和带惩罚项的损失函数如下。
$$
ℓ(w_1,w_2,b)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})\\
ℓ(w_1,w_2,b)+\frac{λ}{2n}||w||^2
$$
其中超参数λ>0,当权重为0时惩罚项最小，其中||w||^2=w1^2+w2^2，其权重的迭代变化如下:
$$
w_1 ←  (1-\frac{nλ}{|B|})w_1 −\frac{η}{|B|}\sum_{i∈B}x^{(i)}_1(x^{(i)}_1w_1 + x^{(i)}_2w_2 + b − y^{(i)})
$$
可见L2范数正则化先令权重w1和w2先自乘小于1的数，在减去不含惩罚项的梯度。因此L2正则化又叫权重衰减。在实际应用，有时会在惩罚项中添加偏差元素的平方和。

### 高维线性回归实验

通过高维线性回归来引入一个过拟合问题，并使用权重衰减来应对过拟合。每个样本的特征维度为p，使用以下的线性函数作为样本的模型, ϵ为噪声项，服从均值为0，标准差为0.01的正态分布。
$$
y=0.05+\sum_{i=1}^p0.01x_i+ ϵ
$$
实验使用了20个样本作为训练集，100个样本作为测试集，样本的特征维度为200

当惩罚项的权重为0时，其损失函数图像如下

![3_5](.\picture\3_5.png)

当惩罚项的权重为3时，其损失函数图像如下

![3_6](.\picture\3_6.png)

可以看出对于过拟合现象得到一定程度的缓解。且权重参数的L2 范数⽐不使⽤权重衰减时的更小，此时的权重
参数更接近0。

## 倒置丢弃法

对于一个含有单层隐藏层的感知器，以上述的感知机为例，其输入个数为4，隐藏层单元个数为5，当使用丢弃发时，隐藏层单元将有一定概率被丢弃掉。设丢弃的概率为p，即有p的概率hi会被清0，有1-p的概率hi会除以1-p做拉伸。其丢弃概率是超参数。设随机变量ξ i 为0和1的概率分别为p和p-1。此时hi‘如下
$$
h_i'=\frac{ξ_i }{1-p}h_i\\
E(ξ_i)=1-p\\
E(h_i')=\frac{E(ξ_i)}{1-p}h_i=h_i
$$
可见丢弃发不改变输入的期望值，由于在训练时隐藏层神经元的丢弃时随机的，输出层无法过度依赖任何一个隐藏层的输出，从而在训练模型是起到正则化的作用，来应对过拟合。在测试模型时，为了稳定性，一帮不适用丢弃法。其简洁实现如下:

```python
import d2lzh as d2l
from mxnet import autograd, gluon, init, nd
from mxnet.gluon import loss as gloss, nn

#数据集、损失函数、小批量、超参数设置
drop_prob1, drop_prob2 = 0.2, 0.5
num_epochs, lr, batch_size = 5, 0.5, 256
loss = gloss.SoftmaxCrossEntropyLoss()
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

#定义模型和初始化参数
net = nn.Sequential()
net.add(nn.Dense(256, activation="relu"),
    nn.Dropout(drop_prob1), # 在第⼀个全连接层后添加丢弃层
    nn.Dense(256, activation="relu"),
    nn.Dropout(drop_prob2), # 在第⼆个全连接层后添加丢弃层
    nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))

#定义优化算法和训练测试模型
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,None, trainer)
```



## 正向传播和反向传播和计算图

以带L2范数正则化的含单隐藏层的多层感知机为样例模型解释正向传播和反向传播

**正向传播** 是指神经网络沿着输入层到输出岑的顺序，依次计算并存储模型的中间变量。设样本的特征维度为d，对于单个样本，在不考虑偏差项时，中间变量
$$
z=W^{(1）}x
$$
之后输入激活函数
$$
h = ϕ(z)
$$
再通过隐藏层到达输出层
$$
o = W^{(2)}h
$$
假设损失函数为ℓ，计算单个样本的损失项
$$
L=ℓ(o,y)
$$
再由给定的超参数定义正则化项
$$
s =\frac{λ}{2}(||W^{(1)}||_F^2+||W^{(2)}||_F^2)
$$
带正则化的损失为
$$
J = L + s
$$
**计算图** 图中左下⻆是输⼊，右上⻆是输出，⽅框代表变量，圆圈代表运算符，箭头表⽰从输⼊到输出之间的依赖关系。

![3_7](.\picture\3_7.png)

**反向传播** 指神经网络计算参数梯度的方法。反向传播依据微积分中的链式法则，沿着从输出层到输⼊层的顺序，依次计算并存储⽬标函数有关神经⽹络各层的中间变量以及参数的梯度。对输⼊或输出X,Y,Z为任意形状张量的函数Y = f(X)和Z = g(Y)，通过链式法则有
$$
\frac{∂Z}{∂X}=prod(\frac{∂Z}{∂Y},\frac{∂Y}{∂X})
$$
prod运算符将根据两个输⼊的形状，在必要的操作（如转置和互换输⼊位置）后对两个输⼊做乘法。

对于含有一层隐藏层的感知机的反向传播如下。

⽬标函数J = L + s有关损失项L和正则项s的梯度:
$$
\frac{∂J}{∂L}=1,\frac{∂J}{∂s}=1
$$
J有关于输出层变量o的梯度：
$$
\frac{∂J}{∂o}=prod(\frac{∂J}{∂L},\frac{∂L}{∂o})=\frac{∂J}{∂o}
$$
正则项关于权重参数的梯度：
$$
\frac{∂s}{∂W^{(i)}}= λW^{(i)}
$$
最靠近输出层的模型参数(W2)的梯度：
$$
\frac{∂J}{∂W^{(2)}}=prod(\frac{∂J}{∂o},\frac{∂o}{∂W^{(2)}})
	+prod(\frac{∂J}{∂s},\frac{∂s}{∂W^{(2)}})
	=\frac{∂J}{∂O}h^T+ λW^{(2)}
$$
隐藏层变量输入h的梯度：
$$
\frac{∂J}{∂h}=prod(\frac{∂J}{∂o},\frac{∂o}{∂h})=W^{(2)^T}\frac{∂J}{∂o}
$$
对于激活函数计算的z的梯度,⊙为按元素乘法符:
$$
\frac{∂J}{∂z}=prod(\frac{∂J}{∂h},\frac{∂h}{∂z})=\frac{∂J}{∂h}⊙  ϕ '(z)
$$
最靠近输入层的模型参数（W1）的梯度:
$$
\frac{∂J}{∂W^{(1)}}=prod(\frac{∂J}{∂z},\frac{∂z}{∂W^{(1)}})
	+prod(\frac{∂J}{∂s},\frac{∂s}{∂W^{(1)}})
	=\frac{∂J}{∂O}x^T+ λW^{(1)}
$$
**训练深度学习模型** 在训练深度学习模型时，正向传播和反向传播之间相互依赖。正一方面，向传播的计算可能依赖当前模型参数的值，而模型的参数是在反向传播的梯度计算后迭代出来的。另一方面，方向传播的值得计算可能依赖于当前各个变量的值，而这些值是通过正向传播计算的。因此在模型参数初始化后，会交替进行正向传播和反向传播。

## 数值稳定性和模型初始化

**衰减和爆炸** 是深度模型有关数值稳定性的典型问题。当神经⽹络的层数较多时，模型的数值稳定性容易变差，容易出现衰减和爆炸。例如，假设输入和所有层的权重参数都是标量，如参数权重为0.2和5，多层感知机的第30层输出是输入X与0.2^30≈10^-21和5^30的乘积，出现了衰减和爆炸。

**随机初始化模型参数** 如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输⼊计算出相同的值，并传递⾄输出层。在反向传播中，每个隐藏单元的参数梯度值相等，之后的迭代也是如此。在这种情况下，⽆论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作⽤，且容易出现衰减和爆炸。因此，正如在前⾯的实验中所做的那样，我们通常将神经⽹络的模型参数，特别是权重参数，进⾏随机初始化。

**MXNet的默认随机初始化** 如`net.initialize(init.Normal(sigma=0.01))`使模型参数按正态分布随机初始化。如果参数为空，默认为-0.07到0.07之间的均匀分布，偏差参数全为0

**Xavier随机初始化** 假设某全连接层的输⼊个数为a，输出个数为b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布,其主要特点是，每层输出的⽅差不该受该层输⼊个数影响，且每层梯度的⽅差也不该受该层输出个数影响。
$$
U(-\sqrt{\frac{6}{a+b}},\sqrt\frac{6}{a+b})
$$
