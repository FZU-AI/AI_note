[TOC]

# 卷积神经网络

卷积神经网络cnn，是在深度学习中很常用的一种神经网络。cnn包括卷积层，池化层，全连接层等

## 卷积层

卷积层其实就通过**卷积核**对输入的数据进行运算，可以加入bias偏差项。

cross-correlation的loop顺序是**从左到右，从上到下**，

而convolution是**从右到左，从下到上**

实际实现时一般使用互相关运算代替卷积运算

特征图是指输入的数据，感受野是影响元素*x的前向计算的所有可能输⼊区域。包括空洞卷积，就是为了扩大感受野减少计算。

### 特点

减少了计算量

### 手工实现

单通道的情况

```
def corr2d(X, K): 
	h, w = K.shape
	Y = nd.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
	for i in range(Y.shape[0]):
		for j in range(Y.shape[1]):
			Y[i, j] = (X[i: i + h, j: j + w] * K).sum()
	return Y
class Conv2D(nn.Block):
	def __init__(self, kernel_size, **kwargs):
		super(Conv2D, self).__init__(**kwargs)
		self.weight = self.params.get('weight',shape=kernel_size)
		self.bias = self.params.get('bias', shape=(1,))
	def forward(self, x):
		return corr2d(x, self.weight.data()) + self.bias.data()
	
```

多通道只需要通过add_n函数来进⾏累加。

```
return nd.add_n(*[d2l.corr2d(x, k) for x, k in zip(X, K)])
```



### 超参数

卷积层常见的两个超参数包括padding 和 stride

#### padding

指在输入高和宽的两侧填充元素，输出形状将会是n-k+p+1。

#### stride

指卷积核滑动的步幅大小，默认为1，可以用于减小输出的大小

## 池化层

为了缓解卷积层对位置的过度敏感性。包括最大池化，平均池化等。

## LeNet

LeNet分为两个部分，卷积层块和全连接层块。

卷积层块⾥的基本单位是卷积层后接最⼤池化层：卷积层⽤来识别图像⾥的空间模式，如线条和物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性。

卷积层块的输出形状为(批量⼤小, 通道, ⾼, 宽)。全连接层的输⼊形状将变成⼆维，其中第⼀维是小批量中的样本，第⼆维是每个样本变平后的向量表示

LeNet交替使⽤卷积层和最⼤池化层后接全连接层来进⾏图像分类。

实现代码：

```
net = nn.Sequential()
net.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'),nn.MaxPool2D(pool_size=2, strides=2),nn.Conv2D(channels=16,kernel_size=5,activation='sigmoid'),nn.MaxPool2D(pool_size=2, strides=2),nn.Dense(120, activation='sigmoid'),nn.Dense(84, activation='sigmoid'),nn.Dense(10))
```

## AlexNet

特点：

1. AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。
2. AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。
3. AlexNet通过丢弃法来控制全连接层的模型复杂度

## VGG

**数个相同**的填充为1、窗口形状为3 *×* 3的卷积层后接上⼀个步幅为2、窗口形状为2 *×* 2的最⼤池化层。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。

## NiN

NiN使⽤1*×*1卷积层来替代全连接层，从而使空间信息能够⾃然传递到后⾯的层中去。

​	

NiN块是NiN中的基础块。它由⼀个卷积层加两个充当全连接层的1 *×* 1卷积层串联而成。

```
def nin_block(num_channels, kernel_size, strides, padding):
	blk = nn.Sequential()
	blk.add(nn.Conv2D(num_channels, kernel_size,
strides, padding,activation='relu'),nn.Conv2D(num_channels, kernel_size=1, activation='relu'),nn.Conv2D(num_channels, kernel_size=1, activation='relu'))
	return blk
```

## GoogLeNet

主要贡献是提出了inception块

![image-20200712224457747](C:\Users\67592\AppData\Roaming\Typora\typora-user-images\image-20200712224457747.png)

通过多个卷积层并行连结丰富特征。

##  批量归⼀化

在模型训练时，批量归⼀化利⽤**小批量上的均值和标准差**，不断调整神经⽹络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。

### 对全连接层做批量归⼀化

将批量归⼀化层置于全连接层中的仿射变换和激活函数之间。

批量归⼀化层引⼊了两个可以学习的模型参数，拉伸（scale）参数 **γ** 和偏移（shift）参数 **β**。

### 对卷积层做批量归⼀化

对卷积层批量归⼀化发⽣在卷积计算之后、应⽤激活函数之前。

```
if len(X.shape) == 2: # 使⽤全连接层的情况，计算特征维	上的均值和⽅差
	mean = X.mean(axis=0)
	var = ((X - mean) ** 2).mean(axis=0)
else:# 使⽤⼆维卷积层的情况，计算通道维上（axis=1）的均值和⽅差。这⾥我们需要保持
	# X的形状以便后⾯可以做⼴播运算
	mean = X.mean(axis=(0, 2, 3), keepdims=True)
	var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True) 
# 训练模式下⽤当前的均值和⽅差做标准化
X_hat = (X - mean) / nd.sqrt(var + eps)
# 更新移动平均的均值和⽅差
moving_mean = momentum * moving_mean + (1.0 -momentum) * mean
moving_var = momentum * moving_var + (1.0 -momentum) * var
```

## ResNet

16年提出，通过跨层累加计算，防止反向传播时梯度消失等问题。

![image-20200712225903465](C:\Users\67592\AppData\Roaming\Typora\typora-user-images\image-20200712225903465.png)

## DenseNet

DenseNet跨层连接上使用联结，ResNet使用的是相加求和。

### 稠密块

先对卷积层进行批量归一化，之后将多个卷积层连结得到基础的稠密块。

```
class DenseBlock(nn.Block):
	def __init__(self, num_convs, num_channels, **kwargs):
		super(DenseBlock, self).__init__(**kwargs)
		self.net = nn.Sequential()
		for _ in range(num_convs):
			self.net.add(conv_block(num_channels))
	def forward(self, X):
		for blk in self.net:
			Y = blk(X)
			X = nd.concat(X, Y, dim=1) 
		return X
```

### 过渡层

过渡层⽤来控制模型复杂度。它通过1 *×* 1卷积层来减小通道数，并使⽤步幅为2的平均池化层减半⾼和宽
