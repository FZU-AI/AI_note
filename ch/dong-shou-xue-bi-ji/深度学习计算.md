[TOC]

# 深度学习计算

## 模型构造

### 构造Sequential实例

```
#创建一个Sequential网络实例
net = nn.Sequential()
#创建中间层，dense创建全连接层，表示有几个输出
net.add(nn.Dense(256, activation='relu'),
nn.Dense(10))
#初始化参数
net.initialize(init.Normal(sigma=0.01))
```

### 继承Block类来构造模型

通过重载了Block类的init函数、forward函数等构造模型

```
from mxnet import nd
from mxnet.gluon import nn
class MLP(nn.Block):
    # 声明带有模型参数的层，这⾥声明了两个全连接层
    def __init__(self, **kwargs):
        # 调⽤MLP⽗类Block的构造函数来进⾏必要的初始化。这样在构造实例时还可以指定其他函数
        # 参数，如“模型参数的访问、初始化和共享”⼀节将介绍的模型参数params
        super(MLP, self).__init__(**kwargs)
        self.hidden = nn.Dense(256, activation='relu') # 隐藏层
        self.output = nn.Dense(10) # 输出层
    # 定义模型的前向计算，即如何根据输⼊x计算返回所需要的模型输出
    def forward(self, x):
    	return self.output(self.hidden(x))
```

<!--注:它的⼦类既可以是⼀个层（如Gluon提供的Dense类），⼜可以是⼀个模型（如这⾥定义的MLP类），或者是模型的⼀个部分。-->

### Sequential类继承自Block类

```
class MySequential(nn.Block):
	def __init__(self, **kwargs):
    	super(MySequential, self).__init__(**kwargs)
    def add(self, block):
        # block是⼀个Block⼦类实例，假设它有⼀个独⼀⽆⼆的名字。我们将它保存在Block类的
        # 成员变量_children⾥，其类型是OrderedDict。当MySequential实例调⽤
        # initialize函数时，系统会⾃动对_children⾥所有成员初始化
        self._children[block.name] = block
    def forward(self, x):
    	# OrderedDict保证会按照成员添加时的顺序遍历成员
        for block in self._children.values():
        x = block(x)
        return x
```

get_constant函数可以创建训练中不被迭代的参数，即常数参数。

## 模型参数的访问、初始化和共享

### 访问模型参数

对于使⽤Sequential类构造的神经⽹络，我们可以通过⽅括号[]来访问⽹络的任⼀层。

```
#索引0表⽰隐藏层为Sequential实例最先添加的层。
net[0].params, type(net[0].params)
```

为了访问特定参数，我们既可以通过名字来访问字典⾥的元素，也可以直接使⽤它的变量名。

```
net[0].params['dense0_weight'], net[0].weight
```

Gluon⾥参数类型为Parameter类，它包含参数和梯度的数值，可以分别通过data函数和grad函数来访问。

collect_params函数获取net变量所有嵌套的层所包含的所有参数。它返回的同样是⼀个由参数名称到参数实例的字典。

```
net.collect_params()
```

### 初始化模型参数

参数初始化的方式其实有很多

**正态分布随机数**

```
# ⾮⾸次对模型初始化需要指定force_reinit为真
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)
```

**常数**

```
net.initialize(init=init.Constant(1),force_reinit=True)
```

**对某个特定参数进行初始化**

```
net[0].weight.initialize(init=init.Xavier(), force_reinit=True)
```

**自定义初始化：以实现⼀个Initializer类的子类**

通常，我们只需要实现_init_weight这个函数，并将其传⼊的NDArray修改成初始化的结果。

```
class MyInit(init.Initializer):
	def _init_weight(self, name, data):
		print('Init', name, data.shape)
		data[:] = nd.random.uniform(low=-10, high=10, shape=data.shape)
		data *= data.abs() >= 5
```

**通过Parameter类的set_data函数来直接改写模型参数**

```
net[0].weight.set_data(net[0].weight.data() + 1)
```

### 共享模型参数

可以在构造某个网络层时通过params指定其使用前面出现过的层的参数。注意的是共享层的参数在反向传播时会叠加计算。

## 模型参数的延后初始化

因为Gluon创建的全连接层都没有指定输⼊个数。所以模型参数的延后初始化。

### 避免延后初始化

- 对已初始化的模型重新初始化

- 创建层的时候指定了它的输⼊个数

  ```
  net = nn.Sequential()
  net.add(nn.Dense(256, in_units=20, activation='relu'))
  net.add(nn.Dense(10, in_units=256))
  ```

## 自定义层

与使⽤Block类构造模型类似。重载一个类，初始化部分__init__继承父类

```
super(class_name, self).__init__(**kwargs)
```

对于层计算的部分重写一个forward函数

对于网络层的参数可以在__init__部分补充

```
#可以使用params.get添加参数
def __init__(self, units, in_units, **kwargs):
    super(MyDense, self).__init__(**kwargs)
    self.weight = self.params.get('weight', shape=(in_units, units))
    self.bias = self.params.get('bias', shape=(units,))
```

## 读取和存储

### 读写NDArray

save函 数 和load函 数 分 别 存 储 和 读 取NDArray

### 读写Gluon模型的参数

Gluon的Block类 提 供了save_parameters函 数 和load_parameters函 数 来 读 写 模 型 参 数。

## GPU计算

主要实现的方法是通过ctx参数指定gpu设备。
