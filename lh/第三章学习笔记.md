# 第三章
----
从单层神经网络延伸到多层神经网络，并通过多层感知机引入深度学习模型
> * 线性回归
> * NDArray运算
> * 索引
> * NDArray和NumPy相互变换
> * 自动求梯度
> * 查阅文档

## 线性回归
---
### 1.模型与模型训练
线性回归假设输出与各个输入之间是线性关系
如： y = x1w1 + x2w2 + b;
基于输入x1和x2来计算输出y的表达式，其中w1和w2是权重（weight），b是偏差（bias），且均为标量。
**它们是线性回归模型的参数（pa-rameter）** 。模型输出^ y是线性回归对真实价格y的预测或估计。
接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程 叫作模型训练（model training） 
### 2.损失函数
选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。它在评估索引为i的样本误差的表达式为：
ℓ (i) (w1; w2; b) = 1/2 * ( Y (i)- y (i) )^2 
误差越小表 示预测价格与真实价格越相近，且当二者相等时误差为0
### 3.矢量计算表达式
```python
from mxnet import nd 
from time import time 
#对两个向量相加的两种方法
a = nd.ones(shape=1000) 
b = nd.ones(shape=1000) 
#向量相加的一种方法是，将这两个向量按元素逐一做标量加法
start = time() 
c = nd.zeros(shape=1000) 
for i in range(1000): 
    c[i] = a[i] + b[i] 
time() - start #计算运算时间 为0.15223002433776855 
#向量相加的另一种方法是，将这两个向量直接做矢量加法
start = time() 
d = a + b 
time() - start #0.00029015541076660156 应该尽可能采用矢量计算，以提升计算效率
```
## 线性回归的从零开始实现
```python
#首先导入包
%matplotlib inline #用于绘图 
from IPython import display 
from matplotlib import pyplot as plt 
from mxnet import autograd, nd 
import random
```
### 1.生成数据集
设训练数据集样本数为1000，输入个数（特征数）为2，我们使用线性回归模型真实权重w = [2,-3.4]^⊤ 和偏差b = 4.2，以及一个随机噪声项ε来生成标签
y = X1w1+X2w2+b+ε
```python
num_inputs = 2 
num_examples = 1000 
true_w = [2, -3.4] 
true_b = 4.2 
features = nd.random.normal(scale=1, shape=(num_examples, num_inputs)) 
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b 
labels += nd.random.normal(scale=0.01, shape=labels.shape) 
#features的每一行是一个⻓度为2的向量，而labels的每一行是一个⻓度为1的向量（标量）
def use_svg_display(): 
# 用矢量图显示 
    display.set_matplotlib_formats('svg') 
def set_figsize(figsize=(3.5, 2.5)): #figsize大小为宽、长
    use_svg_display() 
    # 设置图的尺寸 
    plt.rcParams['figure.figsize'] = figsize 
set_figsize() 
plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1); # 加分号只显示图 
#将上面的plt作图函数以及use_svg_display函数和set_figsize函数定义在d2lzh包 里。以后在作图时，我们将直接调用d2lzh.plt。由于plt在d2lzh包中是一个全局变量，我们 在作图前只需要调用d2lzh.set_figsize()即可打印矢量图并设置图的尺寸
```
### 2.读取数据
读取数据 在训练模型的时候，我们需要遍历数据集并不断读取小批量数据样本。这里我们定义一个函数： 它每次返回batch_size（批量大小）个随机样本的特征和标签
```python
def data_iter(batch_size, features, labels): 
    num_examples = len(features) 
    indices = list(range(num_examples)) 
    random.shuffle(indices) # 样本的读取顺序是随机的 
    for i in range(0, num_examples, batch_size): 
        j = nd.array(indices[i: min(i + batch_size, num_examples)])
        yield features.take(j), labels.take(j) # take函数根据索引返回对应元素 
#每个批量的特征形状为(10, 2)，分别对应批量大小和输入个数；标签形状为批量大小
batch_size = 10 
for X, y in data_iter(batch_size, features, labels): 
    print(X, y) 
    break 
```
### 3.初始化以及定义模型
我们将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0。 
```python
w = nd.random.normal(scale=0.01, shape=(num_inputs, 1)) 
b = nd.zeros(shape=(1,))
#之后的模型训练中，需要对这些参数求梯度来迭代参数的值，因此我们需要创建它们的梯度。 
w.attach_grad() 
b.attach_grad() 

#线性回归的矢量计算表达式的实现。我们使用dot函数做矩阵乘法
def linreg(X, w, b): 
    # 本函数已保存在d2lzh包中方便以后使用 
    return nd.dot(X, w) + b 
```
### 4.定义损失函数
方损失来定义线性回归的损失函数。在实现中，我们需要把真实值y变形成预测值y_hat的形状。
以下函数返回的结果也将和y_hat的形状相同
```python
def squared_loss(y_hat, y): 
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 
```
### 5.优化损失函数
以下的sgd函数实现了上一节中介绍的小批量随机梯度下降算法。它通过不断迭代模型参数来优 化损失函数。这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量 大小来得到平均值
```python
def sgd(params, lr, batch_size): 
    for param in params: 
        param[:] = param - lr * param.grad / batch_size 
```

### 6.训练模型
在每次迭代中，我们根据当前读取的小批量数据样本（特征X和标签y），通过调用反向函数backward计算小批量随机梯度，并调用优化算法sgd迭代模型参数.在一个迭代周期（epoch）中，我们将完整遍历一遍data_iter函数，并对训练数据集中所有 样本都使用一次
```python
lr = 0.03 #学习率
num_epochs = 3  #迭代周期个数 迭代周期数设得越大模型可能越有效，但是训练时间可能过⻓
net = linreg # 线性回归 
loss = squared_loss 
for epoch in range(num_epochs): 
    # 训练模型一共需要num_epochs个迭代周期 
    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）
    # 和y分别是小批量样本的特征和标签 
    for X, y in data_iter(batch_size, features, labels): 
        with autograd.record(): 
            l = loss(net(X, w, b), y) # l是有关小批量X和y的损失 
        l.backward() # 小批量的损失对模型参数求梯度 
        sgd([w, b], lr, batch_size) # 使用小批量随机梯度下降迭代模型参数 
        train_l = loss(net(features, w, b), labels) 
        print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy())) 
```
## 线性回归的简洁实现
介绍如何使用MXNet提供的Gluon接口更方便地实现线性回归的训练
### 1.生成数据集
```python
#features是训练数据特征，labels是标签
from mxnet import autograd, nd 
num_inputs = 2 
num_examples = 1000 
true_w = [2, -3.4] 
true_b = 4.2 
features = nd.random.normal(scale=1, shape=(num_examples, num_inputs)) 
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b 
labels += nd.random.normal(scale=0.01, shape=labels.shape) 
```
### 2.读取数据
Gluon提供了data包来读取数据。由于data常用作变量名，我们将导入的data模块用添加了Gluon首字母的假名gdata代替。在每一次迭代中，我们将随机读取包含10个数据样本的小批量
```python
from mxnet.gluon import data as gdata 
batch_size = 10 
# 将训练数据的特征和标签组合 
dataset = gdata.ArrayDataset(features, labels) 
# 随机读取小批量 
data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True) 
```
### 3.定义模型且初始化模型参数 
```python
# 在Gluon中， Sequential实例可以看作是一个串联各个层的容器。在构造模型时，我们在该容器中依次添加 层。当给定输入数据时，容器中的每一层将依次计算并将输出作为下一层的输入。
from mxnet.gluon import nn net = nn.Sequential() 
# 作为一个单层神经网络，线性回归输出层中的神经元和输入层中各个输入完全连接。因此，线性回归的输出层叫全连接层。在Gluon中，全连接层是一个Dense实例。我们定义该层输出个数为1
net.add(nn.Dense(1)) 
#值得一提的是，在Gluon中我们无须指定每一层输入的形状，例如线性回归的输入个数。当模型 得到数据时，例如后面执行net(X)时，模型将自动推断出每一层的输入个数

#在使用net前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。我们从MXNet导入init模块。该模块提供了模型参数初始化的各种方法。这里的init是initializer的缩写形式。我们通过init.Normal(sigma=0.01)指定权重参数每个元素将在初始化时随机采样于均值为0、标准差为0.01的正态分布。偏差参数默认会初始化为零
from mxnet import init 
net.initialize(init.Normal(sigma=0.01)) 
```
### 4.定义损失函数 
在Gluon中，loss模块定义了各种损失函数。我们用假名gloss代替导入的loss模块，并直接 使用它提供的平方损失作为模型的损失函数。 
```python
from mxnet.gluon import loss as gloss 
loss = gloss.L2Loss() # 平方损失又称L2范数损失,L2范数是指向量各元素的平方和然后求平方根
```
### 5.定义优化算法
我们也无须实现小批量随机梯度下降。在导入Gluon后，我们创建一个Trainer实例，并 指定学习率为0.03的小批量随机梯度下降（sgd）为优化算法。该优化算法将用来迭代net实例所 有通过add函数嵌套的层所包含的全部参数。这些参数可以通过collect_params函数获取。
```python
from mxnet import gluon 
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03}) 
```
### 6.训练模型
在使用Gluon训练模型时， 我们通过调用Trainer实例的step函数来迭代模型参数。上一节中我们提到，由于变量l是⻓度为batch_size的一维NDArray，执行l.backward()等价于执行l.sum().backward()。按照小批量随机梯度下降的定义，我们在step函数中指明批量大小，从 而对批量中样本梯度求平均
```python
num_epochs = 3 
for epoch in range(1, num_epochs + 1): 
    for X, y in data_iter: 
        with autograd.record(): 
            l = loss(net(X), y) 
        l.backward() 
        trainer.step(batch_size) 
    l = loss(net(features), labels) 
    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy())) 

```