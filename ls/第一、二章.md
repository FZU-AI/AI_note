# 第一章 深度学习简介

## 	1.1起源

​		机器学习是通过大量的数据来反向推出函数以实现图像识别，语音识别等效果。

## 	1.4 特点

### 		1.4.1区别

​			（1）机器学习：是研究如何使计算机系统利⽤经验改善性能，它是⼈⼯智能领域的分⽀，也是实现⼈⼯智能的⼀种⼿段。

​			（2）深度学习：是一种具有多级表⽰的表征学习方法，为机器学习的一类，主要是通过函数将输入变换为正确的输出，逐级表示越来越抽象的概念和模型

​			深度学习相对机器学习不同在于：对⾮最优解的包容、对⾮凸⾮线性优化的使⽤，以及勇于尝试没有被证明过的⽅法。

# 第二章 预备知识

## 	2.2 数据操作

		x:	[[1. 1. 1. 1.]
	 	 	 [1. 1. 1. 1.]
	 	 	 [1. 1. 1. 1.]]
### 		2.2.2 运算

​			x.norm()：√（x1²+x2²+......+xn²）

​			输出：[3.4641016]

​			x.norm().asscalar()：将结果转为python的标量

​			输出：3.4641016

### 		2.2.4 索引

		c:	[[0. 1.]
			 [1. 2.]
	 		 [2. 3.]]
​			c[1:3]：输出第一行和第二行，起始为第零行

			[[1. 2.]
	 		 [2. 3.]]
​			c.transpose()：矩阵转置

​			c.transpose()[1:3].transpose()：输出第一列和第二列

			[[1.]
	 		 [2.]
	 		 [3.]]

### 		2.2.5 运算的内存开销

​			X[:] = X + Y 或者X += Y可减少运算的内存开销

### 		2.2.6 NDArray和NumPy相互变换

​			***看不出两者的具体区别和联系。。。***

## 	2.3 自动求梯度

```python
	x = nd.arange(4).reshape((4, 1))
	x
```

```
		[[0.]
 		 [1.]
 		 [2.]
 		 [3.]]
```

```python
	x.attach_grad() #申请存储梯度所需要的内存
	#调⽤record函数来要求MXNet记录与求梯度有关的计算
	with autograd.record(): 
        #dot(a,b)为两数组的点积,y=(0*0+1*1+2*2+3*3)*2
		y = 2 * nd.dot(x.T, x) 
    y.backward()
    #验证求出来的梯度是正确的
    assert (x.grad - 4 * x).norm().asscalar() == 0
	x.grad
```

### 		2.3.2 训练模式和预测模式

```python
	#默认情况autograd会将运⾏模式从预测模式转为训练模式
	with autograd.record():
		print(autograd.is_training())
```

```
	True
```

### 		2.3.3 对Python控制流求梯度

​			python控制流即if，while等语句

## 	2.4 查阅⽂档

​			dir函数可查看一个模块可调用的函数和类

```python
	from mxnet import nd
	print(dir(nd.random))
```

```python
	help(nd.ones_like) #可查看函数的具体用法
```

```python
	nd.ones_like? #同样可查看函数的具体用法，且结果会在新窗口中出现
```

