# 第三章 深度学习基础

## 3.1 线性回归

线性回归：单层神经网络，输出为一个连续值，输出单元为一个，适用于回归问题。

回归问题：如预测房屋价格、⽓温、销售额等连续值的问题。

softmax回归：单层神经网络，输出为一个离散值，输出单元为多个，适用于分类问题。

分类问题：如图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题

### 3.1.1 线性回归的基本要素

以预测房屋价格为例，只考虑面积（m²）和房龄（年）

#### **模型**

首先建立模型

x1：面积（m²）

x2：房龄（年）
$$
ŷ = x_1w_1 + x_2w_2 + b
$$
w1，w2为权重，b为偏差，均为标量，均为模型参数，ŷ是对真实价格y的预测。

#### **模型训练**

通过数据寻找模型参数值，以减小模型误差，其所涉及的3个元素如下：

##### （1）训练数据

训练数据集（training data set）：多栋房屋的真实售出价格和它们对应的⾯积和房龄

样本（sample）：房屋

标签（label）：房屋的真实售出价格

特征（feature）：房屋的面积和房龄

##### （2）损失函数

损失函数为衡量误差的函数。

平方损失函数：

① 索引为i的样本误差
$$
ℓ^{(i)}(w_1,w_2,b) =\frac{1}{2}(ŷ^{(i)}-y^{(i)})^{2}
$$
② 所有样本误差的平均
$$
ℓ(w_1,w_2,b)=\frac{1}{n}\sum_{i=1}^{n}{ℓ^{(i)}(w_1,w_2,b)}
$$
③ 使训练样本平均损失最小的一组模型参数
$$
w_1^{*},w_2^{*},b^{*}={\underset {w_1,w_2,b}{\operatorname {arg\,min} }}\ ℓ(w_1,w_2,b):
$$

##### （3）优化算法

解析解：模型和损失函数形式较简单，误差最小化问题的解可直接用公式表达出来。

数值解：只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。

小批量随机梯度下降：

在小批量B的范围内对参数进行多次迭代，以降低损失函数的值
$$
w_1 ← w_1-\frac{η}{|B|}\sum_{i\epsilon{B}}^{}\frac {\partial{ℓ^{(i)}(w_1,w_2,b)} } {\partial w_1 }=w_1-\frac{η}{|B|}\sum_{i\epsilon{B}}^{}x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})
$$

$$
w_2 ← w_2-\frac{η}{|B|}\sum_{i\epsilon{B}}^{}\frac {\partial{ℓ^{(i)}(w_1,w_2,b)} } {\partial w_2 }=w_2-\frac{η}{|B|}\sum_{i\epsilon{B}}^{}x_2^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})
$$

$$
b ← b-\frac{η}{|B|}\sum_{i\epsilon{B}}^{}\frac {\partial{ℓ^{(i)}(w_1,w_2,b)} } {\partial b }=w_1-\frac{η}{|B|}\sum_{i\epsilon{B}}^{}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})
$$

|B|代表每个小批量样本个数，η称作学习率并取正数，这两个参数人为设定，可通过“调参”来找到其合适的值

#### 模型预测

优先算法停止时获得的参数$w_1$，$w_2$，$b_1$的估计值，虽不是$w_1^{*}$，$w_2^{*}$，$b^{*}$，但也可以用这些参数所构成的模型来预测训练数据集以外房屋的价格。

### 3.1.2 线性回归的表示方法

#### 神经网络图

线性回归是一个单层神经网络

#### 矢量计算表达式

矩阵和向量
$$
矩阵：\left[         
\matrix{
  a_1 & a_4 & a_7\\
  a_2 & a_5 & a_8\\
  a_3 & a_6 & a_9\\
}\right]\
列向量：\left[         
\matrix{
  a_1 \\
  a_2 \\
  a_3 \\
}\right]\
行向量：\left[         
\matrix{
  a_1 
  a_2 
  a_3 \\
}\right]
$$
两个向量直接做矢量相加的速度更快，且numpy的矢量计算速度比nd快ndarray，matlab的矢量计算最慢。

```python
from mxnet import nd
from time import time
a = nd.ones(shape=1000)
b = nd.ones(shape=1000)
start = time()
d = a + b	#直接矢量相加
time() - start
#输出
0.00016546249389648438
```

```python
import numpy as np
from time import time
d = np.ones(shape=1000)
e = np.ones(shape=1000)
f=np.zeros(shape=1000)
start = time()
f = d + e
time() - start
#输出
8.940696716308594e-05
```

```matlab
a=ones(1000,1000);b=ones(1000,1000);tic;z=a+b;toc; t=toc
#输出
t =  0.0081279
```

对于线性回归，可以之前的房屋价格预测为例，将式子转化为矩阵来操作，当数据样本数为n，特征数为d时，线性回归的⽮量计算表达式为
$$
ŷ= Xw + b
$$
其中
$$
模型输出\ ŷ =\left[         
\matrix{
  ŷ^{(1)}\\
  ŷ^{(2)}\\
  ...\\
  ŷ^{(n)}
}\right]\
批量数据样本特征\ X = \left[         
\matrix{
  x_1^{(1)}&x_2^{(1)}&...&x_d^{(1)}\\
  x_1^{(2)}&x_2^{(2)}&...&x_d^{(2)}\\
  ...\\
  x_1^{(n)}&x_2^{(n)}&...&x_d^{(n)}
}\right]\
权重\ w=\left[         
\matrix{
  w_1\\
  w_2\\
  ...\\
  w_d
}\right]\
$$
平方损失函数也可重写为
$$
ℓ(θ) =\frac{1}{2n}(ŷ-y)^T(ŷ-y)
$$
因为这是矩阵运算，所以有n项，要再除以n

小批量随机梯度下降中三个模型参数的偏导数也可改为向量
$$
\frac {\partial{ℓ^{(i)}(w_1,w_2,b)} } {\partial w_1 }=x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})\\
\frac {\partial{ℓ^{(i)}(w_1,w_2,b)} } {\partial w_2 }=x_2^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})\\
\frac {\partial{ℓ^{(i)}(w_1,w_2,b)} } {\partial b }=(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})\\
\left[         
\matrix{
  \frac {\partial{ℓ^{(i)}(w_1,w_2,b)} } {\partial w_1 }\\
  \frac {\partial{ℓ^{(i)}(w_1,w_2,b)} } {\partial w_2 }\\
  \frac {\partial{ℓ^{(i)}(w_1,w_2,b)} } {\partial b }
}\right]=\left[         
\matrix{
  x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})\\
  x_2^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})\\
  (x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})
}\right]=\left[         
\matrix{
  x_1^{(i)}\\
  x_2^{(i)}\\
  1
}\right](ŷ^{(i)}-y^{(i)})
$$

## 3.2 线性回归的从零开始实现

根据模型
$$
y = Xw + b + ϵ
$$
ϵ：随机噪声项

构造⼀个简单的⼈⼯训练数据集。

```python
#将那些用matplotlib绘制的图显示在页面里而不是弹出一个窗口
%matplotlib inline
from IPython import display
from matplotlib import pyplot as plt
from mxnet import autograd, nd
import random

num_inputs = 2 #输入特征数为2
num_examples = 1000 #训练数据集样本数
true_w = [2, -3.4] #真实权重
true_b = 4.2 #偏差
#随机生成的正太分布标准差为1的1000x2样本特征矩阵
features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))
#依据公式y=Xw+b+ϵ生成标签
#features[:, a]表示features矩阵的第一列至第a列，
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += nd.random.normal(scale=0.01, shape=labels.shape)

def use_svg_display():
    # ⽤⽮量图显⽰
    display.set_matplotlib_formats('svg')
def set_figsize(figsize=(3.5, 2.5)):
    use_svg_display()
    # 设置图的尺⼨
    plt.rcParams['figure.figsize'] = figsize
set_figsize()
#第三个参数s=1为散点图中点的大小
plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1); # 加分号只显⽰图
```

输出第二个特征与标签的关系图

![散点图](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\散点图.png)

### 3.2.2 读取数据

用data_iter读取上述所有数据并随机返回batch_size个特征和对应标签

### 3.3.3 初始化模型参数

生成权重w和偏差b，并创建它们的梯度。

```python
w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))
b = nd.zeros(shape=(1,))
w.attach_grad()
b.attach_grad()
```

因为要对损失函数ℓ求w和b的偏导，所以要先用attach_grad()函数来求w和b的梯度。

### 3.2.4 定义模型

$$
y = Xw + b + ϵ
$$

```python
def linreg(X, w, b): 
	return nd.dot(X, w) + b #dot函数做矩阵乘法
```

### 3.2.5 定义损失函数

$$
ℓ(θ) =\frac{1}{2}(ŷ-y)^T(ŷ-y)
$$

```python
def squared_loss(y_hat, y): # y_hat为y的估计，即计算的结果，y为真实值
	return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 #x**2便是x²
```

### 3.2.6 定义优化算法

```python
def sgd(params, lr, batch_size): # lr为学习效率，由自己定值
	for param in params:
		param[:] = param - lr * param.grad / batch_size
```

### 3.2.7 训练模型

```python
lr = 0.03
num_epochs = 3 # 优化的迭代次数
net = linreg
loss = squared_loss
for epoch in range(num_epochs): # 训练模型⼀共需要num_epochs个迭代周期
								# 在每⼀个迭代周期中，会使⽤训练数据集中所有样本⼀次（假设								  # 样本数能够被批量⼤⼩整除）。X和y分别是⼩批量样本的特征和								 # 标签
	for X, y in data_iter(batch_size, features, labels):
		with autograd.record():
			l = loss(net(X, w, b), y) # l是有关⼩批量X和y的损失
			l.backward() # ⼩批量的损失对模型参数求梯度
		sgd([w, b], lr, batch_size) # 使⽤⼩批量随机梯度下降迭代模型参数
	train_l = loss(net(features, w, b), labels) # 计算损失
    # 输出每次迭代后的损失
	print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy())) 

输出：epoch 1, loss 0.040656
	 epoch 2, loss 0.000162
	 epoch 3, loss 0.000050
```

三次迭代过程中，损失在不断变小

### 练习

**•  为什么squared_loss函数中需要使⽤reshape函数？**

因为y_hat为(10,1)，而y为(1,10)

```python
y_hat:
[[ 4.599308  ]
 [ 2.4624949 ]
 [ 5.564998  ]
 [ 1.381691  ]
 [ 2.271425  ]
 [10.588075  ]
 [-0.4179983 ]
 [-0.92913437]
 [ 1.7362962 ]
 [ 1.3737757 ]]
<NDArray 10x1 @cpu(0)>
y:
[ 4.593405    2.447357    5.550131    1.4004962   2.2778223  10.583371
 -0.41123724 -0.9210661   1.7255168   1.3707838 ]
<NDArray 10 @cpu(0)>
```

**•  尝试使⽤不同的学习率，观察损失函数值的下降快慢。**

当lr≥0.03时，损失函数值在第三个周期都会降至0.000050。lr越小损失数值下降越快，但本身数值也越大。

```
lr=0.02
输出：epoch 1, loss 0.305448
	 epoch 2, loss 0.005925
	 epoch 3, loss 0.000166
lr=0.03
输出：epoch 1, loss 0.040656
	 epoch 2, loss 0.000162
	 epoch 3, loss 0.000050
lr=0.04
输出：epoch 1, loss 0.005319
	 epoch 2, loss 0.000051
	 epoch 3, loss 0.000050
lr=0.05
输出：epoch 1, loss 0.000704
	 epoch 2, loss 0.000050
	 epoch 3, loss 0.000050
```

**•  如果样本个数不能被批量⼤小整除，data_iter函数的⾏为会有什么变化？**

## 3.3 线性回归的简洁实现

使⽤MXNet提供的Gluon接口更⽅便地实现线性回归的训练，Gluon提供了⼤量预定义的层，这使
我们只需关注使⽤哪些层来构造模型，而无需像上一节那么复杂。

在Gluon中，data模块提供了有关数据处理的⼯具，nn模块定义了⼤量神经⽹络的层，
loss模块定义了各种损失函数。

MXNet的initializer模块提供了模型参数初始化的各种⽅法。

```python
from mxnet import autograd, nd
num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += nd.random.normal(scale=0.01, shape=labels.shape)

from mxnet.gluon import data as gdata
batch_size = 10
# 将训练数据的特征和标签组合
dataset = gdata.ArrayDataset(features, labels)
# 随机读取⼩批量
# shuffle=True，在每个epoch开始的时候，对数据进行重新打乱
data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)

for X, y in data_iter:
    print(X, y)
    break
    
from mxnet.gluon import nn
# Sequential实例可以看作是⼀个串联各个层的容器
# 在构造模型时，可在该容器中依次添加层
net = nn.Sequential()
# 线性回归输出层中的神经元和输⼊层中各个输⼊完全连接，即为全连接层，需定义一个Dense实例
# 定义该层输出个数为1
net.add(nn.Dense(1))

from mxnet import init
# sigma=0.01指定权重参数每个元素将在初始化时随机采样于均值为0、标准差为0.01的正态分布。偏差参数# 默认会初始化为零。
net.initialize(init.Normal(sigma=0.01))
from mxnet.gluon import loss as gloss
loss = gloss.L2Loss() # 平⽅损失⼜称L2范数损失

# 训练模型
num_epochs = 3
for epoch in range(1, num_epochs + 1):
	for X, y in data_iter:
		with autograd.record():
			l = loss(net(X), y)
		l.backward()
		trainer.step(batch_size)
	l = loss(net(features), labels)
	print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))
    
输出：
epoch 1, loss: 0.040574
epoch 2, loss: 0.000157
epoch 3, loss: 0.000051
        
dense = net[0]
true_w, dense.weight.data()

输出：
([2, -3.4],
 
 [[ 1.9996004 -3.3991477]]
 <NDArray 1x2 @cpu(0)>)

true_b, dense.bias.data()

输出：
(4.2,
 
 [4.199441]
 <NDArray 1 @cpu(0)>)
```

### 练习

**•  如果将l = loss(net(X), y)替换成l = loss(net(X), y).mean()， 我们需要将trainer.step(batch_size)相应地改成trainer.step(1)。这是为什么呢？**

因为step(batch_size)函数会根据输入的批量大小对样本梯度求平均，但由于loss(net(X), y).mean()已对损失求得平均值，所以batch_size需取1，即无需再求一次平均。

**•  查阅MXNet⽂档，看看gluon.loss和init模块⾥提供了哪些损失函数和初始化⽅法。**

https://mxnet.apache.org/api/python/docs/api/index.html

gluon.loss模块

| [`Loss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.Loss)(weight, batch_axis, **kwargs) | Base class for loss.                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`L2Loss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.L2Loss)([weight, batch_axis]) | Calculates the mean squared error between label and pred.    |
| [`L1Loss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.L1Loss)([weight, batch_axis]) | Calculates the mean absolute error between label and pred.   |
| [`SigmoidBinaryCrossEntropyLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.SigmoidBinaryCrossEntropyLoss)([…]) | The cross-entropy loss for binary classification.            |
| [`SigmoidBCELoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.SigmoidBCELoss) | The cross-entropy loss for binary classification.            |
| [`SoftmaxCrossEntropyLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss)([axis, …]) | Computes the softmax cross entropy loss.                     |
| [`SoftmaxCELoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.SoftmaxCELoss) | Computes the softmax cross entropy loss.                     |
| [`KLDivLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.KLDivLoss)([from_logits, axis, weight, …]) | The Kullback-Leibler divergence loss.                        |
| [`CTCLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.CTCLoss)([layout, label_layout, weight]) | Connectionist Temporal Classification Loss.                  |
| [`HuberLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.HuberLoss)([rho, weight, batch_axis]) | Calculates smoothed L1 loss that is equal to L1 loss if absolute error exceeds rho but is equal to L2 loss otherwise. |
| [`HingeLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.HingeLoss)([margin, weight, batch_axis]) | Calculates the hinge loss function often used in SVMs:       |
| [`SquaredHingeLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.SquaredHingeLoss)([margin, weight, batch_axis]) | Calculates the soft-margin loss function used in SVMs:       |
| [`LogisticLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.LogisticLoss)([weight, batch_axis, label_format]) | Calculates the logistic loss (for binary losses only):       |
| [`TripletLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.TripletLoss)([margin, weight, batch_axis]) | Calculates triplet loss given three input tensors and a positive margin. |
| [`PoissonNLLLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.PoissonNLLLoss)([weight, from_logits, …]) | For a target (Random Variable) in a Poisson distribution, the function calculates the Negative Log likelihood loss. |
| [`CosineEmbeddingLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.CosineEmbeddingLoss)([weight, batch_axis, margin]) | For a target label 1 or -1, vectors input1 and input2, the function computes the cosine distance between the vectors. |
| [`SDMLLoss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.SDMLLoss)([smoothing_parameter, weight, …]) | Calculates Batchwise Smoothed Deep Metric Learning (SDML) Loss given two input tensors and a smoothing weight SDM Loss learns similarity between paired samples by using unpaired samples in the minibatch as potential negative examples. |

mxnet.initializer模块

| [`Bilinear`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Bilinear)() | Initialize weight for upsampling layers.                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`Constant`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Constant)(value) | Initializes the weights to a given value.                    |
| [`InitDesc`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.InitDesc) | Descriptor for the initialization pattern.                   |
| [`Initializer`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Initializer)(**kwargs) | The base class of an initializer.                            |
| [`LSTMBias`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.LSTMBias)([forget_bias]) | Initialize all biases of an LSTMCell to 0.0 except for the forget gate whose bias is set to custom value. |
| [`Load`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Load)(param[, default_init, verbose]) | Initializes variables by loading data from file or dict.     |
| [`MSRAPrelu`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.MSRAPrelu)([factor_type, slope]) | Initialize the weight according to a MSRA paper.             |
| [`Mixed`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Mixed)(patterns, initializers) | Initialize parameters using multiple initializers.           |
| [`Normal`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Normal)([sigma]) | Initializes weights with random values sampled from a normal distribution with a mean of zero and standard deviation of sigma. |
| [`One`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.One)() | Initializes weights to one.                                  |
| [`Orthogonal`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Orthogonal)([scale, rand_type]) | Initialize weight as orthogonal matrix.                      |
| [`Uniform`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Uniform)([scale]) | Initializes weights with random values uniformly sampled from a given range. |
| [`Xavier`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Xavier)([rnd_type, factor_type, magnitude]) | Returns an initializer performing “Xavier” initialization for weights. |
| [`Zero`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Zero)() | Initializes weights to zero.                                 |

**•  如何访问dense.weight的梯度？**

有help(dense.weight)可知dense.weight.grad()能查看梯度

## 3.4 softmax回归

softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加，但不同在于输出个数有多个。

例如输入有4个特征，输出有3个类别，怎输出为：
$$
o_1 = x_1w_{11} + x_2w_{21} + x_3w_{31} + x_4w_{41} + b_1;\\
o_2 = x_1w_{12} + x_2w_{22} + x_3w_{32} + x_4w_{42} + b_2;\\
o_3 = x_1w_{13} + x_2w_{23} + x_3w_{33} + x_4w_{43} + b_3:
$$
softmax回归也是⼀个单层神经⽹络，也是全连接层

![softmax](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\softmax.png)

**softmax运算**

softmax运算符将所得所有输出按比例转化为和为1的概率分布

### 3.4.3 单样本分类的⽮量计算表达式

给定⼀个小批量样本，其批量⼤小为n，输⼊个数（特征数）为d，输出个数（类别数）为q

$$
权重\
W = \left[         
\matrix{
  w_{11} & w_{12} & w_{13}\\
  w_{21} & w_{22} & w_{23}\\
  w_{31} & w_{32} & w_{33}\\
  w_{41} & w_{42} & w_{43}
}\right]\
偏差\
b = \left[         
\matrix{
  b_1 &b_2 &b_3 
}\right]\
$$

$$
特征\
x^{(i)} = \left[         
\matrix{
  x_1^{(i)} &x_2^{(i)} &x_3^{(i)} &x_4^{(i)}
}\right]\\
输出\
o^{(i)}=\left[         
\matrix{
  o_1^{(i)} &o_2^{(i)} &o_3^{(i)}
}\right]\\
概率分布\ 
ŷ^{(i)}=\left[         
\matrix{
  ŷ^{(i)} &ŷ^{(i)} &ŷ^{(i)}
}\right]\\
$$

softmax回归对样本i分类的⽮量计算表达式为
$$
o^{(i)} = x^{(i)}W + b;\\
ŷ^{(i)} = softmax(o^{(i)}):
$$

### 3.4.5 交叉熵损失函数

因为softmax回归只会输出概率最大的结果，所以无需使用如平方损失函数这样过于严格的损失函数，而采用交叉熵损失函数。
$$
ℓ(Θ) =\frac{1}{n} H(y^{(i)},ŷ^{(i)}) = -\frac{1}{n}\sum_{j=1}^{q}y_j^{(i)}logŷ_j^{(i)}
$$
其中$y_j^{(i)}$非0即1，即让正确类别的预测值变大，因为只要其值⾜够⼤，就可以确保分类结果正确，而其他类别的预测值则变小。
ℓ(Θ)

### 练习

**•  查阅资料，了解最大似然估计。它与最小化交叉熵损失函数有哪些异曲同工之妙？**

## 3.5 图像分类数据集（Fashion-MNIST）

Fashion-MNIST是一个图像内容复杂的数据集

```python
%matplotlib inline
import d2lzh as d2l
from mxnet.gluon import data as gdata
import sys
import time

mnist_train = gdata.vision.FashionMNIST(train=True)
mnist_test = gdata.vision.FashionMNIST(train=False)
# feature对应⾼和宽均为28像素的图像,每个像素的数值为0到255之间8位⽆符号整数(uint8)
# 使用三维的NDArray存储，最后一维是通道数，因为获取的为灰度图，所以通道数为1
# 图像的标签使⽤NumPy的标量表⽰。它的类型为32位整数（int32）
feature, label = mnist_train[0]
```

**灰度图**：单通道，其实RGB三通道图片每个单独拿出来都是灰度图,每部分的 亮暗代表了该通道在RGB图像中颜色的深浅

RGB：三通道

二值图：单通道,每个像素点只能是0或255

### 3.5.2 读取小批量

ToTensor实例：可将图像数据从uint8格式变换成32位浮点数格式。

​							还可将图像通道从最后⼀维移到最前⼀维来⽅便卷积神经⽹络计算。

```python
batch_size = 256
# 获取ToTensor实例
transformer = gdata.vision.transforms.ToTensor()
# DataLoader实例允许使⽤多进程来加速数据读取，但不支持window系统
if sys.platform.startswith('win'):
	num_workers = 0 # 0表⽰不⽤额外的进程来加速读取数据
else:
	num_workers = 4
# transform_first函数将ToTensor的变换应⽤在每个数据样本（图像和标签）的第⼀个元素，即图像之上
train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),
							batch_size, shuffle=True,num_workers=num_workers)
test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),
							batch_size, shuffle=False,num_workers=num_workers)
```

### 练习

**• 减小batch_size（如到1）会影响读取性能吗？**

减小batch_size，读取速度会变慢

```python
start = time.time()
for X, y in train_iter:
	continue
'%.2f sec' % (time.time() - start)
# batch_size == 256
输出：'4.44 sec'
# batch_size == 128
输出：'4.99 sec'
# batch_size == 1
输出：'178.37 sec'
```

**• 非Windows用户请尝试修改num_workers来查看它对读取性能的影响。**

在2到4之间读取性能较佳，而继续往左减小或往右增加，读取性能会变差，所以要选择较合适的num_workers值

```python
# num_workers == 8
输出：'6.18 sec'
# num_workers == 4
输出：'4.44 sec'
# num_workers == 2
输出：'4.40 sec'
# num_workers == 1
输出：'6.52 sec'
```

**• 查阅MXNet文档，mxnet.gluon.data.vision⾥还提供了哪些别的数据集？**

##### Datasets

| [`Dataset`](https://mxnet.apache.org/api/python/docs/api/gluon/data/index.html#mxnet.gluon.data.Dataset) | Abstract dataset class.                                     |
| ------------------------------------------------------------ | ----------------------------------------------------------- |
| [`ArrayDataset`](https://mxnet.apache.org/api/python/docs/api/gluon/data/index.html#mxnet.gluon.data.ArrayDataset)(*args) | A dataset that combines multiple dataset-like objects, e.g. |
| [`RecordFileDataset`](https://mxnet.apache.org/api/python/docs/api/gluon/data/index.html#mxnet.gluon.data.RecordFileDataset)(filename) | A dataset wrapping over a RecordIO (.rec) file.             |
| [`SimpleDataset`](https://mxnet.apache.org/api/python/docs/api/gluon/data/index.html#mxnet.gluon.data.SimpleDataset)(data) | Simple Dataset wrapper for lists and arrays.                |

##### Sampling

| [`Sampler`](https://mxnet.apache.org/api/python/docs/api/gluon/data/index.html#mxnet.gluon.data.Sampler) | Base class for samplers.                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`SequentialSampler`](https://mxnet.apache.org/api/python/docs/api/gluon/data/index.html#mxnet.gluon.data.SequentialSampler)(length[, start]) | Samples elements from [start, start+length) sequentially.    |
| [`RandomSampler`](https://mxnet.apache.org/api/python/docs/api/gluon/data/index.html#mxnet.gluon.data.RandomSampler)(length) | Samples elements from [0, length) randomly without replacement. |
| [`BatchSampler`](https://mxnet.apache.org/api/python/docs/api/gluon/data/index.html#mxnet.gluon.data.BatchSampler)(sampler, batch_size[, last_batch]) | Wraps over another Sampler and return mini-batches of samples. |

**• 查阅MXNet文档，mxnet.gluon.data.vision.transforms还提供了哪些别的变换方法？**

##### vision.transforms

| [`transforms.Compose`](https://mxnet.apache.org/api/python/docs/api/gluon/data/vision/transforms/index.html#mxnet.gluon.data.vision.transforms.Compose) | Sequentially composes multiple transforms.                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`transforms.Cast`](https://mxnet.apache.org/api/python/docs/api/gluon/data/vision/transforms/index.html#mxnet.gluon.data.vision.transforms.Cast) | Cast inputs to a specific data type                          |
| `transforms.ToTensor`                                        | Converts an image NDArray or batch of image NDArray to a tensor NDArray. |
| `transforms.Normalize`                                       | Normalize an tensor of shape (C x H x W) or (N x C x H x W) with mean and standard deviation. |
| `transforms.RandomResizedCrop`                               | Crop the input image with random scale and aspect ratio.     |
| `transforms.CenterCrop`                                      | Crops the image src to the given size by trimming on all four sides and preserving the center of the image. |
| `transforms.Resize`                                          | Resize an image or a batch of image NDArray to the given size. |
| `transforms.RandomFlipLeftRight`                             | Randomly flip the input image left to right with a probability of p(0.5 by default). |
| `transforms.RandomFlipTopBottom`                             | Randomly flip the input image top to bottom with a probability of p(0.5 by default). |
| `transforms.RandomBrightness`                                | Randomly jitters image brightness with a factor chosen from [max(0, 1 - brightness), 1 + brightness]. |
| `transforms.RandomContrast`                                  | Randomly jitters image contrast with a factor chosen from [max(0, 1 - contrast), 1 + contrast]. |
| `transforms.RandomSaturation`                                | Randomly jitters image saturation with a factor chosen from [max(0, 1 - saturation), 1 + saturation]. |
| `transforms.RandomHue`                                       | Randomly jitters image hue with a factor chosen from [max(0, 1 - hue), 1 + hue]. |
| `transforms.RandomColorJitter`                               | Randomly jitters the brightness, contrast, saturation, and hue of an image. |
| `transforms.RandomLighting`                                  | Add AlexNet-style PCA-based noise to an image.               |

## 3.6 softmax回归的从零开始实现

```python
%matplotlib inline
import d2lzh as d2l
from mxnet import autograd, nd

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
# 每个样本输⼊是⾼和宽均为28像素的图像，模型的输⼊向量的长度是28*28 = 784
num_inputs = 784
# 10个分类
num_outputs = 10
W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs))
b = nd.zeros(num_outputs)
# 为什么要有梯度
W.attach_grad()
b.attach_grad()

# 矩阵X的⾏数是样本数，列数是输出个数
# softmax运算：将X转化为每⾏元素和为1且⾮负的矩阵
def softmax(X):
	X_exp = X.exp()
	partition = X_exp.sum(axis=1, keepdims=True)
	return X_exp / partition # 这⾥应⽤了⼴播机制

X = nd.random.normal(shape=(2, 5))
X_prob = softmax(X)

# 定义模型：通过reshape函数将每张原始图像改成长度为num_inputs的向量
def net(X):
	return softmax(nd.dot(X.reshape((-1, num_inputs)), W) + b)

# 交叉熵损失函数
# nd.pick(y_hat,y)：矩阵y中第i列的值为矩阵y_hat中第i行索引，pick函数可依次获取y_hat中的值
def cross_entropy(y_hat, y):
	return -nd.pick(y_hat, y).log()

# 计算分类准确率：即正确预测数量与总预测数量之⽐
def accuracy(y_hat, y):
    # y_hat.argmax(axis=1)返回矩阵y_hat每⾏中最⼤元素的索引
	return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()

def evaluate_accuracy(data_iter, net):
    acc_sum, n = 0.0, 0
    # X为图像，y为标签
    for X, y in data_iter:
        y = y.astype('float32')
        acc_sum += (net(X).argmax(axis=1) == y).sum().asscalar()
        n += y.size
    return acc_sum / n

# 因为net函数中的权重W为随机值，所以该模型的准确率应接近于类别个数10的倒数0.1
evaluate_accuracy(test_iter, net)
输出：0.0925

# 训练模型：与线性回归类似，也用小批量随机梯度下降来优化模型的损失函数
num_epochs, lr = 5, 0.1

def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,
		params=None, lr=None, trainer=None):
	for epoch in range(num_epochs):
		train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
		for X, y in train_iter:
			with autograd.record():
				y_hat = net(X)
				l = loss(y_hat, y).sum()
		l.backward()
		if trainer is None:
			d2l.sgd(params, lr, batch_size)
		else:
			trainer.step(batch_size) # “softmax回归的简洁实现”⼀节将⽤到
		y = y.astype('float32')
		train_l_sum += l.asscalar()
		train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()
		n += y.size
		test_acc = evaluate_accuracy(test_iter, net)
		print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'
			% (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))
        
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size,
			[W, b], lr)
输出：epoch 1, loss 1.6289, train acc 0.510, test acc 0.604
	 epoch 2, loss 1.2469, train acc 0.708, test acc 0.590
	 epoch 3, loss 1.4012, train acc 0.573, test acc 0.548
	 epoch 4, loss 1.3399, train acc 0.604, test acc 0.635
	 epoch 5, loss 1.3018, train acc 0.635, test acc 0.658
```

### 练习

**• 在本节中，我们直接按照softmax运算的数学定义来实现softmax函数。这可能会造成什么问题？（提示：试一试计算exp(50)的⼤小。）**

**• 本节中的cross_entropy函数是按照“softmax回归”⼀节中的交叉熵损失函数的数学定义实现的。这样的实现方式可能有什么问题？（提示：思考⼀下对数函数的定义域。）**

**• 你能想到哪些办法来解决上面的两个问题？**

## 3.7 softmax回归的简洁实现

```python
%matplotlib inline
import d2lzh as d2l
from mxnet import gluon, init
from mxnet.gluon import loss as gloss, nn

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

net = nn.Sequential()
# 添加一个输出个数为10的全连接层
net.add(nn.Dense(10))
# 使⽤均值为0、标准差为0.01的正态分布随机初始化模型的权重参数
net.initialize(init.Normal(sigma=0.01))
# softmax和交叉熵损失函数
loss = gloss.SoftmaxCrossEntropyLoss()
# 定义优化函数
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})

# 训练模型
num_epochs = 5
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,
None, trainer)
```

### 练习

**• 尝试调⼀调超参数，如批量大小、迭代周期和学习率，看看结果会怎样。**

```python
# batch_size==256,epochs==5,learning_rate==0.1
输出：epoch 1, loss 0.7881, train acc 0.747, test acc 0.805
	 epoch 2, loss 0.5739, train acc 0.810, test acc 0.822
	 epoch 3, loss 0.5285, train acc 0.824, test acc 0.832
	 epoch 4, loss 0.5048, train acc 0.830, test acc 0.836
	 epoch 5, loss 0.4892, train acc 0.835, test acc 0.835
```

迭代周期到4以后训练结果便趋于稳定

```python
# epochs==6
输出：epoch 1, loss 0.7902, train acc 0.745, test acc 0.802
	 epoch 2, loss 0.5750, train acc 0.811, test acc 0.824
	 epoch 3, loss 0.5287, train acc 0.824, test acc 0.832
	 epoch 4, loss 0.5056, train acc 0.830, test acc 0.837
	 epoch 5, loss 0.4902, train acc 0.835, test acc 0.840
	 epoch 6, loss 0.4776, train acc 0.838, test acc 0.844
# epochs==4
输出：epoch 1, loss 0.7904, train acc 0.743, test acc 0.807
	 epoch 2, loss 0.5741, train acc 0.812, test acc 0.823
	 epoch 3, loss 0.5286, train acc 0.824, test acc 0.829
	 epoch 4, loss 0.5044, train acc 0.830, test acc 0.835
# epochs==2
输出：epoch 1, loss 0.7873, train acc 0.747, test acc 0.800
	 epoch 2, loss 0.5740, train acc 0.812, test acc 0.823
```

## 3.8 多层感知机

多层感知机就是含有⾄少⼀个隐藏层的由全连接层组成的神经⽹络，且每个隐藏层的输出通过激
活函数进⾏变换。

## ![多层感知机](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\多层感知机.png)

给定批量⼤小为n，输⼊个数为d的小批量样本X，记隐藏层输出为H，输出层输出为O，ϕ表⽰激活函数
$$
H = ϕ(XW_h + b_h)\\
O = HW_o + b_o
$$
多层感知机可运用于线性回归和softmax回归问题中。

### 3.8.2 激活函数

激活函数有ReLu函数，sigmoid函数，tanh函数，均为非线性函数。

### 练习

**• 应⽤链式法则，推导出sigmoid函数和tanh函数的导数的数学表达式。**

**• 查阅资料，了解其他的激活函数。**

Leaky ReLU函数（PReLU）

Relu的输入值为负的时候，输出始终为0，会导致神经元不学习，为了解决Relu函数这个缺点，在Relu函数的负半区间引入一个泄露（Leaky）值，所以称为Leaky Relu函数。

该函数输出对负值输入有很小的坡度，允许基于梯度的学习（虽然会很慢），解决了Relu函数进入负区间后，导致神经元不学习的问题。
$$
PReLU(x) = max(0.01x; 0):
$$
![PReLu函数](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\PReLu函数.png)

## 3.9 多层感知机的从零开始实现

与之前“softmax回归的从零开始实现“类似

```python
%matplotlib inline
import d2lzh as d2l
from mxnet import nd
from mxnet.gluon import loss as gloss

# 获取Fashion-MNIST数据集
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# 定义模型参数
num_inputs, num_outputs, num_hiddens = 784, 10, 256
W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))
b1 = nd.zeros(num_hiddens)
W2 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_outputs))
b2 = nd.zeros(num_outputs)
params = [W1, b1, W2, b2]
for param in params:
	param.attach_grad()

# 定义relu函数
def relu(X):
	return nd.maximum(X, 0)

# 定义模型
def net(X):
	X = X.reshape((-1, num_inputs))
	H = relu(nd.dot(X, W1) + b1)
	return nd.dot(H, W2) + b2

# 定义损失函数
# 仍使用包括softmax运算和交叉熵损失计算的函数
loss = gloss.SoftmaxCrossEntropyLoss()

# 训练模型
num_epochs, lr = 5, 0.5
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,
			params, lr)

输出：epoch 1, loss 0.8195, train acc 0.694, test acc 0.829
	 epoch 2, loss 0.5028, train acc 0.815, test acc 0.826
	 epoch 3, loss 0.4373, train acc 0.838, test acc 0.852
	 epoch 4, loss 0.4006, train acc 0.852, test acc 0.848
	 epoch 5, loss 0.3776, train acc 0.860, test acc 0.872
```

### 练习

**•  改变超参数num_hiddens的值，看看对实验结果有什么影响。**

当num_hiddens值过小时会严重影响实验结果

```python
# num_hiddens==128
输出：epoch 1, loss 0.8269, train acc 0.691, test acc 0.825
	 epoch 2, loss 0.4987, train acc 0.815, test acc 0.848
	 epoch 3, loss 0.4393, train acc 0.837, test acc 0.849
	 epoch 4, loss 0.4040, train acc 0.852, test acc 0.857
	 epoch 5, loss 0.3838, train acc 0.857, test acc 0.867
# num_hiddens==512
输出：epoch 1, loss 0.7995, train acc 0.706, test acc 0.826
	 epoch 2, loss 0.4843, train acc 0.822, test acc 0.853
	 epoch 3, loss 0.4211, train acc 0.846, test acc 0.861
	 epoch 4, loss 0.3905, train acc 0.855, test acc 0.857
	 epoch 5, loss 0.3673, train acc 0.864, test acc 0.874
# num_hiddens==10000
输出：epoch 1, loss 0.7526, train acc 0.733, test acc 0.840
	 epoch 2, loss 0.4540, train acc 0.833, test acc 0.856
	 epoch 3, loss 0.4015, train acc 0.852, test acc 0.870
	 epoch 4, loss 0.3705, train acc 0.863, test acc 0.873
	 epoch 5, loss 0.3470, train acc 0.873, test acc 0.881
# num_hiddens==1
输出：epoch 1, loss 2.0127, train acc 0.189, test acc 0.245
	 epoch 2, loss 2.0164, train acc 0.200, test acc 0.197
	 epoch 3, loss 1.9881, train acc 0.216, test acc 0.186
	 epoch 4, loss 1.9711, train acc 0.225, test acc 0.100
	 epoch 5, loss 1.9517, train acc 0.231, test acc 0.332
        
```

**•  试着加入一个新的隐藏层，看看对实验结果有什么影响。**

加入一个新的隐藏层会严重影响实验结果

```python
# 定义模型参数
num_inputs, num_outputs, num_hiddens = 784, 10, 256
W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))
b1 = nd.zeros(num_hiddens)
W2 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_hiddens))
b2 = nd.zeros(num_hiddens)
W3 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_outputs))
b3 = nd.zeros(num_outputs)
params = [W1, b1, W2, b2, W3, b3]
for param in params:
	param.attach_grad()

# 定义relu函数
def relu(X):
	return nd.maximum(X, 0)

# 定义模型
def net(X):
	X = X.reshape((-1, num_inputs))
	H1 = relu(nd.dot(X, W1) + b1)
	H2 = relu(nd.dot(H1, W2) + b2)
	return nd.dot(H2, W2) + b2

输出：epoch 1, loss 2.3351, train acc 0.289, test acc 0.527
	 epoch 2, loss 1.4924, train acc 0.416, test acc 0.568
	 epoch 3, loss 1.0691, train acc 0.589, test acc 0.694
	 epoch 4, loss 0.7117, train acc 0.727, test acc 0.750
	 epoch 5, loss 0.5919, train acc 0.786, test acc 0.808
```

## 3.10 多层感知机的简洁实现

使⽤Gluon来更简洁地实现上⼀节中的多层感知机

```python
import d2lzh as d2l
from mxnet import gluon, init
from mxnet.gluon import loss as gloss, nn

net = nn.Sequential()
# 多添加了⼀个全连接层作为隐藏层
net.add(nn.Dense(256, activation='relu'),
				nn.Dense(10))
# 使⽤均值为0、标准差为0.01的正态分布随机初始化模型的权重参数
net.initialize(init.Normal(sigma=0.01))

# 训练模型
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
loss = gloss.SoftmaxCrossEntropyLoss()
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})
num_epochs = 5
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,
				None, trainer)

输出：epoch 1, loss 0.8416, train acc 0.688, test acc 0.814
	 epoch 2, loss 0.5000, train acc 0.812, test acc 0.841
	 epoch 3, loss 0.4297, train acc 0.842, test acc 0.836
	 epoch 4, loss 0.4016, train acc 0.853, test acc 0.864
	 epoch 5, loss 0.3771, train acc 0.862, test acc 0.870

```

### 练习

**• 尝试多加入几个隐藏层，对比上一节中从零开始的实现。**

多tian'jia添加隐藏层会影响结果，但影响程度没上一节大

```python
# 多添加一层隐藏层
输出：epoch 1, loss 1.1787, train acc 0.540, test acc 0.793
	 epoch 2, loss 0.5645, train acc 0.786, test acc 0.831
	 epoch 3, loss 0.4706, train acc 0.825, test acc 0.857
	 epoch 4, loss 0.4164, train acc 0.846, test acc 0.855
	 epoch 5, loss 0.3901, train acc 0.856, test acc 0.865
        
# 多添加两层隐藏层
输出：epoch 1, loss 1.9275, train acc 0.236, test acc 0.593
	 epoch 2, loss 1.0240, train acc 0.604, test acc 0.774
	 epoch 3, loss 0.7077, train acc 0.733, test acc 0.787
	 epoch 4, loss 0.5187, train acc 0.808, test acc 0.841
	 epoch 5, loss 0.4669, train acc 0.829, test acc 0.842
```

**• 使用其他的激活函数，看看对结果的影响。**

tanh函数和relu函数的结果较好，sigmoid函数的结果略差一点

```python
# tanh函数
输出：epoch 1, loss 0.7850, train acc 0.711, test acc 0.817
	 epoch 2, loss 0.5261, train acc 0.806, test acc 0.832
	 epoch 3, loss 0.4646, train acc 0.830, test acc 0.851
	 epoch 4, loss 0.4319, train acc 0.841, test acc 0.857
	 epoch 5, loss 0.4096, train acc 0.851, test acc 0.862
# sigmoid函数
输出：epoch 1, loss 1.0584, train acc 0.608, test acc 0.774
	 epoch 2, loss 0.5753, train acc 0.786, test acc 0.803
	 epoch 3, loss 0.5075, train acc 0.814, test acc 0.834
	 epoch 4, loss 0.4719, train acc 0.829, test acc 0.843
	 epoch 5, loss 0.4468, train acc 0.837, test acc 0.849
```

## 3.11 模型选择、欠拟合和过拟合

### 3.11.1 训练误差和泛化误差

训练误差：指模型在训练数据集上表现出的误差

泛化误差：指模型在任意⼀个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似

模型的参数是通过在训练数据集上训练模型而学习出的，这样得到的训练误差无法从中估计泛化误差，一味降低训练误差并不意味着泛化误差一定会降低

机器学习模型应关注降低泛化误差

### 3.11.2 模型选择

**验证数据集**

因无法通过训练误差来估计泛化误差，又不能根据测试数据来选择模型，所以我们可以预留⼀部分在训练数据集和测试数据集以外的数据来进⾏模型选择，即测试数据集。

**K折交叉验证**

将训练数据集分为k个不重合的子数据集，一个为验证集，其他k-1个为训练集，并进行k次训练，每次训练的验证集不同。最后得到k个训练误差和泛化误差分别求其平均

### 3.11.3 欠拟合和过拟合

欠拟合：模型⽆法得到较低的训练误差。

过拟合：是模型的训练误差远小于它在测试数据集上的误差。

可从两个因素进行讨论

**模型复杂度**

如果模型的复杂度过低，很容易出现⽋拟合；如果模型复杂度过⾼，很容易出现过拟合。

![模型复杂度](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\模型复杂度.png)

**训练数据集大小**

当训练数据集的样本过少会容易发生过拟合，泛化误差不会随训练数据集样本数增加而增大。

在计算资源允许的范围之内，我们通常希望训练数据集⼤⼀些，特别是在模型复杂度较⾼时。

### 3.11.4 多项式函数拟合实验

以多项式函数拟合为例来实验，以理解模型复杂度和训练数据集⼤小对⽋拟合和过拟合的影响。

使⽤如下的三阶多项式函数来⽣成该样本的标签：
$$
y = 1.2x - 3.4x^2 + 5.6x^3 + 5 + ϵ
$$
噪声项ϵ服从均值为0、标准差为0.1的正态分布

```python
%matplotlib inline
import d2lzh as d2l
from mxnet import autograd, gluon, nd
from mxnet.gluon import data as gdata, loss as gloss, nn

n_train, n_test, true_w, true_b = 100, 100, [1.2, -3.4, 5.6], 5
# 线性模型
features = nd.random.normal(shape=(n_train + n_test, 1))
# nd.concat(x,y,z):默认dim=1，按列拼接矩阵x,y,z，若dim=0，则按行拼接矩阵
# 三阶多项式模型
poly_features = nd.concat(features, nd.power(features, 2),
							nd.power(features, 3))
labels = (true_w[0] * poly_features[:, 0] + true_w[1] * poly_features[:, 1]
						+ true_w[2] * poly_features[:, 2] + true_b)
labels += nd.random.normal(scale=0.1, shape=labels.shape)

# 本函数已保存在d2lzh包中⽅便以后使⽤
def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,
                legend=None, figsize=(3.5, 2.5)):
    d2l.set_figsize(figsize)
	d2l.plt.xlabel(x_label)
	d2l.plt.ylabel(y_label)
	d2l.plt.semilogy(x_vals, y_vals)
	if x2_vals and y2_vals:
		d2l.plt.semilogy(x2_vals, y2_vals, linestyle=':')
		d2l.plt.legend(legend)
    
# 多项式函数拟合的训练和测试，与之前的softmax回归中的相关步骤类似
num_epochs, loss = 100, gloss.L2Loss()
def fit_and_plot(train_features, test_features, train_labels, test_labels):
    net = nn.Sequential()
    net.add(nn.Dense(1))
    net.initialize()
    batch_size = min(10, train_labels.shape[0])
    train_iter = gdata.DataLoader(gdata.ArrayDataset(
        train_features, train_labels), batch_size, shuffle=True)
    trainer = gluon.Trainer(net.collect_params(), 'sgd',
                                    {'learning_rate': 0.01})
    train_ls, test_ls = [], []
    for _ in range(num_epochs):
        for X, y in train_iter:
            with autograd.record():
                l = loss(net(X), y)
            l.backward()
            trainer.step(batch_size)
        train_ls.append(loss(net(train_features),
                    train_labels).mean().asscalar())
        test_ls.append(loss(net(test_features),
                    test_labels).mean().asscalar())
    print('final epoch: train loss', train_ls[-1], 'test loss', test_ls[-1])
    semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',
                range(1, num_epochs + 1), test_ls, ['train', 'test'])
    print('weight:', net[0].weight.data().asnumpy(),
                '\nbias:', net[0].bias.data().asnumpy())
```

```python
# 前100行为训练数据集的特征和标签，后100行为测试数据集的特征和标签
# 三阶多项式函数拟合，模型复杂度较高
fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :],
				labels[:n_train], labels[n_train:])
```

![三阶模型的拟合程度](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\三阶模型的拟合程度.png)

```python
# 线性函数拟合，模型复杂度较低
fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train],
				labels[n_train:])
```

出现欠拟合现象

![线性函数拟合](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\线性函数拟合.png)

```python
# 模型还是三阶多项式函数，但样本不足
fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2],
			labels[n_train:])
```

出现过拟合现象

![样本不足的拟合过程](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\样本不足的拟合过程.png)

### 练习

**•  如果用⼀个三阶多项式模型来拟合⼀个线性模型⽣成的数据，可能会有什么问题？为什么？**

**•  在本节提到的三阶多项式拟合问题⾥，有没有可能把100个样本的训练误差的期望降到0，为**
**什么？（提示：考虑噪声项的存在。）**

## 3.12 权重衰减

权重衰减通过惩罚绝对值较⼤的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。

### 3.12.1 方法

权重衰减等价于L2范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出
的模型参数值较小，是应对过拟合的常⽤⼿段。

带有L2范数惩罚项的新损失函数为：
$$
\frac{1}{n}\sum_{i=1}^{n}{ℓ^{(i)}(w_1,w_2,b)}+\frac{λ}{2n}∥w∥^2\\
∥w∥^2=w_1^2+w_2^2
$$
超参数λ > 0。当权重参数均为0时，惩罚项最小。当λ较⼤时，惩罚项在损失函数中的⽐重较
⼤，这通常会使学到的权重参数的元素较接近0。当λ设为0时，惩罚项完全不起作⽤。

### 3.12.2 高维线性回归实验

高维线性函数：
$$
y=0.05+\sum_{i=1}^{p}{0.01x_i+ϵ}
$$

```python
def fit_and_plot_gluon(wd):
	net = nn.Sequential()
	net.add(nn.Dense(1))
	net.initialize(init.Normal(sigma=1))
	# 对权重参数衰减。权重名称⼀般是以weight结尾
	trainer_w = gluon.Trainer(net.collect_params('.*weight'), 'sgd',
			{'learning_rate': lr, 'wd': wd})
	# 不对偏差参数衰减。偏差名称⼀般是以bias结尾
	trainer_b = gluon.Trainer(net.collect_params('.*bias'), 'sgd',
			{'learning_rate': lr})
	train_ls, test_ls = [], []
	for _ in range(num_epochs):
		for X, y in train_iter:
			with autograd.record():
				l = loss(net(X), y)
			l.backward()
			# 对两个Trainer实例分别调⽤step函数，从⽽分别更新权重和偏差
			trainer_w.step(batch_size)
			trainer_b.step(batch_size)
		train_ls.append(loss(net(train_features),
						train_labels).mean().asscalar())
		test_ls.append(loss(net(test_features),
						test_labels).mean().asscalar())
	d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',
						range(1, num_epochs + 1), test_ls, ['train', 'test'])
	print('L2 norm of w:', net[0].weight.data().norm().asscalar())
```

```
fit_and_plot_gluon(0)
```

![权重衰减-无惩罚项](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\权重衰减-无惩罚项.png)

```
fit_and_plot_gluon(3)
```

![权重衰减-有惩罚项](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\权重衰减-有惩罚项.png)



### 小结

可以定义多个Trainer实例对不同的模型参数使⽤不同的迭代⽅法。

### 练习

**•  回顾⼀下训练误差和泛化误差的关系。除了权重衰减、增⼤训练量以及使⽤复杂度合适的**
**模型，你还能想到哪些办法来应对过拟合？**

**•  如果你了解贝叶斯统计，你觉得权重衰减对应贝叶斯统计里的哪个重要概念？**

**•  调节实验中的权重衰减超参数，观察并分析实验结果。**

## 3.13 丢弃法

在单隐藏层的多层感知机中，设输入个数为4，隐藏单元个数为5，则隐藏单元
$$
h_i = ϕ (x_1w_{1i} + x_2w_{2i} + x_3w_{3i} + x_4w_{4i} + b_i) ;
$$
ϕ为激活函数。

### 3.13.1 方法

设该隐藏单元层的单元有p的概率被丢弃，1-p概率被保留，则
$$
h_i^{'}=\frac{μ}{1-p}h_i
$$
μ为0和1的概率分别为p和1-p。

由于隐藏层每个单元都有相同几率p被丢弃，从而使输出层的计算不再过度依赖某个单元$h_i$，从而在训练模型时起到和正则化一样的效果，但该方法一般不用于测试模型。

```python
import d2lzh as d2l
from mxnet import autograd, gluon, init, nd
from mxnet.gluon import loss as gloss, nn

# 一般靠近输入层的丢弃概率较低
drop_prob1, drop_prob2 = 0.2, 0.5

net = nn.Sequential()
net.add(nn.Dense(256, activation="relu"),
		nn.Dropout(drop_prob1), # 在第⼀个全连接层后添加丢弃层
		nn.Dense(256, activation="relu"),
		nn.Dropout(drop_prob2), # 在第⼆个全连接层后添加丢弃层
		nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))

num_epochs, lr, batch_size = 5, 0.5, 256
loss = gloss.SoftmaxCrossEntropyLoss()
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,
None, trainer)

输出：epoch 1, loss 1.1691, train acc 0.551, test acc 0.783
	 epoch 2, loss 0.6006, train acc 0.780, test acc 0.831
	 epoch 3, loss 0.5053, train acc 0.816, test acc 0.834
	 epoch 4, loss 0.4561, train acc 0.835, test acc 0.842
	 epoch 5, loss 0.4270, train acc 0.844, test acc 0.857
```

### 练习

**•  如果把本节中的两个丢弃概率超参数对调，会有什么结果？**
**•  增大迭代周期数，比较使用丢弃法与不使用丢弃法的结果。**
**•  如果将模型改得更加复杂，如增加隐藏层单元，使用丢弃法应对过拟合的效果是否更加明**
**显？**
**•  以本节中的模型为例，比较使用丢弃法与权重衰减的效果。如果同时使用丢弃法和权重衰**
**减，效果会如何？**

## 3.14 正向传播、反向传播和计算图

正向传播沿着从输⼊层到输出层的顺序，依次计算并存储神经⽹络的中间变量。

反向传播沿着从输出层到输⼊层的顺序，依次计算并存储神经⽹络中间变量和参数的梯度。

在训练深度学习模型时，正向传播和反向传播相互依赖。

因为反向传播需要用到正向传播计算的中间变量，这一复用导致中间变量不能马上被释放，所以训练要⽐预测占⽤更多内存。中间变量的个数⼤体上与⽹络层数线性相关，每个变量的⼤小跟批量⼤小和输⼊个数也是线性相关的。

### 练习

**•  在本节样例模型的隐藏层和输出层中添加偏差参数，修改计算图以及正向传播和反向传播**
**的数学表达式。**

## 3.15 数值稳定性和模型初始化

### 3.15.1 衰减和爆炸

在假设神经网络有30层，激活函数为恒等映射$ϕ(x) = x$，不考虑偏差参数，权重参数为标量的情况下，输出
$$
H^{(30)}=XW^{30}
$$
若W=0.2，则$W^{30}≈1×10^{-21}$（衰减）

若W=5，则$W^{30}≈9×10^{20}$（爆炸）

### 3.15.2 随机初始化模型参数

在多层感知机中，若同一隐藏层的所有单元，其参数初始化的值一样，则这所有单元等价于只有一个隐藏单元，因为多层感知机全连接。

随机初始化参数的方法有：

**MXNet的默认随机初始化**

```python
net.initialize(init.Normal(sigma=0.01))
```

**Xavier随机初始化**

### 练习

**•  有⼈说随机初始化模型参数是为了“打破对称性”。这⾥的“对称”应如何理解？**

**•  是否可以将线性回归或softmax回归中所有的权重参数都初始化为相同值？**