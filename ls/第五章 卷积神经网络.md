# 第五章 卷积神经网络

## 5.1 二维卷积层

二维卷积层有高和宽两个空间维度，常⽤来处理图像数据。

### 5.1.1 二维互相关运算

![二位卷积层互相关运算](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\二位卷积层互相关运算.png)
$$
0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 = 19;\\
1 × 0 + 2 × 1 + 4 × 2 + 5 × 3 = 25;\\
3 × 0 + 4 × 1 + 6 × 2 + 7 × 3 = 37;\\
4 × 0 + 5 × 1 + 7 × 2 + 8 × 3 = 43:
$$
特征图：输出

感受野：输⼊中阴影部分的四个元素是输出中阴影部分元素的感受野。

核与另⼀个形状为2 × 2的核数组做互相关运算，输出单个元素z。z在核 上的感受野包括核的全部四个
元素，在输⼊上的感受野包括其中全部9个元素。所以我们可以通过更深的卷积神经⽹络使特征图中单个元素的感受野变得更加⼴阔，从而捕捉输⼊上更⼤尺⼨的特征。

```python
from mxnet import autograd, nd
from mxnet.gluon import nn
def corr2d(X, K): 
	h, w = K.shape
	Y = nd.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
	for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
			Y[i, j] = (X[i: i + h, j: j + w] * K).sum()
	return Y
```

### 5.1.2 二维卷积层

```python
# ⾃定义的⼆维卷积层
# ⼆维卷积层将输⼊和卷积核做互相关运算，并加上⼀个标量偏差来得到输出
class Conv2D(nn.Block):
	def __init__(self, kernel_size, **kwargs):
		super(Conv2D, self).__init__(**kwargs)
		self.weight = self.params.get('weight', shape=kernel_size)
		self.bias = self.params.get('bias', shape=(1,))
        
	def forward(self, x):
		return corr2d(x, self.weight.data()) + self.bias.data()
```

### 5.1.3 图像中物体边缘检测

```python
# 构造⼀张6×8的图像，中间4列为⿊（0），其余为⽩
X = nd.ones((6, 8))
X[:, 2:6] = 0
X
# 构造⼀个⾼和宽分别为1和2的卷积核K。当它与输⼊做互相关运算时，如果横向相邻元素相同，输出为0；
# 否则输出为⾮0。
K = nd.array([[1, -1]])
Y = corr2d(X, K)
Y
```

```python
# out
[[1. 1. 0. 0. 0. 0. 1. 1.]
[1. 1. 0. 0. 0. 0. 1. 1.]
[1. 1. 0. 0. 0. 0. 1. 1.]
[1. 1. 0. 0. 0. 0. 1. 1.]
[1. 1. 0. 0. 0. 0. 1. 1.]
[1. 1. 0. 0. 0. 0. 1. 1.]]
<NDArray 6x8 @cpu(0)>
[[ 0. 1. 0. 0. 0. -1. 0.]
[ 0. 1. 0. 0. 0. -1. 0.]
[ 0. 1. 0. 0. 0. -1. 0.]
[ 0. 1. 0. 0. 0. -1. 0.]
[ 0. 1. 0. 0. 0. -1. 0.]
[ 0. 1. 0. 0. 0. -1. 0.]]
<NDArray 6x7 @cpu(0)>
```

### 5.1.4 通过数据学习核数组

我们根据上一小节的输入和输出来训练出核数组K

```python
# 构造⼀个输出通道数为1（将在“多输⼊通道和多输出通道”⼀节介绍通道），核数组形状是(1, 2)的⼆
# 维卷积层
conv2d = nn.Conv2D(1, kernel_size=(1, 2))
conv2d.initialize()
# ⼆维卷积层使⽤4维输⼊输出，格式为(样本, 通道, ⾼, 宽)，这⾥批量⼤⼩（批量中的样本数）和通
# 道数均为1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
for i in range(10):
	with autograd.record():
		Y_hat = conv2d(X)
		l = (Y_hat - Y) ** 2
		l.backward()
	# 简单起见，这⾥忽略了偏差
	conv2d.weight.data()[:] -= 3e-2 * conv2d.weight.grad()
	if (i + 1) % 2 == 0:
		print('batch %d, loss %.3f' % (i + 1, l.sum().asscalar()))
```

```python
conv2d.weight.data().reshape((1, 2))
# out
[[ 0.9895 -0.9873705]]
<NDArray 1x2 @cpu(0)>
```

可以看到训练出来的核数组和上一小节的核数组[1，-1]很接近

### 5.1.5 互相关运算和卷积运算

卷积运算：为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输⼊数组做互相关运算。

卷积层⽆论使⽤互相关运算或卷积运算都不影响模型预测时的输出。

### 练习

**• 构造⼀个输入图像X，令它有水平方向的边缘。如何设计卷积核K来检测图像中水平边缘？**
**如果是对角方向的边缘呢？**

**• 试着对我们自己构造的Conv2D类进行自动求梯度，会有什么样的错误信息？在该类**
**的forward函数里，将corr2d函数替换成nd.Convolution类使得自动求梯度变得可行。**

**• 如何通过变化输⼊和核数组将互相关运算表示成⼀个矩阵乘法？**

**• 如何构造⼀个全连接层来进行物体边缘检测？**

## 5.2 填充和步幅

由上一节可总结出，假设输⼊形状是$n_h × n_w$，卷积核窗口形状是$k_h × k_w$，那么输出形状将会是
$$
(n_h - k_h + 1) × (n_w - k_w + 1)
$$

### 5.2.1 填充

填充（padding）是指在输⼊⾼和宽的两侧填充元素（通常是0元素）。![二维卷层填充](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\二维卷积层填充.png)

<center>在输入的高和宽两侧分别填充了0元素的二维互相关计算</center>

```python
from mxnet import nd
from mxnet.gluon import nn
# 定义⼀个函数来计算卷积层。它初始化卷积层权重，并对输⼊和输出做相应的升维和降维
def comp_conv2d(conv2d, X):
    conv2d.initialize()
    # (1, 1)代表批量⼤⼩和通道数（“多输⼊通道和多输出通道”⼀节将介绍）均为1
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    return Y.reshape(Y.shape[2:]) # 排除不关⼼的前两维：批量和通道
```

```python
# 通道数为1，核数组行列为3，输入行列两侧各填充1
conv2d = nn.Conv2D(1, kernel_size=3, padding=(1,1))
X = nd.random.uniform(shape=(8, 8))
comp_conv2d(conv2d, X).shape
```

### 5.2.2 步幅

卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。我们将每次滑动的⾏数和列数称为步幅（stride）。![互相关运算步幅](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\互相关运算步幅.png)

<center>在⾼上步幅为3、在宽上步幅为2的⼆维互相关运算</center>

当⾼上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为
$$
⌊(n_h - k_h + p_h + s_h)/s_h⌋ × ⌊(n_w - k_w + p_w + s_w)/s_w⌋
$$

```python
# 令⾼和宽上的步幅均为2，从而使输⼊的⾼和宽减半
conv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2)
comp_conv2d(conv2d, X).shape
```

### 练习

**• 对本节最后⼀个例子通过形状计算公式来计算输出形状，看看是否和实验结果⼀致。**

**• 在本节实验中，试⼀试其他的填充和步幅组合。**

### 5.3 多输入通道和多输出通道

彩⾊图像除了在⾼和宽2个维度外还有RGB（红、绿、蓝）3个颜⾊通道，可表示为3×h×w的多维数组，其中⼤小为3的这⼀维称为通道（channel）维。

### 5.3.1 多输入通道

当输⼊数据含多个通道时，我们需要构造⼀个与输⼊数据的通道数相同的卷积核。

当通道数$c_i>1$，我们将会为每个输⼊通道各分配⼀个形状为$k_h×k_w$的核数组。![多通道互相关计算](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\多通道互相关计算.png)

```python
import d2lzh as d2l
from mxnet import nd
def corr2d_multi_in(X, K):
    # ⾸先沿着X和K的第0维（通道维）遍历。然后使⽤*将结果列表变成add_n函数的位置参数
    # （positional argument）来进⾏相加
    return nd.add_n(*[d2l.corr2d(x, k) for x, k in zip(X, K)])

X = nd.array([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],
[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])
K = nd.array([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])
corr2d_multi_in(X, K)

# out
[[ 56. 72.]
[104. 120.]]
<NDArray 2x2 @cpu(0)>
```

### 5.3.2 多输出通道

如果希望得到含多个通道的输出，可以创建一个形状为$c_o × c_i × k_h × k_w$的卷积核，$c_i$为输入通道数，$c_o$为输出通道数。

```python
def corr2d_multi_in_out(X, K):
	# 对K的第0维遍历，每次同输⼊X做互相关计算。所有结果使⽤stack函数合并在⼀起
	return nd.stack(*[corr2d_multi_in(X, k) for k in K])
# 三个核数组K,K+1,K+2构造⼀个输出通道数为3的卷积核
K = nd.stack(K, K + 1, K + 2)
K
```

```python
# out
[[[[0. 1.]
   [2. 3.]]

  [[1. 2.]
   [3. 4.]]]

 [[[1. 2.]
   [3. 4.]]

  [[2. 3.]
   [4. 5.]]]

 [[[2. 3.]
   [4. 5.]]

  [[3. 4.]
   [5. 6.]]]]
<NDArray 3x2x2x2 @cpu(0)>
```

```python
corr2d_multi_in_out(X, K)
```

```python
# out
[[[ 56. 72.]
[104. 120.]]
[[ 76. 100.]
[148. 172.]]
[[ 96. 128.]
[192. 224.]]]
<NDArray 3x2x2 @cpu(0)>
```

### 5.3.3 1 × 1卷积层

1 × 1卷积失去了卷积层可以识别⾼和宽维度上相邻元素构成的模式的功能。

1×1卷积核的互相关计算，其输⼊和输出具有相同的⾼和宽，假设我们将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那么1 × 1卷积层的作⽤与全连接层等价。![1×1卷积层互相关运算](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\1×1卷积层互相关运算.png)

```python
def corr2d_multi_in_out_1x1(X, K):
	c_i, h, w = X.shape
	c_o = K.shape[0]
	X = X.reshape((c_i, h * w))
	K = K.reshape((c_o, c_i))
	Y = nd.dot(K, X) # 全连接层的矩阵乘法
	return Y.reshape((c_o, h, w))

X = nd.random.uniform(shape=(3, 3, 3))
K = nd.random.uniform(shape=(2, 3, 1, 1))
Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out(X, K)
# 1e-6表示的是1*10的-6次方，它是浮点数，浮点数的小数只能精确到小数点的后六位，这个语句是判断
# (Y1 - Y2).norm().asscalar()是否等于0的
(Y1 - Y2).norm().asscalar() < 1e-6

# out
True
```

1 × 1卷积层被当作保持⾼和宽维度形状不变的全连接层使⽤。于是，我们可以通过调整⽹络层之间的通道数来控制模型复杂度。

### 练习

**•  假设输⼊形状为$c_i×h×w$，且使用形状为$c_o×c_i×k_h×k_w$、填充为($p_h, p_w$)、步幅为($s_h, s_w$)的**
**卷积核。那么这个卷积层的前向计算分别需要多少次乘法和加法？**

**•  翻倍输⼊通道数$c_i$和输出通道数$c_o$会增加多少倍计算？翻倍填充呢？**

**•  如果卷积核的高和宽$k_h$ = $k_w$ = 1，能减少多少计算？**

**•  本节最后⼀个例子中的变量$Y_1$和$Y_2$完全⼀致吗？原因是什么？**

**•  当卷积窗口不为1 × 1时，如何用矩阵乘法实现卷积计算？**

## 5.4 池化层

池化层的提出是为了缓解卷积层对位置的过度敏感性。

### 5.4.1 二维最大池化层和平均池化层

不同于卷积层⾥计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最⼤值或者平均
值。

在对实际图像进行边缘检测时，我们感兴趣的物体不会总出现在固定位置，这会导致同⼀个边缘对应的输出可能出现在卷积输出Y中的不同位置，进而对后⾯的模式识别造成不便。可池化层可缓解卷积层对位置的过度敏感性。如下最大池化层，⽆论是X[i, j]和X[i, j+1]值不同，还是X[i, j+1]和X[i, j+2]不同，池化层输出均有Y[i, j]=1。

![池化窗口形状为2×2的最⼤池化](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\池化窗口形状为2×2的最⼤池化.png)
$$
max(0; 1; 3; 4) = 4,\\
max(1; 2; 4; 5) = 5,\\
max(3; 4; 6; 7) = 7,\\
max(4; 5; 7; 8) = 8.
$$

```python
from mxnet import nd
from mxnet.gluon import nn
def pool2d(X, pool_size, mode='max'):
	p_h, p_w = pool_size
	Y = nd.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
	for i in range(Y.shape[0]):
		for j in range(Y.shape[1]):
			if mode == 'max':
				Y[i, j] = X[i: i + p_h, j: j + p_w].max()
			elif mode == 'avg':
				Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
	return Y
```

### 5.4.2 填充和步幅

```python
X = nd.arange(16).reshape((1, 1, 4, 4))
X
# out
[[[[ 0. 1. 2. 3.]
   [ 4. 5. 6. 7.]
   [ 8. 9. 10. 11.]
   [12. 13. 14. 15.]]]]
<NDArray 1x1x4x4 @cpu(0)>

# 默认获得形状为(3, 3)的步幅
pool2d = nn.MaxPool2D(3)
pool2d(X)
# out
[[[[10.]]]]
<NDArray 1x1x1x1 @cpu(0)>

# 也可手动指定步幅和填充
pool2d = nn.MaxPool2D((2, 3), padding=(1, 2), strides=(2, 3))
pool2d(X)
# out
[[[[ 0. 3.]
   [ 8. 11.]
   [12. 15.]]]]
<NDArray 1x1x3x2 @cpu(0)>
```

### 5.4.3 多通道

在处理多通道输⼊数据时，池化层的输出通道数与输⼊通道数相等。

### 练习

**• 分析池化层的计算复杂度。假设输入形状为$c×h×w$，我们使用形状为$p_h ×p_w$的池化窗口，**
**而且使用$(p_h, p_w)$填充和$(s_h, s_w)$步幅。这个池化层的前向计算复杂度有多大？**

**• 想一想，最大池化层和平均池化层在作用上可能有哪些区别？**

**• 你觉得最小池化层这个想法有没有意义？**

## 5.5 卷积神经网络（LeNet）

在“多层感知机的从零开始实现”一节里分类图片时，是先要将宽高28像素图片转为长784的向量，这会导致存储开销过大，所以改用卷积神经网络来对图片进行分类。

### 5.5.1 LeNet模型

LeNet分为卷积层块和全连接层块两个部分。

**卷积层块**

卷积层块里的基本单位是卷积层后接最大池化层：卷积层块来识别图像里的空间模式，之后的最大池化层则用来降低卷积层对位置的敏感性。

**全连接层块**

```python
import d2lzh as d2l
import mxnet as mx
from mxnet import autograd, gluon, init, nd
from mxnet.gluon import loss as gloss, nn
import time
net = nn.Sequential()
net.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'),
		nn.MaxPool2D(pool_size=2, strides=2),
		nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),
		nn.MaxPool2D(pool_size=2, strides=2),
		# Dense会默认将(批量大小, 通道, 高, 宽)形状的输入转换成
		# (批量大小, 通道* 高* 宽)形状的输⼊
		nn.Dense(120, activation='sigmoid'),
		nn.Dense(84, activation='sigmoid'),
		nn.Dense(10))
```

