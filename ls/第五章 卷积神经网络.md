# 第五章 卷积神经网络

## 5.1 二维卷积层

二维卷积层有高和宽两个空间维度，常用来处理图像数据。

### 5.1.1 二维互相关运算

![二位卷积层互相关运算](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\二位卷积层互相关运算.png)
$$
0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 = 19;\\
1 × 0 + 2 × 1 + 4 × 2 + 5 × 3 = 25;\\
3 × 0 + 4 × 1 + 6 × 2 + 7 × 3 = 37;\\
4 × 0 + 5 × 1 + 7 × 2 + 8 × 3 = 43:
$$
特征图：输出

感受野：输入中阴影部分的四个元素是输出中阴影部分元素的感受野。

核与另一个形状为2 × 2的核数组做互相关运算，输出单个元素z。z在核 上的感受野包括核的全部四个
元素，在输入上的感受野包括其中全部9个元素。所以我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输如上更大尺寸的特征。

```python
from mxnet import autograd, nd
from mxnet.gluon import nn
# 用该函数实现上述过程
def corr2d(X, K): 
	h, w = K.shape
	Y = nd.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
	for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
			Y[i, j] = (X[i: i + h, j: j + w] * K).sum()
	return Y
```

### 5.1.2 二维卷积层

```python
# 自定义的二维卷积层
# 二维卷积层将输入和卷积核做互相关运算，并加上加个标量偏差来得到输出
class Conv2D(nn.Block):
	def __init__(self, kernel_size, **kwargs):
		super(Conv2D, self).__init__(**kwargs)
		self.weight = self.params.get('weight', shape=kernel_size)
		self.bias = self.params.get('bias', shape=(1,))
        
	def forward(self, x):
		return corr2d(x, self.weight.data()) + self.bias.data()
```

### 5.1.3 图像中物体边缘检测

```python
# 构造一张6×8的图像，中间4列为黑（0），其余为白
X = nd.ones((6, 8))
X[:, 2:6] = 0
X
# 构造一个高和宽分别为1和2的卷积核K。当它与输入做互相关运算时，如果横向相邻元素相同，输出为0；
# 否则输出为为0。
K = nd.array([[1, -1]])
Y = corr2d(X, K)
Y
```

```python
# out
[[1. 1. 0. 0. 0. 0. 1. 1.]
[1. 1. 0. 0. 0. 0. 1. 1.]
[1. 1. 0. 0. 0. 0. 1. 1.]
[1. 1. 0. 0. 0. 0. 1. 1.]
[1. 1. 0. 0. 0. 0. 1. 1.]
[1. 1. 0. 0. 0. 0. 1. 1.]]
<NDArray 6x8 @cpu(0)>
[[ 0. 1. 0. 0. 0. -1. 0.]
[ 0. 1. 0. 0. 0. -1. 0.]
[ 0. 1. 0. 0. 0. -1. 0.]
[ 0. 1. 0. 0. 0. -1. 0.]
[ 0. 1. 0. 0. 0. -1. 0.]
[ 0. 1. 0. 0. 0. -1. 0.]]
<NDArray 6x7 @cpu(0)>
```

### 5.1.4 通过数据学习核数组

我们根据上一小节的输入和输出来训练出核数组K

```python
# 构造一个输出通道数为1，核数组形状是(1, 2)的
# 二维卷积层
conv2d = nn.Conv2D(1, kernel_size=(1, 2))
conv2d.initialize()
# 二维卷积层使用4维输入输出，格式为(样本, 通道, 高, 宽)，这里批量大小（批量中的样本数）和通
# 道数均为1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
for i in range(10):
	with autograd.record():
		Y_hat = conv2d(X)
		l = (Y_hat - Y) ** 2
		l.backward()
	# 简单起见，这里忽略了偏差
	conv2d.weight.data()[:] -= 3e-2 * conv2d.weight.grad()
	if (i + 1) % 2 == 0:
		print('batch %d, loss %.3f' % (i + 1, l.sum().asscalar()))
```

```python
conv2d.weight.data().reshape((1, 2))
# out
[[ 0.9895 -0.9873705]]
<NDArray 1x2 @cpu(0)>
```

可以看到训练出来的核数组和上一小节的核数组[1，-1]很接近

### 5.1.5 互相关运算和卷积运算

卷积运算：为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。

卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。

### 练习

**• 构造一个输入图像X，令它有水平方向的边缘。如何设计卷积核K来检测图像中水平边缘？**
**如果是对角方向的边缘呢？**

**• 试着对我们自己构造的Conv2D类进行自动求梯度，会有什么样的错误信息？在该类**
**的forward函数里，将corr2d函数替换成nd.Convolution类使得自动求梯度变得可行。**

**• 如何通过变化输⼊和核数组将互相关运算表示成⼀个矩阵乘法？**

**• 如何构造一个全连接层来进行物体边缘检测？**

## 5.2 填充和步幅

由上一节可总结出，假设输入形状是$n_h × n_w$，卷积核窗口形状是$k_h × k_w$，那么输出形状将会是
$$
(n_h - k_h + 1) × (n_w - k_w + 1)
$$

### 5.2.1 填充

填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。![二维卷层填充](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\二维卷积层填充.png)

<center>在输入的高和宽两侧分别填充了0元素的二维互相关计算</center>

```python
from mxnet import nd
from mxnet.gluon import nn
# 定义一个函数来计算卷积层。它初始化卷积层权重，并对输⼊和输出做相应的升维和降维
def comp_conv2d(conv2d, X):
    conv2d.initialize()
    # (1, 1)代表批量大小和通道数均为1
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    return Y.reshape(Y.shape[2:]) # 排除不关⼼的前两维：批量和通道
```

```python
# 通道数为1，核数组行列为3，输入行列两侧各填充1
conv2d = nn.Conv2D(1, kernel_size=3, padding=(1,1))
X = nd.random.uniform(shape=(8, 8))
comp_conv2d(conv2d, X).shape
```

### 5.2.2 步幅

卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。我们将每次滑动的⾏数和列数称为步幅（stride）。![互相关运算步幅](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\互相关运算步幅.png)

<center>在高上步幅为3、在宽上步幅为2的二维互相关运算</center>

当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为
$$
⌊(n_h - k_h + p_h + s_h)/s_h⌋ × ⌊(n_w - k_w + p_w + s_w)/s_w⌋
$$

```python
# 令高和宽上的步幅均为2，从而使输入的高和宽减半
conv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2)
comp_conv2d(conv2d, X).shape
```

### 练习

**• 对本节最后一个例子通过形状计算公式来计算输出形状，看看是否和实验结果一致。**

**• 在本节实验中，试一试其他的填充和步幅组合。**

### 5.3 多输入通道和多输出通道

彩色图像除了在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道，可表示为3×h×w的多维数组，其中大小为3的这一维称为通道（channel）维。

### 5.3.1 多输入通道

当输入数据含多个通道时，我们需要构造一个与输入数据的通道数相同的卷积核。

当通道数$c_i>1$，我们将会为每个输入通道各分配一个形状为$k_h×k_w$的核数组。![多通道互相关计算](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\多通道互相关计算.png)

```python
import d2lzh as d2l
from mxnet import nd
def corr2d_multi_in(X, K):
    # 首先沿着X和K的第0维（通道维）遍历。然后使用*将结果列表变成add_n函数的位置参数
    # （positional argument）来进行相加
    return nd.add_n(*[d2l.corr2d(x, k) for x, k in zip(X, K)])

X = nd.array([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],
[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])
K = nd.array([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])
corr2d_multi_in(X, K)

# out
[[ 56. 72.]
[104. 120.]]
<NDArray 2x2 @cpu(0)>
```

### 5.3.2 多输出通道

如果希望得到含多个通道的输出，可以创建一个形状为$c_o × c_i × k_h × k_w$的卷积核，$c_i$为输入通道数，$c_o$为输出通道数。

```python
def corr2d_multi_in_out(X, K):
	# 对K的第0维遍历，每次同输入X做互相关计算。所有结果使用stack函数合并在一起
	return nd.stack(*[corr2d_multi_in(X, k) for k in K])
# 三个核数组K,K+1,K+2构造一个输出通道数为3的卷积核
K = nd.stack(K, K + 1, K + 2)
K
```

```python
# out
[[[[0. 1.]
   [2. 3.]]

  [[1. 2.]
   [3. 4.]]]

 [[[1. 2.]
   [3. 4.]]

  [[2. 3.]
   [4. 5.]]]

 [[[2. 3.]
   [4. 5.]]

  [[3. 4.]
   [5. 6.]]]]
<NDArray 3x2x2x2 @cpu(0)>
```

```python
corr2d_multi_in_out(X, K)
```

```python
# out
[[[ 56. 72.]
[104. 120.]]
[[ 76. 100.]
[148. 172.]]
[[ 96. 128.]
[192. 224.]]]
<NDArray 3x2x2 @cpu(0)>
```

### 5.3.3 1 × 1卷积层

1 × 1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。

1×1卷积核的互相关计算，其输入和输出具有相同的高和宽，假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1 × 1卷积层的作用与全连接层等价。![1×1卷积层互相关运算](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\1×1卷积层互相关运算.png)

```python
def corr2d_multi_in_out_1x1(X, K):
	c_i, h, w = X.shape
	c_o = K.shape[0]
	X = X.reshape((c_i, h * w))
	K = K.reshape((c_o, c_i))
	Y = nd.dot(K, X) # 全连接层的矩阵乘法
	return Y.reshape((c_o, h, w))

X = nd.random.uniform(shape=(3, 3, 3))
K = nd.random.uniform(shape=(2, 3, 1, 1))
Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out(X, K)
# 1e-6表示的是1*10的-6次方，它是浮点数，浮点数的小数只能精确到小数点的后六位，这个语句是判断
# (Y1 - Y2).norm().asscalar()是否等于0的
(Y1 - Y2).norm().asscalar() < 1e-6

# out
True
```

1 × 1卷积层被当作保持高和宽维度形状不变的全连接层使用。于是，我们可以通过调整网络层之间的通道数来控制模型复杂度。

### 练习

**•  假设输⼊形状为$c_i×h×w$，且使用形状为$c_o×c_i×k_h×k_w$、填充为($p_h, p_w$)、步幅为($s_h, s_w$)的**
**卷积核。那么这个卷积层的前向计算分别需要多少次乘法和加法？**

**•  翻倍输⼊通道数$c_i$和输出通道数$c_o$会增加多少倍计算？翻倍填充呢？**

**•  如果卷积核的高和宽$k_h$ = $k_w$ = 1，能减少多少计算？**

**•  本节最后一个例子中的变量$Y_1$和$Y_2$完全⼀致吗？原因是什么？**

**•  当卷积窗口不为1 × 1时，如何用矩阵乘法实现卷积计算？**

## 5.4 池化层

池化层的提出是为了缓解卷积层对位置的过度敏感性。

### 5.4.1 二维最大池化层和平均池化层

不同于卷积层的计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均
值。

在对实际图像进行边缘检测时，我们感兴趣的物体不会总出现在固定位置，这会导致同一个边缘对应的输出可能出现在卷积输出Y中的不同位置，进而对后面的模式识别造成不便。可池化层可缓解卷积层对位置的过度敏感性。如下最大池化层，无论是X[i, j]和X[i, j+1]值不同，还是X[i, j+1]和X[i, j+2]不同，池化层输出均有Y[i, j]=4。

![池化窗口形状为2×2的最⼤池化](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\池化窗口形状为2×2的最⼤池化.png)
```python
from mxnet import nd
from mxnet.gluon import nn
def pool2d(X, pool_size, mode='max'):
	p_h, p_w = pool_size
	Y = nd.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
	for i in range(Y.shape[0]):
		for j in range(Y.shape[1]):
			if mode == 'max':
				Y[i, j] = X[i: i + p_h, j: j + p_w].max()
			elif mode == 'avg':
				Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
	return Y
```

### 5.4.2 填充和步幅

```python
X = nd.arange(16).reshape((1, 1, 4, 4))
X
# out
[[[[ 0. 1. 2. 3.]
   [ 4. 5. 6. 7.]
   [ 8. 9. 10. 11.]
   [12. 13. 14. 15.]]]]
<NDArray 1x1x4x4 @cpu(0)>

# 使用形状为(3, 3)的池化窗口，默认获得形状为(3, 3)的步幅
pool2d = nn.MaxPool2D(3)
pool2d(X)
# out
[[[[10.]]]]
<NDArray 1x1x1x1 @cpu(0)>

# 也可手动指定stride步幅和padding填充
pool2d = nn.MaxPool2D((2, 3), padding=(1, 2), strides=(2, 3))
pool2d(X)
# out
[[[[ 0. 3.]
   [ 8. 11.]
   [12. 15.]]]]
<NDArray 1x1x3x2 @cpu(0)>
```

### 5.4.3 多通道

在处理多通道输入数据时，池化层的输出通道数与输入通道数相等。

### 练习

- **分析池化层的计算复杂度。假设输入形状为$c×h×w$，我们使用形状为$p_h ×p_w$的池化窗口，**
  **而且使用$(p_h, p_w)$填充和$(s_h, s_w)$步幅。这个池化层的前向计算复杂度有多大？**
- **想一想，最大池化层和平均池化层在作用上可能有哪些区别？**
- **你觉得最小池化层这个想法有没有意义？**

## 5.5 卷积神经网络（LeNet）

在“多层感知机的从零开始实现”一节里分类图片时，是先要将宽高28像素图片转为长784的向量，这会导致存储开销过大，所以改用卷积神经网络来对图片进行分类。

### 5.5.1 LeNet模型

LeNet分为卷积层块和全连接层块两个部分。

**卷积层块**

卷积层块里的基本单位是卷积层后接最大池化层：卷积层块来识别图像里的空间模式，之后的最大池化层则用来降低卷积层对位置的敏感性。

**全连接层块**

全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。

**LeNet**

```python
import d2lzh as d2l
import mxnet as mx
from mxnet import autograd, gluon, init, nd
from mxnet.gluon import loss as gloss, nn
import time
net = nn.Sequential()
net.add(# 通道数为6，形状为（5，5）的二维卷积层，加入sigmoid激活函数
    	nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'),
    	# 形状为（2，2）步幅为2的最大池化层
		nn.MaxPool2D(pool_size=2, strides=2),
		nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),
		nn.MaxPool2D(pool_size=2, strides=2),
		# 全连接层Dense会默认将(批量大小, 通道, 高, 宽)形状的输入转换成
		# (批量大小, 通道* 高* 宽)形状的输出
		nn.Dense(120, activation='sigmoid'),
		nn.Dense(84, activation='sigmoid'),
		nn.Dense(10))

# 逐层进行前向计算来查看每个层的输出形状。
X = nd.random.uniform(shape=(1, 1, 28, 28))
net.initialize()
for layer in net:
	X = layer(X)
	print(layer.name, 'output shape:\t', X.shape)

# out
conv0 output shape: (1, 6, 24, 24)
pool0 output shape: (1, 6, 12, 12)
conv1 output shape: (1, 16, 8, 8)
pool1 output shape: (1, 16, 4, 4)
dense0 output shape: (1, 120)
dense1 output shape: (1, 84)
dense2 output shape: (1, 10)
```

### 5.5.2 获取数据和训练模型

```python
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)

# 尝试使用GPU加速计算
def try_gpu(): 
	try:
		ctx = mx.gpu()
		_ = nd.zeros((1,), ctx=ctx)
	except mx.base.MXNetError:
		ctx = mx.cpu()
	return ctx

# 计算准确率
def evaluate_accuracy(data_iter, net, ctx):
	acc_sum, n = nd.array([0], ctx=ctx), 0
	for X, y in data_iter:
		# 如果ctx代表GPU及相应的显存，将数据复制到显存上
		X, y = X.as_in_context(ctx), y.as_in_context(ctx).astype('float32')
		acc_sum += (net(X).argmax(axis=1) == y).sum()
		n += y.size
	return acc_sum.asscalar() / n

def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs):
	print('training on', ctx)
	loss = gloss.SoftmaxCrossEntropyLoss()
	for epoch in range(num_epochs):
		train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()
		for X, y in train_iter:
			X, y = X.as_in_context(ctx), y.as_in_context(ctx)
			with autograd.record():
				y_hat = net(X)
				l = loss(y_hat, y).sum()
			l.backward()
            # 设置批量大小
			trainer.step(batch_size)
			y = y.astype('float32')
			train_l_sum += l.asscalar()
			train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()
			n += y.size
			print("y.size:",y.size)
			print("n:",n)
		test_acc = evaluate_accuracy(test_iter, net, ctx)
        # train_l_sum/n表示平均损失
		print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '
			'time %.1f sec'
		% (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,
			time.time() - start))

ctx = try_gpu()
lr, num_epochs = 0.9, 5
# 非首次对模型初始化需要指定force_reinit为真
# init.Xavier()随机初始化权重
net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())
# collect_params函数来获取net变量所有嵌套（例如通过add函数嵌套）的层所包含的所有参数
# 'sgd'为小批量随机梯度下降优化算法
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)

# out
epoch 1, loss 2.3218, train acc 0.099, test acc 0.100, time 27.2 sec
epoch 2, loss 1.8478, train acc 0.289, test acc 0.594, time 24.9 sec
epoch 3, loss 0.9376, train acc 0.630, test acc 0.708, time 24.1 sec
epoch 4, loss 0.7356, train acc 0.711, test acc 0.744, time 25.3 sec
epoch 5, loss 0.6590, train acc 0.738, test acc 0.755, time 24.9 sec
```

### 练习

- **尝试基于LeNet构造更复杂的网络来提高分类准确率。例如，调整卷积窗口大小、输出通道**
  **数、激活函数和全连接层输出个数。在优化方面，可以尝试使用不同的学习率、初始化方**
  **法以及增加迭代周期。**

### 5.6 深度卷积神经网络（AlexNet）

#### 5.6.2 AlexNet

**AlexNet与LeNet**

1. 与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以
   及1个全连接输出层。
2. AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。
3. AlexNet通过丢弃法来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。
4. AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。

```python
import d2lzh as d2l
from mxnet import gluon, init, nd
from mxnet.gluon import data as gdata, nn
import os
import sys

# AlexNet
net = nn.Sequential()
# 使用较大的11 x 11窗口来捕获物体。同时使用步幅4来较大幅度减小输出高和宽。这里使用的输出通
# 道数比LeNet中的也要大很多
net.add(nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
        nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        # 连续3个卷积层，且使用更大的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。
        # 前两个卷积层后不使用池化层来减小输入的高和宽
        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
        nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        # 这里全连接层的输出个数为LeNet中的大数倍。使用丢弃层来缓解过拟合
        nn.Dense(4096, activation="relu"), nn.Dropout(0.5),
        nn.Dense(4096, activation="relu"), nn.Dropout(0.5),
        # 输出层。由于这里使⽤用Fashion-MNIST，所以用类别数为10，而非论文中的1000
        nn.Dense(10))
X = nd.random.uniform(shape=(1, 1, 224, 224))
net.initialize()
for layer in net:
X = layer(X)
print(layer.name, 'output shape:\t', X.shape)

# out
conv0 output shape: (1, 96, 54, 54)
pool0 output shape: (1, 96, 26, 26)
conv1 output shape: (1, 256, 26, 26)
pool1 output shape: (1, 256, 12, 12)
conv2 output shape: (1, 384, 12, 12)
conv3 output shape: (1, 384, 12, 12)
conv4 output shape: (1, 256, 12, 12)
pool2 output shape: (1, 256, 5, 5)
dense0 output shape: (1, 4096)
dropout0 output shape: (1, 4096)
dense1 output shape: (1, 4096)
dropout1 output shape: (1, 4096)
dense2 output shape: (1, 10)
    
# 读取数据
def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(
'~', '.mxnet', 'datasets', 'fashion-mnist')):
	root = os.path.expanduser(root) # 展开用户路径'~'
	transformer = []
	if resize:
        # 先通过Resize实例将图像高和宽扩大为224
		transformer += [gdata.vision.transforms.Resize(resize)]
    # 再调用ToTensor实例，并用Compose实例将两个变换串联
	transformer += [gdata.vision.transforms.ToTensor()]
	transformer = gdata.vision.transforms.Compose(transformer)
	mnist_train = gdata.vision.FashionMNIST(root=root, train=True)
	mnist_test = gdata.vision.FashionMNIST(root=root, train=False)
    # DataLoader实例允许使用多进程来加速数据读取，但不支持window系统
	num_workers = 0 if sys.platform.startswith('win32') else 4
    # transform_first函数将ToTensor的变换应用在每个数据样本（图像和标签）的第一个元素，即图	# 像之上
	train_iter = gdata.DataLoader(
		mnist_train.transform_first(transformer), batch_size, shuffle=True,
        num_workers=num_workers)
	test_iter = gdata.DataLoader(
		mnist_test.transform_first(transformer), batch_size, shuffle=False,
		num_workers=num_workers)
	return train_iter, test_iter

batch_size = 128
# 如出现“out of memory”的报错信息，可减小batch_size或resize
train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)

# 训练
lr, num_epochs, ctx = 0.01, 5, d2l.try_gpu()
net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)
```

### 练习

- 尝试增加迭代周期。跟LeNet的结果相比，AlexNet的结果有什么区别？为什么？
- AlexNet对Fashion-MNIST数据集来说可能过于复杂。试着简化模型来使训练更快，同时保
  证准确率不明显下降。
- 修改批量大小，观察准确率和内存或显存的变化。

### 5.7 使用重复元素的网络（VGG）

VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。

#### 5.7.1 VGG块

连续使用数个相同的填充为1、窗口形状为3 × 3的卷积层后接上一个步幅为2、窗口形状为2 × 2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。

```python
import d2lzh as d2l
from mxnet import gluon, init, nd
from mxnet.gluon import nn
# VGG块
def vgg_block(num_convs, num_channels):
	blk = nn.Sequential()
    # num_convs指定卷积层的数量
	for _ in range(num_convs):
        # num_channels指定输出通道数
		blk.add(nn.Conv2D(num_channels, kernel_size=3,
            		padding=1, activation='relu'))
		blk.add(nn.MaxPool2D(pool_size=2, strides=2))
	return blk
```

### 5.7.2 VGG网络

VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个vgg_block，超参数conv_arch指定了每个VGG块里卷积层个数和输出通道数。

```python
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))
def vgg(conv_arch):
	net = nn.Sequential()
	# 卷积层部分
	for (num_convs, num_channels) in conv_arch:
		net.add(vgg_block(num_convs, num_channels))
	# 全连接层部分
	net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
			nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
    		nn.Dense(10))
	return net
net = vgg(conv_arch)
```

VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。

### 5.7.3 获取数据和训练模型

**地除法——//**

//除法不管操作数为何种数值类型，总是会舍去小数部分，返回数字序列中比真正的商小的最接近的数字。

```python
ratio = 4
# 输出通道数除以4
small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]
net = vgg(small_conv_arch)
print(small_conv_arch)
# out
[(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]

lr, num_epochs, batch_size, ctx = 0.05, 5, 128, d2l.try_gpu()
net.initialize(ctx=ctx, init=init.Xavier())
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,num_epochs)
```

### 练习

- 与AlexNet相比，VGG通常计算慢很多，也需要更多的内存或显存。试分析原因。
- 尝试将Fashion-MNIST中图像的⾼和宽由224改为96。这在实验中有哪些影响？
- 参考VGG论文里的表1来构造VGG其他常用模型，如VGG-16和VGG-19 [1]。

### 5.8 网络中的网络（NiN）

前几节网络的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。而MiN则是串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。

#### 5.8.1 NiN块

卷积层的输入和输出是四维数组（样本，通道，高，宽），全连接层的输入和输出是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。而1×1卷积层可看成全连接层。

![MiN](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\MiN.png)

```python
import d2lzh as d2l
from mxnet import gluon, init, nd
from mxnet.gluon import nn
# MiN块，由一个卷积层加两个充当全连接层的1×1卷积层串联而成
def nin_block(num_channels, kernel_size, strides, padding):
	blk = nn.Sequential()
    # 第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的
	blk.add(nn.Conv2D(num_channels, kernel_size,
				strides, padding, activation='relu'),
			nn.Conv2D(num_channels, kernel_size=1, activation='relu'),
			nn.Conv2D(num_channels, kernel_size=1, activation='relu'))
	return blk
```

### 5.8.2 NiN模型

```python
net = nn.Sequential()
net.add(nin_block(96, kernel_size=11, strides=4, padding=0),
	nn.MaxPool2D(pool_size=3, strides=2),
	nin_block(256, kernel_size=5, strides=1, padding=2),
	nn.MaxPool2D(pool_size=3, strides=2),
	nin_block(384, kernel_size=3, strides=1, padding=1),
	nn.MaxPool2D(pool_size=3, strides=2), nn.Dropout(0.5),
	# 标签类别数是10
	nin_block(10, kernel_size=3, strides=1, padding=1),
	# 全局平均池化层将窗口形状主动设置成输入的高和宽
    # 可以显著减小模型参数尺寸，从而缓解过拟合。但有时会造成获得有效模型的训练时间的增加。
	nn.GlobalAvgPool2D(),
	# 将四维的输出转成二维的输出，其形状为(批量大小, 10)
	nn.Flatten())

X = nd.random.uniform(shape=(1, 1, 224, 224))
net.initialize()
for layer in net:
	X = layer(X)
	print(layer.name, 'output shape:\t', X.shape)
    
# out
sequential1 output shape: (1, 96, 54, 54)
pool0 output shape: (1, 96, 26, 26)
sequential2 output shape: (1, 256, 26, 26)
pool1 output shape: (1, 256, 12, 12)
sequential3 output shape: (1, 384, 12, 12)
pool2 output shape: (1, 384, 5, 5)
dropout0 output shape: (1, 384, 5, 5)
sequential4 output shape: (1, 10, 5, 5)
pool3 output shape: (1, 10, 1, 1)
flatten0 output shape: (1, 10)
```

### 5.8.3 获取数据和训练模型

```python
lr, num_epochs, batch_size, ctx = 0.1, 5, 128, d2l.try_gpu()
net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,
			num_epochs)
```

### 练习

- **调节超参数，提高分类准确率。**
- **为什么NiN块里要有两个1 × 1卷积层？去除其中的一个，观察并分析实验现象。**

### 5.9 含并行连结的网络（GoogLeNet）

#### 5.9.1 Inception 块

GoogLeNet中的基础卷积块为Inception块。

![Inception块](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\Inception块.png)

<center>Inception块的结构</center>

Inception块中可以自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。

```python
import d2lzh as d2l
from mxnet import gluon, init, nd
from mxnet.gluon import nn
class Inception(nn.Block):
	# c1 - c4为每条线路里的层的输出通道数
	def __init__(self, c1, c2, c3, c4, **kwargs):
		super(Inception, self).__init__(**kwargs)
		# 线路1，单1 x 1卷积层
		self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')
		# 线路2，1 x 1卷积层后接3 x 3卷积层
		self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')
		self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,
					activation='relu')
		# 线路3，1 x 1卷积层后接5 x 5卷积层
		self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')
		self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,
					activation='relu')
		# 线路4，3 x 3最大池化层后接1 x 1卷积层
		self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)
		self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')
	def forward(self, x):
		p1 = self.p1_1(x)
		p2 = self.p2_2(self.p2_1(x))
		p3 = self.p3_2(self.p3_1(x))
		p4 = self.p4_2(self.p4_1(x))
		return nd.concat(p1, p2, p3, p4, dim=1) # 在通道维上连结输出
```

#### 5.9.2 GoogLeNet模型

GoogLeNet跟VGG一样，在主体卷积部分中使用5个模块（block），每个模块之间使⽤步幅为2的3×
3最大池化层来减小输出高宽。

```python
# 模块一
b1 = nn.Sequential()
b1.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),
		nn.MaxPool2D(pool_size=3, strides=2, padding=1))
# 模块二
b2 = nn.Sequential()
b2.add(nn.Conv2D(64, kernel_size=1, activation='relu'),
		nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'),
		nn.MaxPool2D(pool_size=3, strides=2, padding=1))
# 模块三
b3 = nn.Sequential()
b3.add(Inception(64, (96, 128), (16, 32), 32),
		Inception(128, (128, 192), (32, 96), 64),
		nn.MaxPool2D(pool_size=3, strides=2, padding=1))
# 模块四
b4 = nn.Sequential()
b4.add(Inception(192, (96, 208), (16, 48), 64),
		Inception(160, (112, 224), (24, 64), 64),
		Inception(128, (128, 256), (24, 64), 64),
		Inception(112, (144, 288), (32, 64), 64),
		Inception(256, (160, 320), (32, 128), 128),
		nn.MaxPool2D(pool_size=3, strides=2, padding=1))
# 模块五
b5 = nn.Sequential()
b5.add(Inception(256, (160, 320), (32, 128), 128),
		Inception(384, (192, 384), (48, 128), 128),
       # 使用全局平均池化层来将每个通道的高和宽变成1
		nn.GlobalAvgPool2D())

net = nn.Sequential()
# 最后将输出变成二维数组后接上一个输出个数为标签类别数的全连接层
net.add(b1, b2, b3, b4, b5, nn.Dense(10))
```

GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。

#### 5.9.3 获取数据和训练模型

```python
lr, num_epochs, batch_size, ctx = 0.1, 5, 128, d2l.try_gpu()
net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,
			num_epochs)
```

### 练习

- **GoogLeNet有数个后续版本。尝试实现并运用它们，然后观察实验结果。这些后续版本包**
  **括加入批量归一化层（下一节将介绍）[2]、对Inception块做调整[3] 和加入残差连接（“残**
  **差网络（ResNet）”一节将介绍）[4]。**
- **对比AlexNet、VGG和NiN、GoogLeNet的模型参数尺寸。为什么后两个网络可以显著减小**
  **模型参数尺寸？**

### 5.10 批量归一化

对于深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。

批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。

#### 5.10.1 批量归一化层

**对全连接层做批量归一化**

对全连接层的批量归一化置于全连接层的仿射变换和激活函数之间。设全连接层的输入为u，权重参数和偏差参数分别为W和b，激活函数为ϕ。则输出为
$$
ϕ(BN(x))
$$
其中批量归一化输入x由仿射变换得到
$$
x= Wu + b
$$
**对卷积层做批量归一化**

对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。

**预测时的批量归一化**

因为在预测阶段有可能只需要预测一个或很少的样本，没有像训练样本中那么多的数据，此时均值和方差一定是有偏估计，所以若根据经验主义，其中momentum按经验可取0.9。
$$
当前mean = momentum * 当前mean + (1.0 - momentum) * 上一次训练的mean
$$

#### 5.10.2 从零开始实现

**使用批量归一化层的LeNet**

```python
import d2lzh as d2l
from mxnet import autograd, gluon, init, nd
from mxnet.gluon import nn

net = nn.Sequential()
# 卷积层的num_dims=4，全连接层的num_dims=2
net.add(nn.Conv2D(6, kernel_size=5),
	BatchNorm(6, num_dims=4),
	nn.Activation('sigmoid'),
	nn.MaxPool2D(pool_size=2, strides=2),
	nn.Conv2D(16, kernel_size=5),
	BatchNorm(16, num_dims=4),
	nn.Activation('sigmoid'),
    nn.MaxPool2D(pool_size=2, strides=2),
	nn.Dense(120),
	BatchNorm(120, num_dims=2),
	nn.Activation('sigmoid'),
	nn.Dense(84),
	BatchNorm(84, num_dims=2),
	nn.Activation('sigmoid'),
	nn.Dense(10))

class BatchNorm(nn.Block):
	def __init__(self, num_features, num_dims, **kwargs):
		super(BatchNorm, self).__init__(**kwargs)
		if num_dims == 2:
			shape = (1, num_features)
		else:
			shape = (1, num_features, 1, 1)
		# 参与求梯度和迭代的偏移和拉伸参数，分别初始化成0和1
		self.gamma = self.params.get('gamma', shape=shape, init=init.One())
		self.beta = self.params.get('beta', shape=shape, init=init.Zero())
		# 不参与求梯度和迭代的变量，全在内存上初始化成0
		self.moving_mean = nd.zeros(shape)
		self.moving_var = nd.zeros(shape)
	def forward(self, X):
		# 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上
		if self.moving_mean.context != X.context:
			self.moving_mean = self.moving_mean.copyto(X.context)
			self.moving_var = self.moving_var.copyto(X.context)
		# 保存更新过的moving_mean和moving_var
		Y, self.moving_mean, self.moving_var = batch_norm(
			X, self.gamma.data(), self.beta.data(), self.moving_mean,
			self.moving_var, eps=1e-5, momentum=0.9)
		return Y
    
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
	# 通过autograd来判断当前模式是训练模式还是预测模式
	if not autograd.is_training():
		# 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
		X_hat = (X - moving_mean) / nd.sqrt(moving_var + eps)
	else:
		assert len(X.shape) in (2, 4)
		if len(X.shape) == 2:
			# 使用全连接层的情况，计算特征维上的均值和方差
			mean = X.mean(axis=0)
			var = ((X - mean) ** 2).mean(axis=0)
		else:
			# 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持
			# X的形状以便后面可以做广播运算
			mean = X.mean(axis=(0, 2, 3), keepdims=True)
			var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)
		# 训练模式下用当前的均值和方差做标准化
		X_hat = (X - mean) / nd.sqrt(var + eps)
		# 更新移动平均的均值和方差
		moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
		moving_var = momentum * moving_var + (1.0 - momentum) * var
	Y = gamma * X_hat + beta # 拉伸和偏移
	return Y, moving_mean, moving_var
```

#### 5.10.3 简洁实现

Gluon中nn模块定义的BatchNorm类提供了批量归一化的功能，且BatchNorm类中所需的num_features和num_dims参数值可通过延后初始化自动获取。

```python
nn.BatchNorm()
```

#### 练习

- **能否将批量归一化前的全连接层或卷积层中的偏差参数去掉？为什么？（提示：回忆批量归一化中标准化的定义。）**
- **尝试调大学习率。同“卷积神经网络（LeNet）”一节中未使用批量归一化的LeNet相比，现在是不是可以使用更大的学习率？**
- **尝试将批量归一化层插入LeNet的其他地方，观察并分析结果的变化。**
- **尝试一下不学习拉伸参数gamma和偏移参数beta（构造的时候加入参数grad_req='null'来避免计算梯度），观察并分析结果。**
- **查看BatchNorm类的文档来了解更多使用方法，例如，如何在训练时使用基于全局平均的均值和方差。**

### 5.11 残差网络（ResNet）

在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。为此提出了残差网络。

![残差网络](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\残差网络.png)

<center>非残差网络与残差网络</center>

#### 5.11.1 残差块

残差块是ResNet的基础块，在残差块中，输入可通过跨层的数据线路更快地向前传播。

残差块里首先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。

残缺块可以设定输出通道数、是否使用额外的1×1卷积层来修改通道数以及卷积层的步幅。

```python
class Residual(nn.Block): # 本类已保存在d2lzh包中⽅便以后使⽤
    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):
        super(Residual, self).__init__(**kwargs)
        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,
            strides=strides)
        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)
        if use_1x1conv:
            # 若能被strides整除，则输出宽高为输入宽高/strides，否则输出宽高为1
            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,
                strides=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm()
        self.bn2 = nn.BatchNorm()
    def forward(self, X):
        Y = nd.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        return nd.relu(Y + X)
```

#### 5.11.2 ResNet模型

ResNet模型在输出通道数为64、步幅为2的7 × 7卷积层后接步幅为2的3 × 3的最大池化层，并在每个卷积层后增加的批量归一化层。

之后再接4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。

最后，加入全局平均池化层后接上全连接层输出。

```python
net = nn.Sequential()
net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),
		nn.BatchNorm(), nn.Activation('relu'),
		nn.MaxPool2D(pool_size=3, strides=2, padding=1))

# num_residuals为残缺块数
def resnet_block(num_channels, num_residuals, first_block=False):
	blk = nn.Sequential()
	for i in range(num_residuals):
		if i == 0 and not first_block:
			blk.add(Residual(num_channels, use_1x1conv=True, strides=2))
		else:
			blk.add(Residual(num_channels))
	return blk

net.add(resnet_block(64, 2, first_block=True),
		resnet_block(128, 2),
		resnet_block(256, 2),
		resnet_block(512, 2))
net.add(nn.GlobalAvgPool2D(), nn.Dense(10))
```

#### 练习

- **参考ResNet论文的表1来实现不同版本的ResNet [1]。**
- **对于比较深的网络，ResNet论文中介绍了一个“瓶颈”架构来降低模型复杂度。尝试实现它[1]。**
- **在ResNet的后续版本里，作者将残差块里的“卷积、批量归一化和激活”结构改成了“批量归一化、激活和卷积”，实现这个改进（[2]，图1）。**

### 5.12 稠密连接网络（DenseNet）

与ResNet的主要区别在于，DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是在通道维上连结。

![残差网络与稠密连接网络](C:\Users\Administrator\Desktop\个人\刘哂-福州大学\AI小组\笔记\残差网络与稠密连接网络.png)

<center>残差网络与稠密连接网络</center>

DenseNet的主要构建模块是稠密块（dense block）和过渡层（transition layer）。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。

#### 5.12.1 稠密块

DenseNet使用“批量归一化、激活和卷积”结构。

```python
import d2lzh as d2l
from mxnet import gluon, init, nd
from mxnet.gluon import nn
def conv_block(num_channels):
	blk = nn.Sequential()
	blk.add(nn.BatchNorm(), nn.Activation('relu'),
			nn.Conv2D(num_channels, kernel_size=3, padding=1))
	return blk

class DenseBlock(nn.Block):
	def __init__(self, num_convs, num_channels, **kwargs):
		super(DenseBlock, self).__init__(**kwargs)
		self.net = nn.Sequential()
        for _ in range(num_convs):
			self.net.add(conv_block(num_channels))
            
	def forward(self, X):
		for blk in self.net:
			Y = blk(X)
        	# 在通道维上将输入和输出连结
        	# dim=1表示横向连结
			X = nd.concat(X, Y, dim=1)
		return X

# 定义一个有2个输出通道数为10的卷积块
blk = DenseBlock(2, 10)
blk.initialize()
X = nd.random.uniform(shape=(4, 3, 8, 8))
Y = blk(X)
Y.shape

# out
# 输出通道23 = 2*10+3
# 卷积块的通道数num_channels控制了输出通道数相对于输入通道数的增长，
# 因此也被称为增长率（growth rate）
(4, 23, 8, 8)
```

#### 5.12.2 过渡层

通道数的增加会带来过于复杂的模型，过渡层用来控制模型复杂度。它通过1 × 1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而降低模型复杂度。

```python
def transition_block(num_channels):
	blk = nn.Sequential()
	blk.add(nn.BatchNorm(), nn.Activation('relu'),
		nn.Conv2D(num_channels, kernel_size=1),
		nn.AvgPool2D(pool_size=2, strides=2))
	return blk

#　对上亿节稠密块的输出使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半
blk = transition_block(10)
blk.initialize()
blk(Y).shape

#　out
(4, 10, 4, 4)
```

#### 5.12.3 DenseNet模型

DenseNet首先使用同ResNet一样的单卷积层和最大池化层，之后再接4个稠密块，我们可以设置每个稠密块使用多少个卷积层。并在每个稠密块下接减半宽高和通道数的过渡块。最后再接上全局池化层和全连接层来输出。

```python
net = nn.Sequential()
net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),
		nn.BatchNorm(), nn.Activation('relu'),
		nn.MaxPool2D(pool_size=3, strides=2, padding=1))

num_channels, growth_rate = 64, 32 # num_channels为当前的通道数
num_convs_in_dense_blocks = [4, 4, 4, 4] # 每个稠密块的卷积层个数

# enumerate(x)函数用于为x添加索引序列
for i, num_convs in enumerate(num_convs_in_dense_blocks):
	net.add(DenseBlock(num_convs, growth_rate))
	# 获取上一个稠密块的输出通道数，以便用于下面过渡层对通道数的减半
	num_channels += num_convs * growth_rate
	# 在稠密块之间加入通道数减半的过渡层
	if i != len(num_convs_in_dense_blocks) - 1:
        # 地板除\\即为向下取整除，其结果是将小数点后的位数被除去的商
        num_channels //= 2
        net.add(transition_block(num_channels))
        
net.add(nn.BatchNorm(), nn.Activation('relu'), nn.GlobalAvgPool2D(),
			nn.Dense(10))
```

### 练习

- **DenseNet论文中提到的一个优点是模型参数比ResNet的更小，这是为什么？**
- **DenseNet被人诟病的一个问题是内存或显存消耗过多。真的会这样吗？可以把输入形状换**
  **成224 × 224，来看看实际的消耗。**
- **实现DenseNet论文中的表1提出的不同版本的DenseNet。**