# 第四章 深度学习计算

## 4.1 模型构造

**python 中 *args 和 ******kwargs 的区别**

```python
def test(*args):  
    print(args)
    for i in args:
        print(i)

test(1,2,3)

# 输出
(1,2,3)
1
2
3
```

```python
def test(**kwargs):
    print(kwargs)
    keys = kwargs.keys()
    value = kwargs.values()
    print(keys)
    print(value)

test(a=1,b=2,c=3,d=4)

# 输出值分别为
{'a': 1, 'b': 2, 'c': 3, 'd': 4}
dict_keys(['a', 'b', 'c', 'd'])
dict_values([1, 2, 3, 4])
```

**nd.random.uniform**

nd.random.uniform()

返回一个浮点数N，在[0,1]的范围

nd.random.uniform(x,y)

返回一个浮点数N，在[x,y]的范围

nd.random.uniform(x,y,shape=(2,2))

返回一个2x2的矩阵，矩阵各元素在[x,y]的范围

**x.norm()**

对x中每个元素进行平方，然后对它们求和，最后取平方根。这些操作计算所谓的L2或欧几里德范数。

### 4.1.1 继承Block类来构造模型

Block类的⼦类既可以是⼀个层（如Gluon提供的Dense类），⼜可以是⼀个模型（如这⾥定义的MLP类），或者是模型的⼀个部分。

```python
from mxnet import nd
from mxnet.gluon import nn
# 继承Block类来构造模型
class MLP(nn.Block):
# 声明带有模型参数的层，这⾥声明了两个全连接层
	def __init__(self, **kwargs):
		# 调⽤MLP⽗类Block的构造函数来进⾏必要的初始化。这样在构造实例时还可以指定其他函数
		# 参数，如“模型参数的访问、初始化和共享”⼀节将介绍的模型参数params
		super(MLP, self).__init__(**kwargs)
		self.hidden = nn.Dense(256, activation='relu') # 隐藏层
		self.output = nn.Dense(10) # 输出层
	# 定义模型的前向计算，即如何根据输⼊x计算返回所需要的模型输出
	def forward(self, x):
		return self.output(self.hidden(x))

X = nd.random.uniform(shape=(2, 20))
net = MLP()
net.initialize()
# 输出2x10的矩阵，2表示输入的两行向量，10代表每行输入向量通过10个输出单元输出
net(X)
```

### 4.1.2 Sequential类继承自Block类

Sequential类提供add函数来逐⼀添加串联的Block⼦类实例，而模型的前向计算就是将这些实例
按添加的顺序逐⼀计算。

```python
class MySequential(nn.Block):
	def __init__(self, **kwargs):
		super(MySequential, self).__init__(**kwargs)
	def add(self, block):
		# block是⼀个Block⼦类实例，假设它有⼀个独⼀⽆⼆的名字。我们将它保存在Block类的
		# 成员变量_children⾥，其类型是OrderedDict。当MySequential实例调⽤
		# initialize函数时，系统会⾃动对_children⾥所有成员初始化
		self._children[block.name] = block
	def forward(self, x):
		# OrderedDict保证会按照成员添加时的顺序遍历成员
		for block in self._children.values():
			x = block(x)
		return x
    
net = MySequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize()
net(X)
```

### 4.1.3 构造复杂的模型

```python
class FancyMLP(nn.Block):
	def __init__(self, **kwargs):
		super(FancyMLP, self).__init__(**kwargs)
		# 使⽤get_constant创建的随机权重参数不会在训练中被迭代（即常数参数）
		self.rand_weight = self.params.get_constant(
			'rand_weight', nd.random.uniform(shape=(20, 20)))
		self.dense = nn.Dense(20, activation='relu')
	def forward(self, x):
		x = self.dense(x)
        print("x1")
        x
		# 使⽤创建的常数参数，以及NDArray的relu函数和dot函数
		x = nd.relu(nd.dot(x, self.rand_weight.data()) + 1)
		# 复⽤全连接层。等价于两个全连接层共享参数
		x = self.dense(x)
		# 控制流，这⾥我们需要调⽤asscalar函数来返回标量进⾏⽐较
		while x.norm().asscalar() > 1:
			x /= 2
		if x.norm().asscalar() < 0.8:
			x *= 10
		return x.sum()
    
net = FancyMLP()
net.initialize()
net(X)
```

因为FancyMLP和Sequential类都是Block类的⼦类，所以我们可以嵌套调⽤它们。

```python
class NestMLP(nn.Block):
	def __init__(self, **kwargs):
		super(NestMLP, self).__init__(**kwargs)
		self.net = nn.Sequential()
		self.net.add(nn.Dense(64, activation='relu'),
						nn.Dense(32, activation='relu'))
		self.dense = nn.Dense(16, activation='relu')
	def forward(self, x):
		return self.dense(self.net(x))
 
net = nn.Sequential()
net.add(NestMLP(), nn.Dense(20), FancyMLP())
net.initialize()
net(X)
```

### 练习

**•  如果不在MLP类的init函数里调用父类的init函数，会出现什么样的错误信息？**

AttributeError: 'MLP' object has no attribute '_children'

**•  如果去掉FancyMLP类里面的asscalar函数，会有什么问题？**

asscalar函数可将调用对象转为标量，若去掉asscalar函数，可能会导致向量/矩阵与1进行比较。

**•  如果将NestMLP类中通过Sequential实例定义的self.net改为self.net = [nn.Dense(64, 		activation='relu'), nn.Dense(32, activation='relu')]， 会有什么问题？**

那self.net将变成一个list，无法add，也无法initialize()

## 4.2 模型参数的访问、初始化和共享

### 4.2.1 访问模型参数

```python
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize() # 使⽤默认初始化⽅式、

# 访问模型参数
# 索引0表⽰隐藏层为Sequential实例最先添加的层
net[0].params, type(net[0].params)
net[0].params['dense0_weight'], net[0].weight
# 访问具体数据
net[0].weight.data()
net[0].weight.grad()
# 获取net变量所有嵌套的层所包含的所有参数
net.collect_params()
# 可以通过正则表达式来匹配参数名，从而筛选需要的参数
net.collect_params('.*weight')
```

### 4.2.2 初始化模型参数

```python
# 权重参数初始化成均值为0、标准差为0.01的正态分布随机数,偏差参数清零
# ⾮⾸次对模型初始化需要指定force_reinit为真
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)
net[0].weight.data()[0]

# 使⽤常数来初始化权重参数
net.initialize(init=init.Constant(1), force_reinit=True)

# 对特定参数weight进⾏初始化
# 对隐藏层的权重使⽤Xavier随机初始化⽅法
net[0].weight.initialize(init=init.Xavier(), force_reinit=True)
```

### 4.2.3 自定义初始化方法

```python
# 继承Initializer类，并实现_init_weight函数
class MyInit(init.Initializer):
	def _init_weight(self, name, data):
		print('Init', name, data.shape)
		data[:] = nd.random.uniform(low=-10, high=10, shape=data.shape)
		data *= data.abs() >= 5
        
net.initialize(MyInit(), force_reinit=True)
```

```python
#　通过Parameter类的set_data函数来直接改写模型参数
net[0].weight.set_data(net[0].weight.data() + 1)
```

### 4.2.4 共享模型参数

```python
net = nn.Sequential()
shared = nn.Dense(8, activation='relu')
# 在构造第三隐藏层时通过params来指定它使⽤第⼆隐藏层的参数
# 因为模型参数⾥包含了梯度，所以在反向传播计算时，第⼆隐藏层和第三隐藏层的梯度都会被累加在
# shared.params.grad()⾥。
net.add(nn.Dense(8, activation='relu'),
			shared,
			nn.Dense(8, activation='relu', params=shared.params),
			nn.Dense(10))
net.initialize()
```

### 练习

**•  查阅有关init模块的MXNet文档，了解不同的参数初始化方法。**

| [`Bilinear`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Bilinear)() | Initialize weight for upsampling layers.                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`Constant`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Constant)(value) | Initializes the weights to a given value.                    |
| [`InitDesc`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.InitDesc) | Descriptor for the initialization pattern.                   |
| [`Initializer`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Initializer)(**kwargs) | The base class of an initializer.                            |
| [`LSTMBias`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.LSTMBias)([forget_bias]) | Initialize all biases of an LSTMCell to 0.0 except for the forget gate whose bias is set to custom value. |
| [`Load`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Load)(param[, default_init, verbose]) | Initializes variables by loading data from file or dict.     |
| [`MSRAPrelu`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.MSRAPrelu)([factor_type, slope]) | Initialize the weight according to a MSRA paper.             |
| [`Mixed`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Mixed)(patterns, initializers) | Initialize parameters using multiple initializers.           |
| [`Normal`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Normal)([sigma]) | Initializes weights with random values sampled from a normal distribution with a mean of zero and standard deviation of sigma. |
| [`One`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.One)() | Initializes weights to one.                                  |
| [`Orthogonal`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Orthogonal)([scale, rand_type]) | Initialize weight as orthogonal matrix.                      |
| [`Uniform`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Uniform)([scale]) | Initializes weights with random values uniformly sampled from a given range. |
| [`Xavier`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Xavier)([rnd_type, factor_type, magnitude]) | Returns an initializer performing “Xavier” initialization for weights. |
| [`Zero`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Zero)() | Initializes weights to zero.                                 |

**•  尝试在net.initialize()后、net(X)前访问模型参数，观察模型参数的形状。**

```python
from mxnet import init, nd
from mxnet.gluon import nn
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize() # 使⽤默认初始化⽅式

net[0].params
```

```
dense4_ (
  Parameter dense4_weight (shape=(256, 0), dtype=float32)
  Parameter dense4_bias (shape=(256,), dtype=float32)
)
```

**•  构造⼀个含共享参数层的多层感知机并训练。在训练过程中，观察每⼀层的模型参数和梯**
**度。**

## 4.3 模型参数的延后初始化

### 4.3.1 延后初始化

参数在刚执行完net.initialize()时不会初始化，而只有在知道了输入大小，即执行前向计算时才会被初始化。

这样就不⽤⼈⼯推测输⼊个数，但也经常要额外做⼀次前向计算来迫使参数被真正地初始化，以直接操作模型参数。

### 4.3.2 避免延后初始化

```python
from mxnet import init, nd
from mxnet.gluon import nn
net = nn.Sequential()
# 在创建层时指定输⼊个数
net.add(nn.Dense(256, in_units=20, activation='relu'))
net.add(nn.Dense(10, in_units=256))
net.initialize()
print(net[0].weight)
print(net[1].weight)
```

```python
# out
Parameter dense6_weight (shape=(256, 20), dtype=float32)
Parameter dense7_weight (shape=(10, 256), dtype=float32)
```

### 练习

**•  如果在下⼀次前向计算net(X)前改变输入X的形状，包括批量大小和输入个数，会发生什么？**

## 4.4 自定义层

### 4.4.1 不含模型参数的自定义层

```python
from mxnet import gluon, nd
from mxnet.gluon import nn

# 通过继承Block类⾃定义了⼀个将输⼊减掉均值后输出的层
class CenteredLayer(nn.Block):
	def __init__(self, **kwargs):
		super(CenteredLayer, self).__init__(**kwargs)
	def forward(self, x):
		return x - x.mean()
    
net = nn.Sequential()
# CenteredLayer的输出个数与nn.Dense(128)的单元个数128一致
net.add(nn.Dense(128),CenteredLayer())
net.initialize()
y = net(nd.random.uniform(shape=(4, 8)))
```

### 4.4.2 含模型参数的自定义层

自定义模型参数

```python
params = gluon.ParameterDict()
# 自定义一个2x3的param2参数
params.get('param2', shape=(2, 3))

# out
(
Parameter param2 (shape=(2, 3), dtype=<class 'numpy.float32'>)
)
```

实现⼀个含权重参数和偏差参数的全连接层

```python
class MyDense(nn.Block):
	# units为该层的输出个数，in_units为该层的输⼊个数
	def __init__(self, units, in_units, **kwargs):
		super(MyDense, self).__init__(**kwargs)
		self.weight = self.params.get('weight', shape=(in_units, units))
		self.bias = self.params.get('bias', shape=(units,))
	def forward(self, x):
		linear = nd.dot(x, self.weight.data()) + self.bias.data()
		return nd.relu(linear)
```

实例化MyDense类并做向前计算

```python
dense = MyDense(units=3, in_units=5)
dense.initialize()
dense(nd.random.uniform(shape=(2, 5)))
```

也可以使⽤⾃定义层MyDense构造模型

```python
net = nn.Sequential()
net.add(MyDense(8, in_units=64),
MyDense(1, in_units=8))
net.initialize()
net(nd.random.uniform(shape=(2, 64)))
```

## 4.5 读取和存储

### 4.5.1 读写NDArray

可以直接使⽤save函数和load函数分别存储和读取NDArray

### 4.5.2 读写Gluon模型的参数

```python
# 实例化自定义多层感知机
net = MLP()
net.initialize()
X = nd.random.uniform(shape=(2, 20))
Y = net(X)

# 存取模型参数
filename = 'mlp.params'
net.save_parameters(filename)
net2 = MLP()
net2.load_parameters(filename)
```

### 练习

**•  即使无须把训练好的模型部署到不同的设备，存储模型参数在实际中还有哪些好处？**

## 4.6 GPU计算

### NDArray的GPU计算

```python
import mxnet as mx
from mxnet import nd
from mxnet.gluon import nn

# 将NDArray变量a创建在gpu(0)上
a = nd.array([1, 2, 3], ctx=mx.gpu())

# 通过copyto函数在设备之间传输数据
# copyto函数是为⽬标变量开新的内存或显存
y = x.copyto(mx.gpu())
# 通过as_in_context函数在设备之间传输数据
# 如果源变量和⽬标变量的context⼀致，as_in_context函数使⽬标变量和源变量共享源变量的内存或显
# 存。
z = x.as_in_context(mx.gpu())
```

MXNet要求计算的所有输⼊数据都在内存或同⼀块显卡的显存上。

当我们打印NDArray或将NDArray转换成NumPy格式时，如果数据不在内存⾥，MXNet会将它先复制到内存，从而造成额外的传输开销。

### Gluon的GPU计算

```python
net = nn.Sequential()
net.add(nn.Dense(1))
net.initialize(ctx=mx.gpu())
```

### 练习

**•  试试大一点儿的计算任务，如大矩阵的乘法，看看使用CPU和GPU的速度区别。如果是计算量很小的任务呢？**

**•  GPU上应如何读写模型参数？**