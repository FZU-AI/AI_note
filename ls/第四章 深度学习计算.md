# 第四章 深度学习计算

## 4.1 模型构造

**python 中 *args 和 ******kwargs 的区别**

```python
def test(*args):  
    print(args)
    for i in args:
        print(i)

test(1,2,3)

# 输出
(1,2,3)
1
2
3
```

```python
def test(**kwargs):
    print(kwargs)
    keys = kwargs.keys()
    value = kwargs.values()
    print(keys)
    print(value)

test(a=1,b=2,c=3,d=4)

# 输出值分别为
{'a': 1, 'b': 2, 'c': 3, 'd': 4}
dict_keys(['a', 'b', 'c', 'd'])
dict_values([1, 2, 3, 4])
```

**nd.random.uniform**

nd.random.uniform()

返回一个浮点数N，在[0,1]的范围

nd.random.uniform(x,y)

返回一个浮点数N，在[x,y]的范围

nd.random.uniform(x,y,shape=(2,2))

返回一个2x2的矩阵，矩阵各元素在[x,y]的范围

**x.norm()**

对x中每个元素进行平方，然后对它们求和，最后取平方根。这些操作计算所谓的L2或欧几里德范数。

### 4.1.1 继承Block类来构造模型

Block类的⼦类既可以是⼀个层（如Gluon提供的Dense类），⼜可以是⼀个模型（如这⾥定义的MLP类），或者是模型的⼀个部分。

```python
from mxnet import nd
from mxnet.gluon import nn
# 继承Block类来构造模型
class MLP(nn.Block):
# 声明带有模型参数的层，这⾥声明了两个全连接层
	def __init__(self, **kwargs):
		# 调⽤MLP⽗类Block的构造函数来进⾏必要的初始化。这样在构造实例时还可以指定其他函数
		# 参数，如“模型参数的访问、初始化和共享”⼀节将介绍的模型参数params
		super(MLP, self).__init__(**kwargs)
		self.hidden = nn.Dense(256, activation='relu') # 隐藏层
		self.output = nn.Dense(10) # 输出层
	# 定义模型的前向计算，即如何根据输⼊x计算返回所需要的模型输出
	def forward(self, x):
		return self.output(self.hidden(x))

X = nd.random.uniform(shape=(2, 20))
net = MLP()
net.initialize()
# 输出2x10的矩阵，2表示输入的两行向量，10代表每行输入向量通过10个输出单元输出
net(X)
```

### 4.1.2 Sequential类继承自Block类

Sequential类提供add函数来逐⼀添加串联的Block⼦类实例，而模型的前向计算就是将这些实例
按添加的顺序逐⼀计算。

```python
class MySequential(nn.Block):
	def __init__(self, **kwargs):
		super(MySequential, self).__init__(**kwargs)
	def add(self, block):
		# block是⼀个Block⼦类实例，假设它有⼀个独⼀⽆⼆的名字。我们将它保存在Block类的
		# 成员变量_children⾥，其类型是OrderedDict。当MySequential实例调⽤
		# initialize函数时，系统会⾃动对_children⾥所有成员初始化
		self._children[block.name] = block
	def forward(self, x):
		# OrderedDict保证会按照成员添加时的顺序遍历成员
		for block in self._children.values():
			x = block(x)
		return x
    
net = MySequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize()
net(X)
```

### 4.1.3 构造复杂的模型

```python
class FancyMLP(nn.Block):
	def __init__(self, **kwargs):
		super(FancyMLP, self).__init__(**kwargs)
		# 使⽤get_constant创建的随机权重参数不会在训练中被迭代（即常数参数）
		self.rand_weight = self.params.get_constant(
			'rand_weight', nd.random.uniform(shape=(20, 20)))
		self.dense = nn.Dense(20, activation='relu')
	def forward(self, x):
		x = self.dense(x)
        print("x1")
        x
		# 使⽤创建的常数参数，以及NDArray的relu函数和dot函数
		x = nd.relu(nd.dot(x, self.rand_weight.data()) + 1)
		# 复⽤全连接层。等价于两个全连接层共享参数
		x = self.dense(x)
		# 控制流，这⾥我们需要调⽤asscalar函数来返回标量进⾏⽐较
		while x.norm().asscalar() > 1:
			x /= 2
		if x.norm().asscalar() < 0.8:
			x *= 10
		return x.sum()
    
net = FancyMLP()
net.initialize()
net(X)
```

因为FancyMLP和Sequential类都是Block类的⼦类，所以我们可以嵌套调⽤它们。

```python
class NestMLP(nn.Block):
	def __init__(self, **kwargs):
		super(NestMLP, self).__init__(**kwargs)
		self.net = nn.Sequential()
		self.net.add(nn.Dense(64, activation='relu'),
						nn.Dense(32, activation='relu'))
		self.dense = nn.Dense(16, activation='relu')
	def forward(self, x):
		return self.dense(self.net(x))
 
net = nn.Sequential()
net.add(NestMLP(), nn.Dense(20), FancyMLP())
net.initialize()
net(X)
```

### 练习

**•  如果不在MLP类的init函数里调用父类的init函数，会出现什么样的错误信息？**

AttributeError: 'MLP' object has no attribute '_children'

**•  如果去掉FancyMLP类里面的asscalar函数，会有什么问题？**

asscalar函数可将调用对象转为标量，若去掉asscalar函数，可能会导致向量/矩阵与1进行比较。

**•  如果将NestMLP类中通过Sequential实例定义的self.net改为self.net = [nn.Dense(64, 		activation='relu'), nn.Dense(32, activation='relu')]， 会有什么问题？**

那self.net将变成一个list，无法add，也无法initialize()

## 4.2 模型参数的访问、初始化和共享

### 4.2.1 访问模型参数

```python
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize() # 使⽤默认初始化⽅式、

# 访问模型参数
# 索引0表⽰隐藏层为Sequential实例最先添加的层
net[0].params, type(net[0].params)
net[0].params['dense0_weight'], net[0].weight
# 访问具体数据
net[0].weight.data()
net[0].weight.grad()
# 获取net变量所有嵌套的层所包含的所有参数
net.collect_params()
# 可以通过正则表达式来匹配参数名，从而筛选需要的参数
net.collect_params('.*weight')
```

### 4.2.2 初始化模型参数

```python
# 权重参数初始化成均值为0、标准差为0.01的正态分布随机数,偏差参数清零
# ⾮⾸次对模型初始化需要指定force_reinit为真
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)
net[0].weight.data()[0]

# 使⽤常数来初始化权重参数
net.initialize(init=init.Constant(1), force_reinit=True)

# 对特定参数weight进⾏初始化
# 对隐藏层的权重使⽤Xavier随机初始化⽅法
net[0].weight.initialize(init=init.Xavier(), force_reinit=True)
```

### 4.2.3 自定义初始化方法

```python
# 继承Initializer类，并实现_init_weight函数
class MyInit(init.Initializer):
	def _init_weight(self, name, data):
		print('Init', name, data.shape)
		data[:] = nd.random.uniform(low=-10, high=10, shape=data.shape)
		data *= data.abs() >= 5
        
net.initialize(MyInit(), force_reinit=True)
```

```python
#　通过Parameter类的set_data函数来直接改写模型参数
net[0].weight.set_data(net[0].weight.data() + 1)
```

### 4.2.4 共享模型参数

```python
net = nn.Sequential()
shared = nn.Dense(8, activation='relu')
# 在构造第三隐藏层时通过params来指定它使⽤第⼆隐藏层的参数
# 因为模型参数⾥包含了梯度，所以在反向传播计算时，第⼆隐藏层和第三隐藏层的梯度都会被累加在
# shared.params.grad()⾥。
net.add(nn.Dense(8, activation='relu'),
			shared,
			nn.Dense(8, activation='relu', params=shared.params),
			nn.Dense(10))
net.initialize()
```

### 练习

**•  查阅有关init模块的MXNet文档，了解不同的参数初始化方法。**

| [`Bilinear`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Bilinear)() | Initialize weight for upsampling layers.                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`Constant`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Constant)(value) | Initializes the weights to a given value.                    |
| [`InitDesc`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.InitDesc) | Descriptor for the initialization pattern.                   |
| [`Initializer`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Initializer)(**kwargs) | The base class of an initializer.                            |
| [`LSTMBias`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.LSTMBias)([forget_bias]) | Initialize all biases of an LSTMCell to 0.0 except for the forget gate whose bias is set to custom value. |
| [`Load`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Load)(param[, default_init, verbose]) | Initializes variables by loading data from file or dict.     |
| [`MSRAPrelu`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.MSRAPrelu)([factor_type, slope]) | Initialize the weight according to a MSRA paper.             |
| [`Mixed`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Mixed)(patterns, initializers) | Initialize parameters using multiple initializers.           |
| [`Normal`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Normal)([sigma]) | Initializes weights with random values sampled from a normal distribution with a mean of zero and standard deviation of sigma. |
| [`One`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.One)() | Initializes weights to one.                                  |
| [`Orthogonal`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Orthogonal)([scale, rand_type]) | Initialize weight as orthogonal matrix.                      |
| [`Uniform`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Uniform)([scale]) | Initializes weights with random values uniformly sampled from a given range. |
| [`Xavier`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Xavier)([rnd_type, factor_type, magnitude]) | Returns an initializer performing “Xavier” initialization for weights. |
| [`Zero`](https://mxnet.apache.org/api/python/docs/api/initializer/index.html#mxnet.initializer.Zero)() | Initializes weights to zero.                                 |

**•  尝试在net.initialize()后、net(X)前访问模型参数，观察模型参数的形状。**

```python
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)
net[0].weight.data()
```

```
[[ 0.00957075 -0.00084031  0.00617897 ...  0.00564483  0.00275536
   0.00234086]
 [-0.01550176  0.00167047 -0.00197037 ...  0.00506291 -0.01210642
  -0.01024502]
 [-0.01584712 -0.02477347  0.00864046 ...  0.01414129 -0.00594405
   0.00053162]
 ...
 [-0.003001   -0.00373877  0.00598197 ...  0.01762538 -0.00077467
  -0.01505519]
 [ 0.02092716  0.0014451  -0.00919948 ... -0.00058619 -0.00692117
   0.00487524]
 [-0.00490216  0.0038899   0.00983088 ...  0.01914853 -0.00731475
  -0.00848357]]
<NDArray 256x20 @cpu(0)>
```

**•  构造⼀个含共享参数层的多层感知机并训练。在训练过程中，观察每⼀层的模型参数和梯**
**度。**

