# 深度学习第三章

## 线性回归

**公式**：![](C:\Users\郑国粦\Desktop\图\线性回归公式.png)

例子：y[i] = 2 * x[i][0] - 3.4 * x[i][1] + 4.2 + noise
$$
y[i] = 2 * x[i][0] - 3.4 * x[i][1] + 4.2 + noise
$$

```
from mxnet import ndarray as nd
from mxnet import autograd

num_inputs = 2
num_examples = 1000

true_w = [2,-3.4]
true_b = 4.2

x = nd.random_normal(shape=(num_examples, num_inputs))
y = true_w[0] * x[:,0] + true_w[1] * x[:,1] +true_b
#               x的第一列              x的第二列
y+= .01 * nd.random_normal(shape=y.shape)
#  noise
print(x[0:10],y[0:10])
```

#### 矢量计算表达式

```
from mxnet import nd
from time import time
a = nd.ones(shape = 1000)
b = nd.ones(shape = 1000)
```

> 向量相加的一种方法，将这两个向量按元素逐一做标量相加

```
start = time()#用于获取当前时间戳
c = nd.zeros(shape=1000)
for i in range(1000):
    c[i] = a[i] + b[i]
time() - start
```

[^out]: `0.4036376476287842`



> 向量相加的另一种方法，将这两个向量直接做矢量加法

```
start = time()
d = a + b
time() - start
```

[^out]: `0.0009965896606445312`

**小结：**

- 和大多数深度学习模型一样，对于线性回归这样一种单层神经网络，它的基本要素包括模型，训练数据，损失函数和优化算法。
- 既可以用神经网络图表示线性回归，又可以用矢量计算表示该模型。
- 应尽可能采用矢量计算，以提高计算效率

## 线性回归从零开始

```
%matplotlib inline
from IPython import display
from matplotlib import pyplot as plt
from mxnet import nd,autograd
import random
```

[matplotlib]: https://blog.csdn.net/m0_37712157/article/details/81872610

```
num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += nd.random.normal(scale=0.01, shape=labels.shape)
```

> `features`的每一行是一个长度为2的向量，而`labels`的每一行是一个长度为1的向量（标量）

```
def use_svg_display():
    # 用矢量图显示
    display.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
    use_svg_display()
    # 设置图的尺寸
    plt.rcParams['figure.figsize'] = figsize

set_figsize()
plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1);  # 加分号只显示图
```

![out](C:\Users\郑国粦\Desktop\图\linear_outcome.png)

```
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)  # 样本的读取顺序是随机的
    for i in range(0, num_examples, batch_size):
        j = nd.array(indices[i: min(i + batch_size, num_examples)])
        yield features.take(j), labels.take(j)  # take函数根据索引返回对应元素
```



**总结：**

先数据，再定义模型，定义损失函数，再训练。

## 线性回归的简洁实现

**生成数据集**

```
from mxnet import autograd, nd
num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
features = nd.random.normal(scale=1,shape=(num_examples,num_inputs))
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += nd.random.normal(scale=0.01, shape=labels.shape)
```

> 同上，features是训练数据特征，labels是标签

**读取数据**

Gluon提供了data包来读取数据。

```
from mxnet.gluon import data as gdata
batch_size = 10
dataset = gdata.ArrayDataset(features, labels)
data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)
```

**定义模型**

nn模块，“nn”是neural networks(神经网络)的缩写。该模块定义了大量神经网络的层。定义一个模型变量net，它是一个Sequential实例。在Gluon中，Sequential实例可以看做一个串联各个层的容器。在构造模型是，我们在该容器中依次添加层。

```
from mxnet.gluon import nn
net = nn.Sequential()
net.add(nn.Dense(1))
```

**初始化模型参数**

```
from mxnet import init
net.initialize(init.Normal(sigma=0.01))
```

> init.Normal(sigma=0.01)指定权重参数每个元素将在初始化时随机采样于均值为0、标准差为0.01的正太分布。偏差参数默认会初始化为零

**定义损失函数**

在Gluon中，loss模块定义了各种损失函数，我们用假名gloss代替导入的loss模块，并直接使用它提供的平方损失作为模型的损失函数。

```
from mxnet.gluon import loss as gloss
loss = gloss.L2Loss()
```

> 平方损失又称L2范数损失

**定义优化算法**

创建一个Trainer实例，并指定学习率为0.03的小批量随机梯度下降（sgd）为优化算法。该优化算法将用来迭代net实例所有通过add函数嵌套的层所包含的全部参数。这些参数可以通过collect_params函数获取。

```
from mxnet import gluon
trainer = gluon.Trainer(net.collect_params, 'sgd', {'learning_rate': 0.03} )
```

**训练模型**

在使用Gluon训练模型时，我们通过调用Trainer实例的step函数来迭代模型参数。

由于变量l是长度为batch_size的一维NDArray，执行l.backward()等价于执行l.sum().backward()。在step函数中指明批量大小，从而对批量中样本梯度求平均。

```
num_epochs = 3
for epoch in range(1,num_epochs + 1):
    for X, y in data_iter:
        with autograd.record():
            l = loss(net(X), y)
        l.backward()
        trainer.step(batch_size)
    l = loss(net(features), labels)
    print('epoch %d, loss: %f'%(epoch, l.mean().asnumpy()))
```

**小结**

- 使用Gluon可以更简洁地实现模型。
- 在Gluon中，data模块提供了有关数据处理的工具，nn模块定义了大量神经网络的层，在loss模块定义了各种损失函数。
- MXNet的initializer模块提供了模型参数初始化的各种方法。

**softmax回归**

softmax回归和线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于softmax回归的输入值个数等于标签里的类别数。

softmax运算符

- 解决由于输出层的输出值范围不确定，难以直观判断这些值得意义

- 由于真实范围的标签是离散值，这些离散值与不确定范围的输出值直接的误差难以衡量

**单样本分类的矢量计算表达式**

**小批量样本分类的矢量计算表达式**

**交叉熵损失函数**

使用更适合衡量两个概率分布差异的测量函数--交叉熵

**小结**

- softmax回归使用于分类问题。它使用softmax运算输出类别的概率分布。
- softmax回归是一个单层神经网络，输出个数等于分类问题中的类别个数
- 交叉熵适合衡量两个概率分布的差异

## 图像分类数据集（Fashion-MNIST）

多类图像分类数据集，以方便我们观察比较算法之间在模型精度和计算效率上的区别。

**获取数据集**

```
%matplotlib inline
import d2lzh as d2l
from mxnet.gluon import data as gdata
import sys
import time
```

> 我们通过Gluon的`data`包来下载这个数据集。第一次调用时会自动从网上获取数据。我们通过参数`train`来指定获取训练数据集或测试数据集（testing data set）。测试数据集也叫测试集（testing set），只用来评价模型的表现，并不用来训练模型。

```
mnist_train = gdata.vision.FashionMNIST(train=True)
mnist_test = gdata.vision.FashionMNIST(train=False)
len(mnist_train), len(mnist_test)
out:(60000, 10000)
```

> 训练集中和测试集中的每个类别的图像数分别为6,000和1,000。因为有10个类别，所以训练集和测试集的样本数分别为60,000和10,000。

```
feature, label = mnist_train[0]
```

> 我们可以通过方括号`[]`来访问任意一个样本，下面获取第一个样本的图像和标签。

```
feature.shape, feature.dtype
out:((28, 28, 1), numpy.uint8)
```

> 变量`feature`对应高和宽均为28像素的图像。每个像素的数值为0到255之间8位无符号整数（uint8）。它使用三维的`NDArray`存储。其中的最后一维是通道数。因为数据集中是灰度图像，所以通道数为1。为了表述简洁，我们将高和宽分别为ℎh和𝑤w像素的图像的形状记为ℎ×𝑤h×w或`（h，w）`。

```
label, type(label), label.dtype
out:(2, numpy.int32, dtype('int32'))
```

> 图像的标签使用NumPy的标量表示。它的类型为32位整数（int32）。

```
# 本函数已保存在d2lzh包中方便以后使用
def get_fashion_mnist_labels(labels):
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]
```

> Fashion-MNIST中一共包括了10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。以下函数可以将数值标签转成相应的文本标签。

```
# 本函数已保存在d2lzh包中方便以后使用
def show_fashion_mnist(images, labels):
    d2l.use_svg_display()
    # 这里的_表示我们忽略（不使用）的变量
    _, figs = d2l.plt.subplots(1, len(images), figsize=(12, 12))
    for f, img, lbl in zip(figs, images, labels):
        f.imshow(img.reshape((28, 28)).asnumpy())
        f.set_title(lbl)
        f.axes.get_xaxis().set_visible(False)
        f.axes.get_yaxis().set_visible(False)
```

```
X, y = mnist_train[0:9]
show_fashion_mnist(X, get_fashion_mnist_labels(y))
out:<Figure size 864x864 with 9 Axes>
```

## 多层感知机

**隐藏层**

多层感知机在单层神经网络的基础上引入了一到多个隐藏层。隐藏层位于输入层和输出层之间。

多层感知机中的隐藏层和输出层都是全连接层。

神经网络虽然引入了隐藏层，却依然等价于一个单层神经网络。

**激活函数**

根源：全连接层只是对数据做映射变换，而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换。

**ReLU函数**

**sigmoid函数**

**tanh函数**

**小结**

- 多层感知机在输出层和输入层之间加入了一个或多个全连接隐藏层，并通过激活函数对隐藏层输出进行变换。
- 常用的激活函数包括ReLU函数、sigmoid函数和tanh函数

**训练误差**

指模型在训练数据集上表现出的误差

**泛化误差**

指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。

> 以高考为例：训练误差可以认为是做往年高考考试题（训练题）是的错误率，泛化误差则可以通过真正参加高考（测试题）是的答题错误率来近似。假设训练题和测试题都随机采样于一个未知的依照相同考纲的巨大试题库。如果让一名未学习中学知识的小学生去答题，那么测试题和训练题的答题错误率可能很相近。但如果换成一名反复练习训练题的高三备考生答题，即使在训练题上做到了错误率为0，也不代表真实的高考成绩会如此。

**验证数据集**

从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集。

**K折交叉验证**

由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈，一种改善的方法是K折交叉验证。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，我们每次用来验证的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求分均。

**欠拟合**

模型无法得到较低的训练误差。

**过拟合**

模型的训练误差远小于它再测试数据集上的误差。

## 权重衰减

## 丢弃法

