# 深度学习计算

> [相关]: https://www.cnblogs.com/hellcat/p/9046655.htm

## 4.1 模型构造

> 首先构造`Sequential`实例，然后依次添加两个全连接层。其中第一层的输出大小为256，即隐藏层单元个数是256；第二层的输出大小为10，即输出层单元个数是10。

### 4.1.1 继承Block类来构造模型

Block类是nn模块里提供的一个模型构造类，我们可以继承它来定义我们想要的模型。

```
from mxnet import nd
from mxnet.gluon import nn

class MLP(nn.Block):
	#声明带有模型参数的层，这里声明了两个全连接层
    def __init__(self, **kwargs):
    	#调用MLP父类Block的构造函数来进行必要的初始化。这样在构造   	  #实例时还可以指定其他函数参数，如“模型参数的访问、初始化和         #共享”一节将介绍的模型参数params
        super(MLP, self).__init__(**kwargs)
        #隐藏层
        self.hidden = nn.Dense(256, activation='relu')
        #输出层
        self.output = nn.Dense(10)
    #定义模型的向前计算，即如何根据输入x计算返回所需要的模型输出 
    def  forward(self, x):
        return self.output(self.hidden(x))
    
```

> 以上的MLP类中无须定义反向传播函数。系统将通过自动求梯度而自动反向传播所需的backward函数

```
X = nd.random.uniform(shape=(2,20))
net = MLP()
net.initialize()
net(X)
print("net(X)",net(X))
```

> 实例化MLP类得到模型变量net。代码初始化net并传入输入数据X做一次前向计算。其中，net(X)会调用MLP继承自Block类的__call__函数，这个函数将调用MLP类定义的forward函数来完成前向计算。
>
> [前向计算]: https://www.cnblogs.com/shona/p/10938533.htm

### 4.1.2 Sequential类继承自Block类

Sequential类目的：它提供add函数来逐一添加串联的Block子类实例，而模型的前向计算就是将这些实例按添加顺序逐一计算。

```
class MySequential(nn.Block):
    def __init__(self, **kwargs):
        super(MySequential, self).__init__(**kwargs)

    def add(self, block):
        # block是一个Block子类实例，假设它有一个独一无二的名字。我们将它保存在Block类的
        # 成员变量_children里，其类型是OrderedDict。当MySequential实例调用
        # initialize函数时，系统会自动对_children里所有成员初始化
        self._children[block.name] = block

    def forward(self, x):
        # OrderedDict保证会按照成员添加时的顺序遍历成员
        for block in self._children.values():
            x = block(x)
        return x
```

```
net = MySequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize()
net(X)
```

### 4.1.3 构造复杂的模型

> 通过`get_constant`函数创建训练中不被迭代的参数，即常数参数。

```
class FancyMLP(nn.Block):
    def __init__(self, **kwargs):
        super(FancyMLP, self).__init__(**kwargs)
        # 使用get_constant创建的随机权重参数不会在训练中被迭代（即常数参数）
        self.rand_weight = self.params.get_constant(
            'rand_weight', nd.random.uniform(shape=(20, 20)))
        self.dense = nn.Dense(20, activation='relu')

    def forward(self, x):
        x = self.dense(x)
        # 使用创建的常数参数，以及NDArray的relu函数和dot函数
        x = nd.relu(nd.dot(x, self.rand_weight.data()) + 1)
        # 复用全连接层。等价于两个全连接层共享参数
        x = self.dense(x)
        # 控制流，这里我们需要调用asscalar函数来返回标量进行比较
        while x.norm().asscalar() > 1:
            x /= 2
        if x.norm().asscalar() < 0.8:
            x *= 10
        return x.sum()
```

> 在这个`FancyMLP`模型中，我们使用了常数权重`rand_weight`（注意它不是模型参数）、做了矩阵乘法操作（`nd.dot`）并重复使用了相同的`Dense`层。

```
#该模型的随机初始化和前向计算
net = FancyMLP()
net.initialize()
net(X)
```

> 嵌套调用

```
class NestMLP(nn.Block):
    def __init__(self, **kwargs):
        super(NestMLP, self).__init__(**kwargs)
        self.net = nn.Sequential()
        self.net.add(nn.Dense(64, activation='relu'),
                     nn.Dense(32, activation='relu'))
        self.dense = nn.Dense(16, activation='relu')

    def forward(self, x):
        return self.dense(self.net(x))

net = nn.Sequential()
net.add(NestMLP(), nn.Dense(20), FancyMLP())

net.initialize()
net(X)
```

## 小结

- 可以通过继承`Block`类来构造模型。
- `Sequential`类继承自`Block`类。
- 虽然`Sequential`类可以使模型构造更加简单，但直接继承`Block`类可以极大地拓展模型构造的灵活性。

## 4.2 模型参数的访问、初始化和共享

> 从MXNet中导入init模块，它包含了多种模型初始化方法

```
from mxnet import init, nd
from mxnet.gluon import nn

net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize()  # 使用默认初始化方式

X = nd.random.uniform(shape=(2, 20))
Y = net(X)  # 前向计算
```

```
net[0].params, type(net[0].params)
out:
(dense0_ (
   Parameter dense0_weight (shape=(256, 20), dtype=float32)
   Parameter dense0_bias (shape=(256,), dtype=float32)
 ),
 mxnet.gluon.parameter.ParameterDict)
```

> ​	Gluon里参数类型为`Parameter`类，它包含参数和梯度的数值，可以分别通过`data`函数和`grad`函数来访问。

> 使用`collect_params`函数来获取`net`变量所有嵌套（例如通过`add`函数嵌套）的层所包含的所有参数。它返回的同样是一个由参数名称到参数实例的字典。

### 初始化模型参数

```
# 非首次对模型初始化需要指定force_reinit为真
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)
net[0].weight.data()[0]
```

> 使用常数来初始化权重参数

```
net.initialize(init=init.Constant(1), force_reinit=True)
net[0].weight.data()[0]
```

### 自定义初始化方法

```
class MyInit(init.Initializer):
    def _init_weight(self, name, data):
        print('Init', name, data.shape)
        data[:] = nd.random.uniform(low=-10, high=10, shape=data.shape)
        data *= data.abs() >= 5

net.initialize(MyInit(), force_reinit=True)
net[0].weight.data()[0]
```

### 共享模型参数

> 法2：它在构造层的时候指定使用特定的参数。如果不同层使用同一份参数，那么它们在前向计算和反向传播时都会共享相同的参数。

```
net = nn.Sequential()
shared = nn.Dense(8, activation='relu')
net.add(nn.Dense(8, activation='relu'),
        shared,
        nn.Dense(8, activation='relu', params=shared.params),
        nn.Dense(10))
net.initialize()

X = nd.random.uniform(shape=(2, 20))
net(X)

net[1].weight.data()[0] == net[2].weight.data()[0]
```

> 让模型的第二隐藏层（`shared`变量）和第三隐藏层共享模型参数。

### 小结

- 有多种方法来访问、初始化和共享模型参数。
- 可以自定义初始化方法。

## 模型参数的延后初始化

### 延后初始化

```
from mxnet import init, nd
from mxnet.gluon import nn

class MyInit(init.Initializer):
    def _init_weight(self, name, data):
        print('Init', name, data.shape)
        # 实际的初始化逻辑在此省略了

net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'),
        nn.Dense(10))

net.initialize(init=MyInit())
```

> 系统将真正的参数初始化延后到获得足够信息时才执行的行为叫作延后初始化。它可以让模型的创建更加简单：只需要定义每个层的输出大小，而不用人工推测它们的输入个数。这对于之后将介绍的定义多达数十甚至数百层的网络来说尤其方便。

系统将真正的参数初始化延后到获得足够信息时才执行的行为叫作延后初始化（deferred initialization）。它可以让模型的创建更加简单：只需要定义每个层的输出大小，而不用人工推测它们的输入个数。这对于之后将介绍的定义多达数十甚至数百层的网络来说尤其方便。

然而，任何事物都有两面性。正如本节开头提到的那样，延后初始化也可能会带来一定的困惑。在第一次前向计算之前，我们无法直接操作模型参数，例如无法使用`data`函数和`set_data`函数来获取和修改参数。因此，我们经常会额外做一次前向计算来迫使参数被真正地初始化。

### 避免延后初始化

> 第一种情况是我们要对已初始化的模型重新初始化时。因为参数形状不会发生变化，所以系统能够立即进行重新初始化。

```
net.initialize(init=MyInit(), force_reinit=True)
```

> 第二种情况是我们在创建层的时候指定了它的输入个数，使系统不需要额外的信息来推测参数形状。下例中我们通过`in_units`来指定每个全连接层的输入个数，使初始化能够在`initialize`函数被调用时立即发生。

```
net = nn.Sequential()
net.add(nn.Dense(256, in_units=20, activation='relu'))
net.add(nn.Dense(10, in_units=256))

net.initialize(init=MyInit())
```

### 小结

- 系统将真正的参数初始化延后到获得足够信息时才执行的行为叫作延后初始化。
- 延后初始化的主要好处是让模型构造更加简单。例如，我们无须人工推测每个层的输入个数。
- 也可以避免延后初始化。

## 自定义层

### 4.4.1 不含模型参数的自定义层

> `CenteredLayer`类通过继承`Block`类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了`forward`函数里。

```
from mxnet import gluon, nd
from mxnet.gluon import nn

class CenteredLayer(nn.Block):
    def __init__(self, **kwargs):
        super(CenteredLayer, self).__init__(**kwargs)

    def forward(self, x):
        return x - x.mean()#平均值
```

```
layer = CenteredLayer()
layer(nd.array([1, 2, 3, 4, 5]))
```

```
net = nn.Sequential()
net.add(nn.Dense(128),
        CenteredLayer())
```

```
打印自定义层各个输出的均值
net.initialize()
y = net(nd.random.uniform(shape=(4, 8)))
y.mean().asscalar()
```

### 含模型参数的自定义层

> 利用`Block`类自带的`ParameterDict`类型的成员变量`params`。它是一个由字符串类型的参数名字映射到Parameter类型的模型参数的字典。我们可以通过`get`函数从`ParameterDict`创建`Parameter`实例。

```
params = gluon.ParameterDict()
params.get('param2', shape=(2, 3))
params
```

```
class MyDense(nn.Block):
    # units为该层的输出个数，in_units为该层的输入个数
    def __init__(self, units, in_units, **kwargs):
        super(MyDense, self).__init__(**kwargs)
        self.weight = self.params.get('weight', shape=(in_units, units))
        self.bias = self.params.get('bias', shape=(units,))

    def forward(self, x):
        linear = nd.dot(x, self.weight.data()) + self.bias.data()
        return nd.relu(linear)
```

> 例化`MyDense`类并访问它的模型参数。

```
dense = MyDense(units=3, in_units=5)
dense.params
```

> 使用自定义层做前向计算。

```
dense.initialize()
dense(nd.random.uniform(shape=(2, 5)))
```

```
net = nn.Sequential()
net.add(MyDense(8, in_units=64),
        MyDense(1, in_units=8))
net.initialize()
net(nd.random.uniform(shape=(2, 64)))
```

### 小结

- 可以通过`Block`类自定义神经网络中的层，从而可以被重复调用。

## 4.5 读取和储存

### 4.5.1 读写NDArray

> save函数和load函数分别储存和读取NDArray

```
from mxnet import nd
from mxnet.gluon import nn

x = nd.ones(3)
nd.save('x', x)
x2 = nd.load('x')
x2
```

> 然后我们将数据从存储的文件读回内存。

```
y = nd.zeros(4)
nd.save('xy', [x, y])
x2, y2 = nd.load('xy')
(x2, y2)
```

> 存储一列`NDArray`并读回内存。

```
mydict = {'x': x, 'y': y}
nd.save('mydict', mydict)
mydict2 = nd.load('mydict')
mydict2
```

> 存储并读取一个从字符串映射到`NDArray`的字典。

### 读写Gluon模型的参数

> Gluon的`Block`类提供了`save_parameters`函数和`load_parameters`函数来读写模型参数。

> 创建一个多层感知机，并将其初始化

```
class MLP(nn.Block):
    def __init__(self, **kwargs):
        super(MLP, self).__init__(**kwargs)
        self.hidden = nn.Dense(256, activation='relu')
        self.output = nn.Dense(10)

    def forward(self, x):
        return self.output(self.hidden(x))

net = MLP()
net.initialize()
X = nd.random.uniform(shape=(2, 20))
Y = net(X)
```

```
filename = 'mlp.params'
net.save_parameters(filename)
net2 = MLP()
net2.load_parameters(filename)
Y2 = net2(X)
Y2 == Y
```

> 实例化一次定义好的多层感知机，读取保存在文件中的参数

### 小结

- 通过`save`函数和`load`函数可以很方便地读写`NDArray`。
- 通过`load_parameters`函数和`save_parameters`函数可以很方便地读写Gluon模型的参数。