# 《第一章》

#### 1.3表示学习

**表示学习**（ Representation Learning）：如果有一种算法可以自动地学习出有效的特征， 并提高最终机器学习模型的性能 .  

- **局部表示**（ Local Representation）：也叫离散表示，符号表示，例如one-hot向量。
- **分布式表示**（ Distributed Representation）：例如RGB值，通常表示低维的稠密向量。

**嵌入**（Embedding）：使用神经网络，将高维的局部表示空间映射到非常低维的分布式表示空间，在这个低维的空间中，每个特征不再是坐标轴上的点，而是分散在整个低维空间中。自然语言中的分布式表示，也叫**词嵌入**。

#### 1.4深度学习

**深度学习**（Deep Learning）：为了学习一种好的表示， 需要构建具有一定“深度”的模型， 并通过学习算法来让模型自动学习出好的特征表示（ 从底层特征， 到中层特征， 再到高层特征），从而最终提升预测模型的准确率. 所谓“深度” 是指**原始数据进行非线性特征转换的次数**. 如果把一个表示学习系统看作是一个有向图结构， 深度也可以看作是从输入节点到输出节点所经过的最长路径的长度.  某种意义上可以看作一种强化学习（Reinforcement Learning）

深度学习采用的模型主要是**神经网络模型**， 其主要原因是神经网络模型可以使用误差反向传播算法， 从而可以比较好地解决贡献度分配问题.   随着模型深度的不断增加， 其特征表示的能力也越来越强， 从而
使后续的预测更加容易  

**端到端学习**（ End-to-End Learning）： 也称端到端训练， 是指在学习过程中不进行分模块或分阶段训练， 直接优化任务的总体目标. 在端到端学习中， 一般不需要明确地给出不同模块或阶段的功能， 中间过程不需要人为干预.   大部分采用神经网络模型的深度学习可以看作一种端到端的学习。

#### 1.5神经网络

**神经网络**（机器学习领域）：由很多人工神经元构成的网络结构模型， 这些人工神经元之间的连接强度是可学习的参数.  

**赫布型学习**（ Hebbian learning）：如果两个神经元总是相关联地受到刺激， 它们之间的突触强度增加.  

**凝固作用** ：短期记忆转化为长期记忆的过程 

**网络容量**（ Network Capacity）：指人工神经网络塑造复杂函数的能力， 与可以被储存在网络中的信息的复杂度以及数量相关

**1.8总结**

**特征工程**：要开发一个实际的机器学习系统， 人们往往需要花费大量的精力去尝试设计不同的特征以及特征组合， 来提高最终的系统能力。

