# 陈蕴侬视频

#### 二、模型结构、损失函数、优化、反向传播

偏差（bias)的理解：相当于给一个初值，然后通过学习调整这个初值。

感知层（perception layer）的理解：每一层相当于一个切割，可以通过二层模拟出一个凸，越多层表达越多场景。

激活函数（activate function）：选非线性的，线性跟权重没差。

损失函数（loss function）：定义一个损失值，越小越接近正确的参数值。

梯度下降（Gradient Descent）的理解：越倾斜，下降越快，越平稳下降越慢；容易达到局部最小值，卡在局部。随机小批量梯度下降（SGD，选1个）比较快。小批量梯度下降（Mini-Batch GD，选k个）.

训练速度：mini-batch>SGD>GD，因为现代电脑矩阵相乘的速度大于矩阵和向量相乘。

学习率：过大会学习过头，越过最小值。过小会学的很慢。

建议：1.数据随机；2.使用固定批量；3.调整学习率。

反向传播（backward propagation）：通过梯度和学习率更新权重。其实就是微积分链式法则在模型中的体现。反向传播计算出的梯度乘以前向计算的结果，就是下一个变数的偏微分了。

![image-20200714172502220](http://qclf7esue.bkt.clouddn.com/20200714172521.png)

#### 三、语言表达、RNN、批量

共现矩阵：表示一起出现过的单词的关系。

奇异值分解（singular value decomposition，SVD）:降低维度。

SVD问题：计算复杂度过高，难以加新词。

解决方法：降低维度，通过embedding的方法嵌入一个空间中的位置。常用word2vec，Glove方法。

知识型表示（knowledge-based representation）：通过符号等来表示知识（知识图谱）

语料库表示（corpus-based representation）：基于近邻的高维（共现矩阵），低维（降维或embedding）；原子特征（atomic symbol，one-hot向量）

循环神经网络(recurrent neural net，RNN)：将前面的影响传递给后面的网络。

梯度消失，梯度爆炸（Vanishing/Exploding Gradient）：指数太多次，导致大的越大，小的越小。解决方法：裁剪（clipping）

双向循环神经网络（Bidirectional RNN）：当时间可以双向的时候，可以使用。（不能预测股市这种单向时间的）

编码器-解码器：编码器生成W或背景向量C，解码器利用编码器结果来生成输出。

批量归一化计算：先归一化，后缩放和平移。

从经验法则来讲，L2正则化一般比L1正则化有效。

#### 四、注意力机制

编码器-解码器实现注意力机制：编码器收集信息，收集完一整句（注意力在这）之后，保存在编码器，用解码器生成输出，直到遇到<end>。

Q,K,V：Q是指query，K是指编码器中的key，V是指最后一层的Value。

#### 五、word2vec

最大化和最小化：多个概率相乘求其最大值，相当于对其求log后加个负号求最小值。也就是说，凡是求最大值的，都可以通过符号变成求最小值。