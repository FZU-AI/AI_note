{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import d2lzh as d2l\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.contrib import text\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, rnn, utils as gutils\n",
    "import os\n",
    "import random\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh包中方便以后使用\n",
    "def download_imdb(data_dir='../data'):\n",
    "    url = ('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n",
    "    sha1 = '01ada507287d82875905620988597833ad4e0903'\n",
    "    fname = gutils.download(url, data_dir, sha1_hash=sha1)\n",
    "    with tarfile.open(fname, 'r') as f:\n",
    "        f.extractall(data_dir)\n",
    "\n",
    "download_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imdb(folder='train'):  # 本函数已保存在d2lzh包中方便以后使用\n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join('../data/aclImdb/', folder, label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "train_data, test_data = read_imdb('train'), read_imdb('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "# 基于空格分词\n",
    "def get_tokenized_imdb(data):  # 本函数已保存在d2lzh包中方便以后使用\n",
    "#     def tokenizer(text):\n",
    "#         return [tok.lower() for tok in text.split(' ')]\n",
    "    def tokenizer(text): \n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "    return [tokenizer(review) for review, _ in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('# words in vocab:', 31278)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 过滤掉次数少于5的词\n",
    "def get_vocab_imdb(data):  # 本函数已保存在d2lzh包中方便以后使用\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return text.vocab.Vocabulary(counter, min_freq=5,\n",
    "                                 reserved_tokens=['<pad>'])\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "'# words in vocab:', len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb(data, vocab):  # 本函数已保存在d2lzh包中方便以后使用\n",
    "    max_l = 500  # 将每条评论通过截断或者补'<pad>'，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [\n",
    "            vocab.token_to_idx['<pad>']] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    features = nd.array([pad(vocab.to_indices(x)) for x in tokenized_data])\n",
    "    labels = nd.array([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = gdata.ArrayDataset(*preprocess_imdb(train_data, vocab))\n",
    "test_set = gdata.ArrayDataset(*preprocess_imdb(test_data, vocab))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (64, 500) y (64,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches:', 391)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Block):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # bidirectional设为True即得到双向神经循环网络\n",
    "        self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers, bidirectional=True, input_size=embed_size)\n",
    "        self.decoder = nn.Dense(2)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # inputs的形状（批量，词数）因为LSTM需要序列作为第一维度，\n",
    "        # 所以输入转置为（词数，批量），提取词特征，输出形状为（词数，批量，词向量维度）\n",
    "        embeddings = self.embedding(input.T)\n",
    "        # rnn.LSTM只输入embeddings,因此只返回最后一层的隐藏层在各个时间步上的隐藏状态。\n",
    "        # outputs的形状是（词数， 批量， 2*隐藏单元个数）\n",
    "        outputs = self.encoder(embeddings)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        # 形状为（批量， 4*隐藏单元个数）\n",
    "        encoding = nd.concat(outputs[0], outputs[-1])\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, ctx = 100, 100, 2, d2l.try_all_gpus()\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)\n",
    "net.initialize(init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载词向量\n",
    "glove_embedding = text.embedding.create(\n",
    "    'glove', pretrained_file_name='glove.6B.100d.txt', vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练词向量的维度需要与创建的模型中的嵌入层输出大小embed_size一致\n",
    "net.embedding.weight.set_data(glove_embedding.idx_to_vec)\n",
    "# 设置空梯度不再更新向量\n",
    "net.embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on [gpu(0)]\n",
      "epoch 1, loss 0.4989, train acc 0.743, test acc 0.840, time 40.3 sec\n",
      "epoch 2, loss 0.3526, train acc 0.852, test acc 0.857, time 41.9 sec\n",
      "epoch 3, loss 0.3076, train acc 0.872, test acc 0.862, time 43.3 sec\n",
      "epoch 4, loss 0.2753, train acc 0.888, test acc 0.860, time 44.0 sec\n",
      "epoch 5, loss 0.2433, train acc 0.902, test acc 0.852, time 44.2 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh包中方便以后使用\n",
    "def predict_sentiment(net, vocab, sentence):\n",
    "    sentence = nd.array(vocab.to_indices(sentence), ctx=d2l.try_gpu())\n",
    "    label = nd.argmax(net(sentence.reshape((1, -1))), axis=1)\n",
    "    return 'positive' if label.asscalar() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'beautiful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'stupid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
