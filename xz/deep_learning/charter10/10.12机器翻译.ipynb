{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import io\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.contrib import text\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, rnn\n",
    "\n",
    "def softmax(X):   #softmax函数\n",
    "        return np.exp(X) / np.sum(np.exp(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nd.array([[1,2,3],[2,4,6]])\n",
    "test.reshape([-1])\n",
    "\n",
    "# <pad>”（padding）符号用来添加在较短序列后，直到每个序列等长，\n",
    "# 而“<bos>”和“<eos>”符号分别表示序列的开始和结束。\n",
    "PAD, BOS, EOS = '<pad>', '<bos>', '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列\n",
    "# 长度变为max_seq_len，然后将序列保存在all_seqs中\n",
    "def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):\n",
    "    all_tokens.extend(seq_tokens)\n",
    "    seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)\n",
    "    all_seqs.append(seq_tokens)\n",
    "    \n",
    "# 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造NDArray实例\n",
    "def build_data(all_tokens, all_seqs):\n",
    "    # 构造词典\n",
    "    vocab = text.vocab.Vocabulary(collections.Counter(all_tokens),\n",
    "                                  reserved_tokens=[PAD, BOS, EOS])\n",
    "    # 转换为索引\n",
    "    indices = [vocab.to_indices(seq) for seq in all_seqs]\n",
    "    return vocab, nd.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(max_seq_len):\n",
    "    # in和out分别是input和output的缩写\n",
    "    in_tokens, out_tokens, in_seqs, out_seqs = [], [], [], []\n",
    "    with io.open('../data/fr-en-small.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        # in是法语，out是英语\n",
    "        in_seq, out_seq = line.rstrip().split('\\t')\n",
    "        in_seq_tokens, out_seq_tokens = in_seq.split(' '), out_seq.split(' ')\n",
    "        if max(len(in_seq_tokens), len(out_seq_tokens)) > max_seq_len - 1:\n",
    "            continue  # 如果加上EOS后长于max_seq_len，则忽略掉此样本\n",
    "        process_one_seq(in_seq_tokens, in_tokens, in_seqs, max_seq_len)\n",
    "        process_one_seq(out_seq_tokens, out_tokens, out_seqs, max_seq_len)\n",
    "    in_vocab, in_data = build_data(in_tokens, in_seqs)\n",
    "    out_vocab, out_data = build_data(out_tokens, out_seqs)\n",
    "    return in_vocab, out_vocab, gdata.ArrayDataset(in_data, out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [ 6.  5. 46.  4.  3.  1.  1.]\n",
       " <NDArray 7 @cpu(0)>,\n",
       " \n",
       " [ 9.  5. 28.  4.  3.  1.  1.]\n",
       " <NDArray 7 @cpu(0)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = 7\n",
    "in_vocab, out_vocab, dataset = read_data(max_seq_len)\n",
    "dataset[0] # 该样本分别包含法语词索引序列和英语词索引序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 drop_prob=0, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=drop_prob)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # 输入形状是(批量大小, 时间步数)。将输出互换样本维和时间步维\n",
    "        embedding = self.embedding(inputs).swapaxes(0, 1)\n",
    "        return self.rnn(embedding, state)\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, 4, 16), (2, 4, 16))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.initialize()\n",
    "output, state = encoder(nd.zeros((4, 7)), encoder.begin_state(batch_size=4))\n",
    "# state列表中只含一个元素，即隐藏状态；如果使用长短期记忆，state列表中还将包含另一个元素，即记忆细胞。\n",
    "output.shape, state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 希望全连接层只对输入的最后一维做仿射变换，\n",
    "# 而保持其他维度上的形状不变，便需要将Dense实例的flatten选项设为False。\n",
    "dense = nn.Dense(2, flatten=False)\n",
    "dense.initialize()\n",
    "dense(nd.zeros((3, 5, 7))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现“注意力机制”一节中定义的函数 a\n",
    "def attention_model(attention_size):\n",
    "    model = nn.Sequential()\n",
    "    model.add(nn.Dense(attention_size, activation='tanh', use_bias=False,\n",
    "                       flatten=False),\n",
    "              nn.Dense(1, use_bias=False, flatten=False))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_forward(model, enc_states, dec_state):\n",
    "    # 将解码器隐藏状态广播到和编码器隐藏状态形状相同后进行连结\n",
    "    dec_states = nd.broadcast_axis(dec_state.expand_dims(0), axis=0, size=enc_states.shape[0])\n",
    "    enc_and_dec_states = nd.concat(enc_states, dec_states, dim=2)\n",
    "    e = model(enc_and_dec_states)  # 形状为(时间步数, 批量大小, 1)\n",
    "    alpha = nd.softmax(e, axis=0)  # 在时间步维度做softmax运算\n",
    "    return (alpha * enc_states).sum(axis=0)  # 返回背景变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len, batch_size, num_hiddens = 10, 4, 8\n",
    "model = attention_model(10)\n",
    "model.initialize()\n",
    "enc_states = nd.zeros((seq_len, batch_size, num_hiddens))\n",
    "dec_state = nd.zeros((batch_size, num_hiddens))\n",
    "attention_forward(model, enc_states, dec_state).shape\n",
    "# （批量大小，隐藏单元个数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 attention_size, drop_prob=0, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = attention_model(attention_size)\n",
    "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=drop_prob)\n",
    "        self.out = nn.Dense(vocab_size, flatten=False)\n",
    "\n",
    "    def forward(self, cur_input, state, enc_states):\n",
    "        # 使用注意力机制计算背景向量\n",
    "        c = attention_forward(self.attention, enc_states, state[0][-1])\n",
    "        # 将嵌入后的输入和背景向量在特征维连结\n",
    "        input_and_c = nd.concat(self.embedding(cur_input), c, dim=1)\n",
    "        # 为输入和背景向量的连结增加时间步维，时间步个数为1\n",
    "        output, state = self.rnn(input_and_c.expand_dims(0), state)\n",
    "        # 移除时间步维，输出形状为(批量大小, 输出词典大小)\n",
    "        output = self.out(output).squeeze(axis=0)\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, enc_state):\n",
    "        # 直接将编码器最终时间步的隐藏状态作为解码器的初始隐藏状态\n",
    "        return enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小批量的损失\n",
    "def batch_loss(encoder, decoder, X, Y, loss):\n",
    "    batch_size = X.shape[0]\n",
    "    enc_state = encoder.begin_state(batch_size=batch_size)\n",
    "    enc_outputs, enc_state = encoder(X, enc_state)\n",
    "    # 初始化解码器的隐藏状态\n",
    "    dec_state = decoder.begin_state(enc_state)\n",
    "    # 解码器在最初时间步的输入是BOS\n",
    "    dec_input = nd.array([out_vocab.token_to_idx[BOS]] * batch_size)\n",
    "    # 我们将使用掩码变量mask来忽略掉标签为填充项PAD的损失\n",
    "    mask, num_not_pad_tokens = nd.ones(shape=(batch_size,)), 0\n",
    "    l = nd.array([0])\n",
    "    for y in Y.T:\n",
    "        dec_output, dec_state = decoder(dec_input, dec_state, enc_outputs)\n",
    "        l = l + (mask * loss(dec_output, y)).sum()\n",
    "        dec_input = y  # 使用强制教学\n",
    "        num_not_pad_tokens += mask.sum().asscalar()\n",
    "        # 当遇到EOS时，序列后面的词将均为PAD，相应位置的掩码设成0\n",
    "        mask = mask * (y != out_vocab.token_to_idx[EOS])\n",
    "    return l / num_not_pad_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataset, lr, batch_size, num_epochs):\n",
    "    encoder.initialize(init.Xavier(), force_reinit=True)\n",
    "    decoder.initialize(init.Xavier(), force_reinit=True)\n",
    "    enc_trainer = gluon.Trainer(encoder.collect_params(), 'adam',\n",
    "                                {'learning_rate': lr})\n",
    "    dec_trainer = gluon.Trainer(decoder.collect_params(), 'adam',\n",
    "                                {'learning_rate': lr})\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum = 0.0\n",
    "        for X, Y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = batch_loss(encoder, decoder, X, Y, loss)\n",
    "            l.backward()\n",
    "            enc_trainer.step(1)\n",
    "            dec_trainer.step(1)\n",
    "            l_sum += l.asscalar()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"epoch %d, loss %.3f\" % (epoch + 1, l_sum / len(data_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 0.575\n",
      "epoch 20, loss 0.269\n",
      "epoch 30, loss 0.235\n",
      "epoch 40, loss 0.166\n",
      "epoch 50, loss 0.063\n"
     ]
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers = 64, 64, 2\n",
    "attention_size, drop_prob, lr, batch_size, num_epochs = 10, 0.5, 0.01, 2, 50\n",
    "encoder = Encoder(len(in_vocab), embed_size, num_hiddens, num_layers,\n",
    "                  drop_prob)\n",
    "decoder = Decoder(len(out_vocab), embed_size, num_hiddens, num_layers,\n",
    "                  attention_size, drop_prob)\n",
    "train(encoder, decoder, dataset, lr, batch_size, num_epochs)\n",
    "\n",
    "# 生成解码器在每个时间步的输出，贪婪搜索\n",
    "def translate(encoder, decoder, input_seq, max_seq_len):\n",
    "    in_tokens = input_seq.split(' ')\n",
    "    in_tokens += [EOS] + [PAD] * (max_seq_len - len(in_tokens) - 1)\n",
    "    enc_input = nd.array([in_vocab.to_indices(in_tokens)])\n",
    "    enc_state = encoder.begin_state(batch_size=1)\n",
    "    enc_output, enc_state = encoder(enc_input, enc_state)\n",
    "    dec_input = nd.array([out_vocab.token_to_idx[BOS]])\n",
    "    dec_state = decoder.begin_state(enc_state)\n",
    "    output_tokens = []\n",
    "    for _ in range(max_seq_len):\n",
    "        dec_output, dec_state = decoder(dec_input, dec_state, enc_output)\n",
    "        pred = dec_output.argmax(axis=1)\n",
    "        pred_token = out_vocab.idx_to_token[int(pred.asscalar())]\n",
    "        if pred_token == EOS:  # 当任一时间步搜索出EOS时，输出序列即完成\n",
    "            break\n",
    "        else:\n",
    "            output_tokens.append(pred_token)\n",
    "            dec_input = pred\n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'are', 'watching', '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = 'ils regardent .'\n",
    "translate(encoder, decoder, input_seq, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.97 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['they', 'are', 'watching', '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = 'ils regardent .'\n",
    "translate(encoder, decoder, input_seq, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.09 ms ± 84.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "input_seq = 'ils regardent .'\n",
    "%timeit translate(encoder, decoder, input_seq, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_tokens, label_tokens, k):\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[''.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[''.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[''.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(input_seq, label_seq, k):\n",
    "    pred_tokens = translate(encoder, decoder, input_seq, max_seq_len)\n",
    "    label_tokens = label_seq.split(' ')\n",
    "    print('bleu %.3f, predict: %s' % (bleu(pred_tokens, label_tokens, k),\n",
    "                                      ' '.join(pred_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu 1.000, predict: they are watching .\n"
     ]
    }
   ],
   "source": [
    "score('ils regardent .', 'they are watching .', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu 0.658, predict: they are actors .\n"
     ]
    }
   ],
   "source": [
    "score('ils sont canadiens .', 'they are canadian .', k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
