# 《第一章》《第二章》

## 《第一章》

### 机器学习

研究如何使计算机系统利用经验改善性能。

### 表征学习

关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出。

### 深度学习

具有多级表示的表征学习方法。在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合的函数足够多时，深度学习模型就可以表达非常复杂的变换。

### 特点

1. 端到端的训练——将整个系统组建好之后一起训练。（整个模型一起训练，调整）
2. 从含参数统计模型转向完全无参数的模型——当数据非常稀缺时，我们需要通过简化对现实的假设来得到实用的模型。当数据充足时，我们就可以用能更好地拟合现实的无参数模型来替代这些含参数模型。（像人类一样，有经验根据经验判断，没经验用直觉）

### 不同之处

对非最优解的包容、对非凸非线性优化的使用，以及勇于尝试没有被证明过的方法。（某种意义上的直觉）

## 《第二章》

### 2.1. 环境搭建

1. 踩了个大坑，不要直接下载NVIDIA官网的cuda11.0，找不到mxnet-cu110的包。
2. 如果下载mxnet-cu101，也需要下载cuda10.1版本，才能import mxnet。
3. 在import mxnet的时候发现了dll文件找不到，下载了最新的vc和一个dependences walker软件查看dll。而后重启jupyter内核，莫名奇妙解决了，至今不知问题在哪。
4. 能翻墙就翻墙吧，然后不要乱配置源，保持原始的就可以了，有些包在国内版本比较低，有冲突。
5. 学会使用conda管理包，以及查看下载包时候出现的异常，尤其是标红的，很重要，不要轻易忽略，不然就是下一个坑。
6. jupyter lab也挺好用的，但是第一次使用，由于各个包版本没有搞好，导致404无法连接服务器，也就无法运行python，原因如第4点所说。
7. 吐槽一下，在使用jupyter时候，好像不能同时打开同一个虚拟环境的另一个命令行，也就难以同时进行包管理。或者是我打开方式不对？

### 2.2. 数据操作

1. NDArray\(N维数组，也就是张量\)
2. arange\(n\)——\[0,1...,n\]，理解为a指一维，range指范围
3. shape——数组的形状，即每个维度的长度所组合成的元组，（3，4）表示三行四列
4. size——元素总数
5. reshape——将数组变成指定形状的数组
6. zeros默认置零，ones默认置1，记得因为复数，有s
7. array——多维数组最普通的定义方式，array\(\)括号里的即为数组
8. nd.random.normal\(a,b,shape=\(3,4\)\)——random指随机，normal指正态分布,a-b范围
9. +-\*/，还有==，都是逐个元素进行的。exp\(\),dot\(\)也是，其中dot记得通过X.T转置
10. concat\(X,Y,dim=0\)，将第0个维度（shape中的第0个元素）保持不变，其他结合起来。
11. sum\(\)——获得所有元素的和，list内一个浮点数
12. norm\(\)——逐个元素的平方之和后开根号=欧几里得范数（长度）=L2范数（长度），是list
13. asscalar\(\)——as scalar,作为标量，即list--&gt;num
14. broadcasting，广播机制——犹如C语言中的类型自动转换，将多维数组的长度扩大到可以进行运算，以保证运算的正确性。经测试发现，只有一行或者一列才可以广播，此时无歧义。
15. 内存不变的操作：Z\[:\] = X + Y；nd.elemwise\_add\(X, Y, out=Z\)；X += Y; X\[:\] = X + Y
16. Y = Y + X; 会新开内存Y和临时内存X+Y
17. nd.array\(P\)——转换numpy--&gt;NDArray

    D.asnumpy\(\)——转换NDArray--&gt;numpy

### 2.2. 练习

* 运行本节中的代码。将本节中条件判别式`X == Y`改为`X < Y`或`X > Y`，看看能够得到什么样的`NDArray`。

  答：逐个判断元素地比较大小

* 将广播机制中按元素运算的两个`NDArray`替换成其他形状，结果是否和预期一样？

  答：一行或者一列才能广播

### 2.3. 自动求梯度

```text
from mxnet import autograd, nd
x = nd.arange(4).reshape((4, 1))
# 申请梯度的内存
x.attach_grad() 
# 在运算的过程中记录梯度，此时从预测模式转为训练模式
with autograd.record():
    y = 2 * nd.dot(x.T, x)
# 求梯度，y如果不是标量会求和
y.backward()
```

