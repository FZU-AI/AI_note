{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import init, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(ctx=mx.gpu())\n",
    "\n",
    "X = nd.random.uniform(shape=(2, 20), ctx=mx.gpu())\n",
    "Y = net(X) # 前向计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dense4_ (\n",
       "   Parameter dense4_weight (shape=(256, 20), dtype=float32)\n",
       "   Parameter dense4_bias (shape=(256,), dtype=float32)\n",
       " ),\n",
       " mxnet.gluon.parameter.ParameterDict)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].params, type(net[0].params) # 通过方括号[]来访问网络的任一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dense5_ (\n",
       "   Parameter dense5_weight (shape=(10, 256), dtype=float32)\n",
       "   Parameter dense5_bias (shape=(10,), dtype=float32)\n",
       " ),\n",
       " mxnet.gluon.parameter.ParameterDict)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].params, type(net[1].params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter dense4_weight (shape=(256, 20), dtype=float32),\n",
       " Parameter dense4_weight (shape=(256, 20), dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].params['dense4_weight'], net[0].weight # 两者方法等价\n",
    "# 每执行一次，会有增加一个dense？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.04247847  0.06293995 -0.01837847 ... -0.06219994  0.01467837\n",
       "  -0.00683771]\n",
       " [ 0.0334969  -0.06720173 -0.06451371 ... -0.0396449   0.0269461\n",
       "   0.00912645]\n",
       " [ 0.0093242   0.05111437 -0.03284547 ...  0.02060438  0.03028581\n",
       "   0.04779406]\n",
       " ...\n",
       " [ 0.0268476   0.06148554 -0.04265065 ...  0.00752284 -0.04634099\n",
       "  -0.06273054]\n",
       " [-0.00812264  0.01937782  0.05937877 ... -0.04052389  0.01052459\n",
       "   0.05550297]\n",
       " [-0.03866927  0.02501589  0.04987388 ... -0.02471803  0.03994805\n",
       "  -0.05723546]]\n",
       "<NDArray 256x20 @gpu(0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data() # 分别通过data函数和grad函数来访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " ...\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]]\n",
       "<NDArray 256x20 @gpu(0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还没有进行反向传播计算，所以梯度的值全为0\n",
    "net[0].weight.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "<NDArray 10 @gpu(0)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential2_ (\n",
       "  Parameter dense4_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense4_bias (shape=(256,), dtype=float32)\n",
       "  Parameter dense5_weight (shape=(10, 256), dtype=float32)\n",
       "  Parameter dense5_bias (shape=(10,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取net变量所有嵌套（例如通过add函数嵌套）的层所包含的所有参数。\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential2_ (\n",
       "  Parameter dense4_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense5_weight (shape=(10, 256), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params('.*weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.00958589 -0.01497647  0.00660516 -0.00189036 -0.00273026  0.00918154\n",
       "  0.02173063  0.000752    0.00071856  0.0112562  -0.00355964 -0.00253765\n",
       " -0.00040472  0.00496598  0.01380103  0.01802712  0.00645719  0.01811526\n",
       " -0.01252964  0.00387312]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 非首次对模型初始化需要指定force_reinit为真\n",
    "net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.14208373  0.1430282  -0.00637825  0.07477359 -0.00076924 -0.14624824\n",
       "  0.0411282  -0.06797681 -0.03875229 -0.02639443 -0.10707226 -0.02116564\n",
       "  0.09498733 -0.0596132  -0.09145886 -0.02915448  0.00333779 -0.11186215\n",
       " -0.08129447  0.1417506 ]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.cnblogs.com/adong7639/p/9547789.html\n",
    "# 下面权重的初始化是使用了一种特殊的均匀分布，可以参考上面教程\n",
    "net[0].weight.initialize(init=init.Xavier(), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init dense4_weight (256, 20)\n",
      "\n",
      "[[1. 0. 1. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 1. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]]\n",
      "<NDArray 256x20 @cpu(0)>\n",
      "Init dense5_weight (10, 256)\n",
      "\n",
      "[[1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 1. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 1.]]\n",
      "<NDArray 10x256 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 6.2993298 -0.         9.709829  -0.         9.379435   0.\n",
       "  8.098967   7.9414024 -0.        -7.578802   9.840225  -5.169086\n",
       " -5.011599  -5.4347277 -7.881877  -0.         9.0190525  7.830454\n",
       " -5.331595  -0.       ]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def _init_weight(self, name, data):\n",
    "        print('Init', name, data.shape)\n",
    "        data[:] = nd.random.uniform(low=-10, high=10, shape=data.shape)\n",
    "        # 绝对值小于5的设置为0\n",
    "        data *= data.abs() >= 5\n",
    "\n",
    "net.initialize(MyInit(), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 7.2993298  1.        10.709829   1.        10.379435   1.\n",
       "  9.098967   8.941402   1.        -6.578802  10.840225  -4.169086\n",
       " -4.011599  -4.4347277 -6.881877   1.        10.0190525  8.830454\n",
       " -4.331595   1.       ]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接设置权重+1\n",
    "net[0].weight.set_data(net[0].weight.data() + 1)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "shared = nn.Dense(8, activation='relu')\n",
    "# 我们让模型的第二隐藏层（shared变量）和第三隐藏层共享模型参数。\n",
    "net.add(nn.Dense(8, activation='relu'),\n",
    "        shared,\n",
    "        nn.Dense(8, activation='relu', params=shared.params),\n",
    "        nn.Dense(10))\n",
    "# net.initialize()\n",
    "\n",
    "# X = nd.random.uniform(shape=(2, 20))\n",
    "# net(X)\n",
    "\n",
    "# net[1].weight.data()[0] == net[2].weight.data()[0]\n",
    "# # 在反向传播计算时，第二隐藏层和第三隐藏层的梯度都会被累加在shared.params.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dense(None -> 8, Activation(relu)),\n",
       " Dense(None -> 8, Activation(relu)),\n",
       " Dense(None -> 8, Activation(relu)),\n",
       " Dense(None -> 10, linear))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0],net[1],net[2],net[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "<NDArray 8 @cpu(0)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "shared = nn.Dense(8, activation='relu')\n",
    "# 我们让模型的第二隐藏层（shared变量）和第三隐藏层共享模型参数。\n",
    "net.add(nn.Dense(8, activation='relu'),\n",
    "        shared,\n",
    "        nn.Dense(8, activation='relu', params=shared.params),\n",
    "        nn.Dense(10))\n",
    "net.initialize()\n",
    "\n",
    "X = nd.random.uniform(shape=(2, 20))\n",
    "net(X)\n",
    "\n",
    "net[1].weight.data()[0] == net[2].weight.data()[0]\n",
    "# 在反向传播计算时，第二隐藏层和第三隐藏层的梯度都会被累加在shared.params.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
