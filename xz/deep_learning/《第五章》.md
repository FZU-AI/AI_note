# 《第五章》

#### 5.1二维卷积层

1. 二维互相关运算：

![二维互相关运算](https://zh.d2l.ai/_images/correlation.svg)

2. 卷积运算：与互相关运算类似。为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。

3. 特征图（feature map）：二维卷积层输出的二维数组可以看作输入在空间维度（宽和高）上某一级的表征。

4. x的感受野（receptive field）：影响元素x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）。

5. 卷积神经网络：通过数据来学习卷积核

6. corr2d因为用了[i,j]=导致自动求导失败，这是由于autograd目前还有局限性。在该类的`forward`函数里，将`corr2d`函数替换成`nd.Convolution`类使得自动求梯度变得可行。

**练习**

- 构造一个输入图像`X`，令它有水平方向的边缘。如何设计卷积核`K`来检测图像中水平边缘？如果是对角方向的边缘呢？

  答：检测水平边缘的核函数应该是[[1],[-1]]；对角方向的核函数[1, -1]或[[1],[-1]]或[[1, 0],[-1, 0]]；

  对角方向的边缘监测方法有按水平的，按垂直的，或者都要。根据不同的方法，宽高会发生相应变化。

- 试着对我们自己构造的`Conv2D`类进行自动求梯度，会有什么样的错误信息？在该类的`forward`函数里，将`corr2d`函数替换成`nd.Convolution`类使得自动求梯度变得可行。

  答：infershape[attrs.op](attrs, &in_shapes, &out_shapes)

  corr2d因为用了[i,j]=导致自动求导失败，这是由于autograd目前还有局限性。改成 nd.Convolution 会没问题，因为它手动实现了backward函数。

- 如何通过变化输入和核数组将互相关运算表示成一个矩阵乘法？

  答：卷积变成矩阵乘法，从乘法和加法的运算次数上看，两者没什么差别，但是转化成矩阵后，运算时需要的数据被存在连续的内存上，这样访问速度大大提升。

  方法参考如下：https://www.cnblogs.com/marsggbo/p/12074297.html

- 如何构造一个全连接层来进行物体边缘检测？

  答：全连接层是指使用nn.Dense？

#### 5.2填充和步幅

$$
若输入矩阵:n_h\times n_w\\
且卷积核:k_h\times k_w\\
则输出形状:(n_h-k_h+1) \times (n_w-k_w+1)
$$

填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。

![在输入的高和宽两侧分别填充了0元素的二维互相关计算](https://zh.d2l.ai/_images/conv_pad.svg)
$$
若高填充p_h行，宽填充p_w行，则输出形状(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)\\
可以设置p_h=k_h-1,p_w=k_w-1使得输入和输出形状一致。
$$
步幅（stride）：我们将每次滑动的行数和列数。输出第一列第二个元素时，卷积窗口向下滑动了3行，而在输出第一行第二个元素时卷积窗口向右滑动了2列。当卷积窗口在输入上再向右滑动2列时，由于输入元素无法填满窗口，无结果输出。

![高和宽上步幅分别为3和2的二维互相关运算](https://zh.d2l.ai/_images/conv_stride.svg)
$$
当高上步幅为s_h ，宽上步幅为s_w 时，输出形状为:\\
\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor\\如果设置 p_h=k_h−1 和 p_w=k_w−1 ，那么输出形状将简化为 :\\⌊(n_h+s_h−1)/s_h⌋×⌊(n_w+s_w−1)/s_w⌋ \\
更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是 (n_h/s_h)×(n_w/s_w) 。
$$
**练习**

- 对本节最后一个例子通过形状计算公式来计算输出形状，看看是否和实验结果一致。

  一致，分别是8/3和8/4的下取整。

- 在本节实验中，试一试其他的填充和步幅组合。

#### 5.3多输入和多输出通道

通道（channel）维：例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。称其通道维为3.

![含2个输入通道的互相关计算](https://zh.d2l.ai/_images/conv_multi_in.svg)

多输出通道：每个输出通道上的结果由卷积核在**该输出通道上的核数组**与**整个输入数组**计算而来。

卷积窗口形状为1×1（k_h=k_w=1）的多通道卷积层。

假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×1卷积层的作用与全连接层等价。

**练习**

- 假设输入形状为ci×h×w，且使用形状为co×ci×kh×kw、填充为(ph,pw)、步幅为(sh,sw)的卷积核。那么这个卷积层的前向计算分别需要多少次乘法和加法？

  答：

  输出形状为：
  $$
  c_o\times\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor
  $$
  乘法次数：
  $$
  k_h\times k_w \times c_o\times\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor
  $$
  加法次数：
  $$
  [(k_h\times k_w)个数连加]\times(c_i个数连加)\times\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor
  $$

- 翻倍输入通道数ci和输出通道数co会增加多少倍计算？翻倍填充呢？

  答：通过上述公式可以看出，翻倍导致增加的计算程度大致为：c_o > c_i > p_h(p_w)

- 如果卷积核的高和宽kh=kw=1，能减少多少计算？

  答：乘法降低至1/(k_h乘k_w)，加法降低至1/(k_w*k_h-1)

- 本节最后一个例子中的变量`Y1`和`Y2`完全一致吗？原因是什么？

  答：不完全一致，一个是通过矩阵乘法的方式、一个是通过互相关计算。某个地方的精度不一样，比如说没有批量求和。

- 当卷积窗口不为1×1时，如何用矩阵乘法实现卷积计算？

  答：将卷积窗口reshape为行向量或者列向量。

#### 5.4池化层

池化层：池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。

![池化窗口形状为\ :math:`2\times 2`\ 的最大池化](https://zh.d2l.ai/_images/pooling.svg)

池化层的输出通道数与输入通道数相等。

```python
pool2d  = nn.MaxPool2D((2, 3), padding=(1, 2), strides=(2, 3)) # 二维最大池化层函数
pool2d(X) # 调用
```

池化层的一个主要作用是缓解卷积层对位置的过度敏感性。（把数组按局部矩阵最大值或者局部矩阵平均值缩小）

- 分析池化层的计算复杂度。假设输入形状为c×h×w，我们使用形状为ph×pw的池化窗口，而且使用(ph,pw)填充和(sh,sw)步幅。这个池化层的前向计算复杂度有多大？

  答：时间复杂度为
  $$
  c×⌊(h−k_h+p_h+s_h)/s_h⌋×⌊(w−k_w+p_w+s_w)/s_w⌋×(max的时间)
  $$
  
- 想一想，最大池化层和平均池化层在作用上可能有哪些区别？ 

  答：平均池化可以提取背景信息，减少冲击失真，模糊，平滑。最大池化可以提取特征纹理，增强图片亮度

- 觉得最小池化层这个想法有没有意义？

  答：因为像素值为正数决定的，0代表为黑色。采用最小池化很可能全部都是0。最小池化甚至会让你的神经网络轻易过拟合甚至无法训练。

#### 5.5卷积神经网络

卷积层块里的基本单位是**卷积层后接最大池化层**：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。

LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。

```python
net.add(nn.Conv2D(channels=6, kernel_size=5, activation='relu'),
        # 池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。
        nn.MaxPool2D(pool_size = 2, strides = 2),
        # 增加输出通道使两个卷积层的参数尺寸类似。
        nn.Conv2D(channels=10, kernel_size=5, activation='relu'),
        nn.MaxPool2D(pool_size = 2, strides = 2),
        # Dense会默认将（批量，通道，宽，高）的输入转换成（批量，通道*宽*高）
        # 全连接层块会将小批量中每个样本变平（flatten）。
        nn.Dense(120, activation='sigmoid'),
        nn.Dense(84, activation='sigmoid'),
        nn.Dense(10))
```

**练习**

- 尝试基于LeNet构造更复杂的网络来提高分类准确率。例如，调整卷积窗口大小、输出通道数、激活函数和全连接层输出个数。在优化方面，可以尝试使用不同的学习率、初始化方法以及增加迭代周期。

  答：1.换成relu激活函数显著提高。2.第二个卷积层的通道可以适当减少。神经网络定义如上，其他超参数如下。

  ```python
  lr, num_epochs = 0.95, 7
  ——————————————————————————结果————————————————————————————————————
  training on gpu(0)
  epoch 1, loss 1.0864, train acc 0.577, test acc 0.778, time 4.3 sec
  epoch 2, loss 0.5540, train acc 0.782, test acc 0.826, time 4.3 sec
  epoch 3, loss 0.4597, train acc 0.828, test acc 0.850, time 4.3 sec
  epoch 4, loss 0.4063, train acc 0.849, test acc 0.862, time 4.3 sec
  epoch 5, loss 0.3754, train acc 0.860, test acc 0.872, time 4.3 sec
  epoch 6, loss 0.3539, train acc 0.868, test acc 0.877, time 4.3 sec
  epoch 7, loss 0.3320, train acc 0.876, test acc 0.881, time 4.3 se
  ```

#### 5.6深度卷积网络

LeNet优缺点：小数据集成绩不错，但在大数据集上表现不尽人意。

AlexNet：

```python
# 使用较大的11 x 11窗口来捕获物体。同时使用步幅4来较大幅度减小输出高和宽。这里使用的输出通
# 道数比LeNet中的也要大很多
net.add(nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
        nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。
        # 前两个卷积层后不使用池化层来减小输入的高和宽
        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
        nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合
        nn.Dense(4096, activation="relu"), nn.Dropout(0.5),
        nn.Dense(4096, activation="relu"), nn.Dropout(0.5),
        # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
        nn.Dense(10))
```

**练习**

- 尝试增加迭代周期。跟LeNet的结果相比，AlexNet的结果有什么区别？为什么？

  答：AlexNet的训练时间长了三倍，训练结果精确度提高了10%左右。AlexNet精确率提高的慢一点。应该是用了dropout以及lr比较低的原因。同时神经网络复杂度大大提高。

- AlexNet对Fashion-MNIST数据集来说可能过于复杂。试着简化模型来使训练更快，同时保证准确率不明显下降。

  答：看5.5章我的笔记中的改进后的LeNet。个人以为，改为relu激活就能显著提高精确率了，对于Fashion-MNIST而言。

- 修改批量大小，观察准确率和内存或显存的变化。

  答：批量改成256，准确率降低。显存没有看出显著变化。可能观测方法有问题，只是通过任务管理器的性能来查看。

#### 5.7VGG使用重复元素的网络

VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为3×3的卷积层后接上一个步幅为2、窗口形状为2×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。

**练习**

- 与AlexNet相比，VGG通常计算慢很多，也需要更多的内存或显存。试分析原因。

  答：通道更多，刚开始时候的宽高更大。

  ```
  ————————————————AlexNet的每一层输出形状————————————————
  conv0 output shape:	 (1, 96, 54, 54)
  pool0 output shape:	 (1, 96, 26, 26)
  conv1 output shape:	 (1, 256, 26, 26)
  pool1 output shape:	 (1, 256, 12, 12)
  conv2 output shape:	 (1, 384, 12, 12)
  conv3 output shape:	 (1, 384, 12, 12)
  conv4 output shape:	 (1, 256, 12, 12)
  pool2 output shape:	 (1, 256, 5, 5)
  dense0 output shape:	 (1, 4096)
  dropout0 output shape:	 (1, 4096)
  dense1 output shape:	 (1, 4096)
  dropout1 output shape:	 (1, 4096)
  dense2 output shape:	 (1, 10)
  ————————————————VGG的每一层输出形状————————————————
  sequential1 output shape:	 (1, 64, 112, 112)
  sequential2 output shape:	 (1, 128, 56, 56)
  sequential3 output shape:	 (1, 256, 28, 28)
  sequential4 output shape:	 (1, 512, 14, 14)
  sequential5 output shape:	 (1, 512, 7, 7)
  dense0 output shape:	 (1, 4096)
  dropout0 output shape:	 (1, 4096)
  dense1 output shape:	 (1, 4096)
  dropout1 output shape:	 (1, 4096)
  dense2 output shape:	 (1, 10)
  ```

- 尝试将Fashion-MNIST中图像的高和宽由224改为96。这在实验中有哪些影响？

  答：时间缩短到了1/4，精确率略低于224的。

- 参考VGG论文里的表1来构造VGG其他常用模型，如VGG-16和VGG-19 [1]。

  ![image-20200628002402855](charter5\image-20200628002402855.png)

![test](https://raw.githubusercontent.com/FZU-AI/AI_note/master/xz/deep_learning/charter5/image-20200628002402855.png)