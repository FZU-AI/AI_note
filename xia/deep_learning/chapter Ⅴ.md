# 卷积神经⽹络
## 二维卷积层
**输入数组**和**核数组**通过**互相关运算**得到输出数组  
二维互相关运算：  
//图片  
⼆维卷积层将输⼊和卷积核做互相关运算，并加上⼀个标量偏差来得到输出。    
卷积层可通过重复使⽤卷积核有效地表征局部空间。    
卷积运算：为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输⼊数组做互相关运算。可⻅，卷积运算和互相关运算虽然类似，但如果它们使⽤相同的核数组，对于同⼀个输⼊，输出往往并不相同。    
特征图：⼆维卷积层输出的⼆维数组可以看作是输⼊在空间维度（宽和⾼）上某⼀级的表征    
x的感受野：影响元素x的前向计算的所有可能输⼊区域（可能⼤于输⼊的实际尺⼨）  
• ⼆维卷积层的核⼼计算是⼆维互相关运算。在最简单的形式下，它对⼆维输⼊数据和卷积核做互相关运算然后加上偏差。  
• 我们可以设计卷积核来检测图像中的边缘。  
• 我们可以通过数据来学习卷积核。  
## 填充和步幅
假设输⼊形状是nh × nw，卷积核窗口形状是kh × kw，那么输出形状将会是  
(nh − kh + 1) × (nw − kw + 1)  
**填充**（padding）是指在输⼊⾼和宽的两侧填充元素（通常是0元素）  
//配图
如果在⾼的两侧⼀共填充ph⾏，在宽的两侧⼀共填充pw列，那么输出形状将会是  
(nh − kh + ph + 1) × (nw − kw + pw + 1),  
多数情况下，会设置ph = kh−1和pw = kw −1来使输⼊和输出具有相同的⾼和宽。（k为奇数）  
卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。  
将每次滑动的⾏数和列数称为**步幅（stride）**
//插图  
当⾼上步幅为sh，宽上步幅为sw时，输出形状为  
⌊(nh − kh + ph + sh)/sh⌋ × ⌊(nw − kw + pw + sw)/sw⌋.  
## 多输⼊通道和多输出通道
**通道维**：例如图像的高，宽，色彩（红黄蓝），由3*h*w多维数组表示，其中3这一维，成为通道维 
//tu  
如果希望得到**多通道输出**，可以为每个输出通道分别创建形状为ci × kh × kw的核数组。将它们在输出通道维上连结，卷积核的形状即co × ci × kh × kw。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输⼊数组计算而来。    
将卷积窗口形状为1 × 1（kh = kw = 1）的多通道卷积层，通常称之为1 × 1卷积层，并将其中的卷积运算称为1 × 1卷积。  
假设将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那么1 × 1卷积层的作⽤与全连接层等价。  
## 池化层
**池化层**每次对输⼊数据的⼀个固定形状窗口（⼜称池化窗口）中的元素计算输出。不同于卷积层⾥计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最⼤值或者平均值。该运算也分别叫做最⼤池化或平均池化。
同卷积层⼀样，池化层也可以在输⼊的⾼和宽两侧的填充并调整窗口的移动步幅来改变输出形状  
在处理多通道输⼊数据时，池化层对每个输⼊通道分别池化  
## 卷积神经⽹络（LeNet）
LeNet分为卷积层块和全连接层块两个部分。  
卷积层块⾥的基本单位是卷积层后接最⼤池化层：  
卷积层⽤来识别图像⾥的空间模式，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性。  
## 深度卷积神经⽹络（AlexNet）
与相对较小的LeNet相⽐，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层  
AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数  
AlexNet通过丢弃法来控制模型复杂度  
AlexNet引⼊了⼤量的图像增⼴，如翻转、裁剪和颜⾊变化，从而进⼀步扩⼤数据集来缓解过拟合。  
## 使⽤重复元素的⽹络（VGG）
VGG块的组成规律是：连续使⽤数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。  
## ⽹络中的⽹络（NiN）
串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。  
NiN使⽤1×1卷积层来替代全连接层，从而使空间信息能够⾃然传递到后⾯的层中去。  
//插图  
## 含并⾏连结的⽹络（GoogLeNet）
//插图
Inception块⾥有4条并⾏的线路。  
前3条线路使⽤窗口⼤小分别是1 × 1、3 × 3和5 × 5的卷积层来抽取不同空间尺⼨下的信息，其中中间2个线路会对输⼊先做1 × 1卷积来减少输⼊通道数，以降低模型复杂度。第四条线路则使⽤3 × 3最⼤池化层，后接1 × 1卷积层来改变通道数。  
4条线路都使⽤了合适的填充来使输⼊与输出的⾼和宽⼀致。最后将每条线路的输出在通道维上连结，并输⼊接下来的层中去。  
## 批量归⼀化
批量归⼀化利⽤小批量上的均值和标准差，不断调整神经⽹络中间输出，从而使整个神经⽹络在各层的中间输出的数值更稳定  
**对全连接层做批量归⼀化**：将批量归⼀化层置于全连接层中的仿射变换和激活函数之间  
**对卷积层做批量归⼀化**：对卷积层来说，批量归⼀化发⽣在卷积计算之后、应⽤激活函数之前。如果卷积计算输出多个通道，需要对这些通道的输出分别做批量归⼀化，且每个通道都拥有独⽴的拉伸和偏移参数，并均为标量   
## 残差⽹络（ResNet）
//插图
## 稠密连接⽹络（DenseNet）
//插图

