# 优化算法
## 优化与深度学习
**优化算法**的⽬标函数通常是⼀个基于训练数据集的损失函数，优化的⽬标在于降低训练误差  
**深度学习**的⽬标在于降低泛化误差。为了降低泛化误差，除了使⽤优化算法降低训练误差以外，还需要注意应对过拟合  
深度学习模型的⽬标函数可能有若⼲局部最优值。  
当⼀个优化问题的数值解在局部最优解附近时，由于⽬标函数有关解的梯度接近或变成零，最终迭代求得的数值解可能只令⽬标函数局部最小化而⾮全局最小化。   
• 当函数的海森矩阵在梯度为零的位置上的特征值全为正时，该函数得到局部最小值。  
• 当函数的海森矩阵在梯度为零的位置上的特征值全为负时，该函数得到局部最⼤值。  
• 当函数的海森矩阵在梯度为零的位置上的特征值有正有负时，该函数得到鞍点。  
## 梯度下降和随机梯度下降
梯度下降算法中的正数η通常叫作学习率。这是⼀个超参数，需要⼈⼯设定  
随机梯度下降中⾃变量的迭代轨迹相对于梯度下降中的来说更为曲折。这是由于实验所添加的噪声使模拟的随机梯度的准确度下降。  
## ⼩批量随机梯度下降
小批量随机梯度下降中每次迭代的计算开销为O(|B|)。当批量⼤小为1时，该算法即为随机梯度下降；当批量⼤小等于训练数据样本数时，该算法即为梯度下降  
## 动量法
在时间步0，动量法创建速度变量v0，并将其元素初始化成0。  
在时间步t > 0，动量法对每次迭代的步骤做如下修改：   
vt ← γvt−1 + ηtgt，  
xt ← xt−1 − vt,  
其中，动量超参数γ满⾜0 ≤ γ < 1。当γ = 0时，动量法等价于小批量随机梯度下降。  
动量法使⽤了指数加权移动平均的思想。它将过去时间步的梯度做了加权平均，且权重按时间步指数衰减。  
## AdaGrad算法
AdaGrad算法，根据⾃变量在每个维度的梯度值的⼤小来调整各个维度上的学习率，从而避免统⼀的学习率难以适应所有维度的问题。  
AdaGrad算法会使⽤⼀个小批量随机梯度Gt按元素平⽅的累加变量St。在时间步0，AdaGrad将S0中每个元素初始化为0。在时间步t，⾸先将小批量随机梯度Gt按元素平⽅后累加到变量St：  
St ← St−1 + Gt ⊙ Gt,  
其中⊙是按元素相乘。接着，将⽬标函数⾃变量中每个元素的学习率通过按元素运算重新调整⼀下：
Xt ← Xt−1 −η/√(St + ϵ)⊙ Gt,  
其中η是学习率，ϵ是为了维持数值稳定性而添加的常数，如10−6。这⾥开⽅、除法和乘法的运算都是按元素运算的。这些按元素运算使得⽬标函数⾃变量中每个元素都分别拥有⾃⼰的学习率。   
由于st⼀直在累加按元素平⽅的梯度，⾃变量中每个元素的学习率在迭代过程中⼀直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有⽤的解。  
## RMSProp算法
对AdaGrad算法的改进  
RMSProp算法将这些梯度按元素平⽅做指数加权移动平均。具体来说，给定超参数0 ≤ γ < 1，RMSProp算法在时间步t > 0计算  
St ← γSt−1 + (1 − γ)Gt ⊙ Gt.  
如此⼀来，⾃变量每个元素的学习率在迭代过程中就不再⼀直降低（或不变）。  
## AdaDelta算法
AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有⽤解的问题做了改进    
与RMSProp算法不同的是，AdaDelta算法还维护⼀个额外的状态变量∆xt，其元素同样在时间步0时被初始化为0。使⽤∆xt−1来计算⾃变量的变化量：    
//插图   
如不考虑ϵ的影响，AdaDelta算法与RMSProp算法的不同之处在于使⽤√∆Xt−1来替代超参数η。  
## Adam算法
Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均   
Adam算法使⽤了动量变量vt和RMSProp算法中小批量随机梯度按元素平⽅的指数加权移动平均变量St，并在时间步0将它们中每个元素初始化为0。给定超参数0 ≤ β1 < 1（算法作者建议设为0.9），时间步t的动量变量Vt即小批量随机梯度Gt的指数加权移动平均：  
Vt ← β1Vt−1 + (1 − β1)Gt.  
和RMSProp算法中⼀样，给定超参数0 ≤ β2 < 1（算法作者建议设为0.999），将小批量随机梯度按元素平⽅后的项Gt ⊙ Gt做指数加权移动平均得到St：  
St ← β2St−1 + (1 − β2)Gt ⊙ Gt.   

