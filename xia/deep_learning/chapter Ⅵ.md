# 循环神经⽹络
循环神经⽹络是为更好地处理**时序信息**而设计的。它引⼊状态变量来存储过去的信息，并⽤其与当前的输⼊共同决定当前的输出。  
## 语⾔模型
把⼀段⾃然语⾔⽂本看作⼀段离散的时间序列。  
假设⼀段⻓度为T的⽂本中的词依次为w1, w2, . . . , wT，那么在离散的时间序列中，wt（1 ≤ t ≤ T）可看作在时间步（time step）t的输出或标签。  
为了计算语⾔模型，需要计算词的概率，以及⼀个词在给定前⼏个词的情况下的条件概率，即语⾔模型参数。   
**n元语法**通过**⻢尔可夫假设**（虽然并不⼀定成⽴）简化了语⾔模型的计算。  
基于n − 1阶⻢尔可夫链，我们可以将语⾔模型改写为  
P(w1, w2, . . . , wT ) ≈∏P(wt | wt−(n−1), . . . , wt−1).  
## 循环神经⽹络
通过隐藏状态来存储之前时间步的信息  
考虑输⼊数据存在时间相关性的情况  
//插图
## 语⾔模型数据集（周杰伦专辑歌词）
首先读取数据集  
其次建立字符索引  
最后进行时序数据的采样，分为随机采样和相邻采样  
## 循环神经⽹络的从零开始实现
one-hot向量，向量⻓度等于词典⼤小。  
模型参数中的隐藏单元个数 num_hiddens是⼀个超参数。  
激活函数使⽤了tanh函数。当元素在实数域上均匀分布时，tanh函数值的均值为0。  
基于前缀prefix（含有数个字符的字符串）来预测接下来的num_chars个字符  
为了应对梯度爆炸，可以裁剪梯度（clip gradient）  
通常使⽤困惑度（perplexity）来评价语⾔模型的好坏，困惑度是对交叉熵损失函数做指数运算后得到的值：
• 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；  
• 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正⽆穷；  
• 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。  
跟之前章节的模型训练函数相⽐，这⾥的模型训练函数有以下⼏点不同：  
1. 使⽤困惑度评价模型。  
2. 在迭代模型参数前裁剪梯度。  
3. 对时序数据采⽤不同采样⽅法将导致隐藏状态初始化的不同。  
## 循环神经⽹络的简洁实现




