# 循环神经⽹络
循环神经⽹络是为更好地处理**时序信息**而设计的。它引⼊状态变量来存储过去的信息，并⽤其与当前的输⼊共同决定当前的输出。  
## 语⾔模型
把⼀段⾃然语⾔⽂本看作⼀段离散的时间序列。  
假设⼀段⻓度为T的⽂本中的词依次为w1, w2, . . . , wT，那么在离散的时间序列中，wt（1 ≤ t ≤ T）可看作在时间步（time step）t的输出或标签。  
为了计算语⾔模型，需要计算词的概率，以及⼀个词在给定前⼏个词的情况下的条件概率，即语⾔模型参数。   
**n元语法**通过**⻢尔可夫假设**（虽然并不⼀定成⽴）简化了语⾔模型的计算。  
基于n − 1阶⻢尔可夫链，我们可以将语⾔模型改写为  
P(w1, w2, . . . , wT ) ≈∏P(wt | wt−(n−1), . . . , wt−1).  
## 循环神经⽹络
通过隐藏状态来存储之前时间步的信息  
考虑输⼊数据存在时间相关性的情况  
//插图
## 语⾔模型数据集（周杰伦专辑歌词）
首先读取数据集  
其次建立字符索引  
最后进行时序数据的采样，分为随机采样和相邻采样  
## 循环神经⽹络的从零开始实现
one-hot向量，向量⻓度等于词典⼤小。  
模型参数中的隐藏单元个数 num_hiddens是⼀个超参数。  
激活函数使⽤了tanh函数。当元素在实数域上均匀分布时，tanh函数值的均值为0。  
基于前缀prefix（含有数个字符的字符串）来预测接下来的num_chars个字符  
为了应对梯度爆炸，可以裁剪梯度（clip gradient）  
通常使⽤困惑度（perplexity）来评价语⾔模型的好坏，困惑度是对交叉熵损失函数做指数运算后得到的值：
• 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；  
• 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正⽆穷；  
• 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。  
跟之前章节的模型训练函数相⽐，这⾥的模型训练函数有以下⼏点不同：  
1. 使⽤困惑度评价模型。  
2. 在迭代模型参数前裁剪梯度。  
3. 对时序数据采⽤不同采样⽅法将导致隐藏状态初始化的不同。  
## 循环神经⽹络的简洁实现
使⽤Gluon来更简洁地实现基于循环神经⽹络的语⾔模型    
首先读取数据集    
其次，使用Gluon的rnn模块实现循环神经⽹络  
## 通过时间反向传播
将循环神经⽹络按时间步展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应⽤反向传播计算并存储梯度。  
//插图  
## 7 ⻔控循环单元（GRU）
**⻔控循环神经⽹络**（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较⼤的依赖关系。它通过可以学习的⻔来控制信息的流动。  
//插图    
• 重置⻔有助于捕捉时间序列⾥短期的依赖关系；  
• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。  
 ## ⻓短期记忆（LSTM）
LSTM 中引⼊了3个⻔，即输⼊⻔（input gate）、遗忘⻔（forget gate）和输出⻔（output gate），以及与隐藏状态形状相同的记忆细胞（某些⽂献把记忆细胞当成⼀种特殊的隐藏状态），从而记录额外的信息。  
//插图  
可以通过元素值域在[0, 1]的输⼊⻔、遗忘⻔和输出⻔来控制隐藏状态中信息的流动，这⼀般也是通过使⽤按元素乘法（符号为⊙）来实现的。  
## 深度循环神经⽹络
含有多个隐藏层的循环神经⽹络，也称作**深度循环神经⽹络**  
//插图  
## 双向循环神经⽹络
有时候，当前时间步也可能由后⾯时间步决定。双向循环神经⽹络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息  
//插图



