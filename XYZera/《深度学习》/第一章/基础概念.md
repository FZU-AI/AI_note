**1、*****\*机器学习\****：是人工智能的核心，是使计算机具有智能的根本途径。使用计算机作为工具并致力于真实、实时的模拟人类学习方式， 并将现有内容进行知识结构划分来有效提高学习效率。



**2、*****\*深度学习\****：是机器学习领域中一个新的研究方向。深度学习的概念源于人工神经网络的研究，含多个隐藏层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。

 

**3、*****\*强化学习\****：又称再励学习或评价学习，常见模型是标准的马尔可夫决策过程，属于无监督学习。用于描述和解决智能体在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题。

 

**4、*****\*监督学习与无监督学习\****：主要看给定的数据集是否带有数据标签。监督学习是传统机器学习方式，无监督学习常见于聚类和强化学习。



**5、*****\*机器学习算法\****：线性回归、逻辑回归、K-近邻算法、决策树算法、朴素贝叶斯、支持向量机、K-Means算法、PCA算法、协同过滤算法、BP神经网络、卷积神经网络等等。

 

**6、*****\*线性回归\****：是指确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，通过建立一个回归方程（函数）来估计特征值对应的目标变量的可能取值。最常见的是线性回归方程：y = a x + b。

 

**7、*****\*线性回归的求解过程\****：设方程**f**(**x**)=**θ**0**x**0+**θ**1**x**1+**θ**2**x**2...+**θ****n****x****n**                 线性回归的求解过程就是求解回归系数（**θ**0、、**θ**1**θ**2      ...**θ****n**）的过程，并且使误差最小。其中使用损失函数来表示模型误差，采用梯度下降算法来寻找损失函数的最小值，也就是误差最小值。

 

**8、*****\*过拟合问题\****：是指为了得到一致假设而使假设变得过度严格，从而丧失一般性的现象。当回归方程中有过多变量，同时只有非常少的训练集时，就会出现过度拟合的问题，所以可以通过控制关键变量数目来达到避免过拟合的效果。

 

**9、*****\*正则化解决过拟合问题\****：对模型添加正则化可以限制模型的复杂度，使得模型在复杂度和性能间达到平衡。L1和L2正则化可以看作损失函数的惩罚项，所谓的惩罚就是损失函数中的某些参与加以限制。L1正则化：![img](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml66272\wps1.png)，L2正则化：![img](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml66272\wps2.png)



**10、*****\*逻辑回归\****：虽然名字叫回归，但却属于分类算法，是通过Sigmoid函数将线性函数的结果映射到Sigmoid函数中，预估事件出现的概率并分类。Sigmoid是归一化的函数，可以把连续数值收敛至[0,1]的范围，提供了一种将连续型的数据离散化为离散型数据的方法。

 

**11、*****\*线性回归和逻辑回归的比较\****：虽然逻辑回归能够用于分类，不过其本质还是线性回归。它仅在线性回归的基础上，在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和，然后使用sigmoid函数来预测。

 

**12、*****\*K-近邻算法\****：用距离度量各相邻的分类标签，然后取前K个最近的分类标签，求得这K个分类标签中出现频率最大的类别，可近似将该类别作为待预测目标的类别。

 

**13、*****\*决策树\****：通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。决策树可细分为分类决策树和回归决策树，分别用于做分类任务和回归预测任务。

 

**14、*****\*信息熵\****：度量信源整体的不确定的。熵越小，样本的纯度越高，所以决策树的生长过程也是不断的将数据的不纯度降低的过程。希望最后得到的分类结果纯的很高，也就是准确性很高。



**15、*****\*分类决策树算法\****：ID3使用信息增益作为选择特征的准则，信息增益 = 划分前熵 - 划分后熵；C4.5 使用信息增益比作为选择特征的准则，信息增益率 = 信息增益 / 划分前熵。之所以引入了信息增益比，是由于信息增益的一个缺点。信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题。

 

**16、*****\*CART算法\****：全称为分类和回归树，既可以做分类任务，也可以做回归任务，但生成的树必须是二叉树。CART 使用 Gini 指数作为选择特征的准则。Gini 指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。

 

**17、*****\*回归树的构造\****：回归树构造的整体流程与分类树类似，不过回归树在每个结点（不一定是叶子结点）都会得到一个预测值，该预测值等于属于该节点的所有样本属性的平均值。

 

**18、*****\*朴素贝叶斯\****：利用贝叶斯公式，计算可选项的后验概率，选择后验概率最大的类为分类标签。主要应用有文本分类、垃圾文本过滤，情感判别，多分类实时预测等。

 

**19、*****\*贝叶斯网络\****：是一个有向无环图模型，每一个节点代表一个变量，节点间的联系组成了贝叶斯网络语义，此外还包括条件概率表（CPT）。它是一种模拟人类推理过程中因果关系的不确定性处理模型。



**20、*****\*马尔可夫模型\****：将贝叶斯网络退化成线性链的方式，则得到马尔可夫模型。到目前为止，它一直被认为是实现快速精确的语音识别系统的最成功的方法。因为每个结点都是随机变量，将其看成各个时刻的相关变化，以随机过程的视角，则可以看成是马尔可夫过程 。

 

**21、*****\*隐马尔可夫模型\****：是一种统计模型，用来描述一个含有隐含未知参数的马尔可夫过程。 它是结构最简单的动态贝叶斯网，这是一种著名的有向图模型 ，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用。

 

**22、*****\*支持向量机\****：属于监督式的二分类算法。当一个分类问题，数据是线性可分时，只要将线的位置放在让小球距离线的距离最大化的位置即可，寻找这个最大间隔的过程，就叫做最优化。一般的数据是线性不可分的，可以利用核函数，通过核方法，将数据从二维映射到高维，通过超平面将数据切分，即把非线性分类转化为线性分类的过程。

 

**23、*****\*K-Means算法\****：属于非监督式的聚类算法。对于没有标记的数据集，能够自动的将相同元素分为紧密关系的子集或簇，这就是聚类算法。算法步骤：随机生成k个初始点作为质心；将数据集中的数据按照距离质心的远近分到各个簇中；将各个簇中的数据求平均值，作为新的质心（可以是不存在的点），重复上一步，直到所有的簇不再改变。关于K的选择，经典的方法有“肘点法”，即损失函数拐点处。



**24、*****\*PCA降维\****：降维是指将原高维空间中的数据点映射到低维度的空间中。因为高维特征的数目巨大，距离计算困难，分类器的性能会随着特征数的增加而下降；减少高维的冗余信息所造成的误差,可以提高识别的精度。而主成分分析算法（PCA）能从冗余特征中提取主要成分，在不太损失模型质量的情况下，提升了模型训练速度。

 

**25、*****\*协同过滤算法\****：是一种基于近邻的推荐算法，根据用户在物品上的行为找到物品或用户的“近邻”，即用户在物品上的行为-->同类用户/同一用户可能喜欢的不同物品。故协同过滤可分为：基于用户的协同过滤和基于物品的协同过滤，但基于物品的协同过滤用的更多。
