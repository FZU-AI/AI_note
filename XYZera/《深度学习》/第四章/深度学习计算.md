# 深度学习计算

## 前言

> 简要概括深度学习计算的各个重要组成部分，如模型构造、参数的访问和初始化等，⾃定义层，读取、存储和使⽤GPU。为后面章节打下基础。



## 模型构造

##### Block类是nn模块里提供的一个模型构造类，可以继承它来定义我们想要的模型。Sequential类继承⾃Block类。

```python
#这里定义的MLP类重载了Block类的__init__函 数和forward函数。它们分别用于创建模型参数和定义前向计算
from mxnet import nd 
from mxnet.gluon import nn 
class MLP(nn.Block): 
    # 声明带有模型参数的层，这里声明了两个全连接层 
    def __init__(self, **kwargs): 
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数 
    # 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数
    super(MLP, self).__init__(**kwargs) 
    self.hidden = nn.Dense(256, activation='relu')# 隐藏层 
    self.output = nn.Dense(10) # 输出层 
    
    # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出 
    def forward(self, x): 
        return self.output(self.hidden(x)) 
#实现一个与Sequential类有相同功能的MySequential类
class MySequential(nn.Block): 
    def __init__(self, **kwargs): 
        super(MySequential, self).__init__(**kwargs)
        
    def add(self, block):
    # block是一个Block子类实例，假设它有一个独一无二的名字。我们将它保存在Block类的 
    # 成员变量_children里，其类型是OrderedDict。当MySequential实例调用 
    # initialize函数时，系统会自动对_children里所有成员初始化 
    self._children[block.name] = block def forward(self, x): 
    # OrderedDict保证会按照成员添加时的顺序遍历成员 
    for block in self._children.values(): 
        x = block(x) 
    return x 
    
# 使用起来与Sequential差不多    
net = MySequential() 
net.add(nn.Dense(256, activation='relu')) 
net.add(nn.Dense(10)) 
net.initialize() 
net(X) 
```



## 模型参数的访问、初始化和共享

##### 对于使⽤Sequential类构造的神经⽹络，我们可以通过⽅括号[]来访问⽹络的任⼀层。在有些情况下，我们希望在多个层之间共享模型参数。

```python
#让模型的第二隐藏层（shared变量）和第三隐藏层共享模型参数
net = nn.Sequential() 
shared = nn.Dense(8, activation='relu') 
net.add(nn.Dense(8, activation='relu'), shared, 
nn.Dense(8, activation='relu', params=shared.params), 
nn.Dense(10)) 
net.initialize()
#在构造第三隐藏层时通过params来指定它使用第二隐藏层的参数。因为模型参数里包含了 梯度， 所以在反向传播计算时， 第二隐藏层和第三隐藏层的梯度都会被累加在shared.params.grad()里
```



## 读取与存储

##### 在实际中，我们有时需要把训练好的模型部署到很多不同的设备。在这种情况下，我们可以把内存中训练好的模型参数存储在硬盘上供后续读取使⽤。

```python
from mxnet import nd
from mxnet.gluon import nn

x = nd.ones(3)
nd.save('x', x)

x2 = nd.load('x')
y = nd.zeros(4)
nd.save('xy', [x, y])
x2, y2 = nd.load('xy')
```



## 读写Gluon模型的参数

##### Gluon的Block类提供了save_parameters函数和load_parameters函数来读写模型参数。

```python
class MLP(nn.Block): 
    def __init__(self, **kwargs): 
        super(MLP, self).__init__(**kwargs)
        self.hidden = nn.Dense(256, activation='relu') 
        self.output = nn.Dense(10) 
    def forward(self, x): 
        return self.output(self.hidden(x))
        
net = MLP() 
net.initialize() 
X = nd.random.uniform(shape=(2, 20)) 
Y = net(X) 
#把该模型的参数存成文件，文件名为mlp.params
filename = 'mlp.params' net.save_parameters(filename) 
#再实例化一次定义好的多层感知机。与随机初始化模型参数不同，我们在这里直接 读取保存在文件里的参数
net2 = MLP() 
net2.load_parameters(filename) 
#因为这两个实例都有同样的模型参数，那么对同一个输入X的计算结果将会是一样的
Y2 = net2(X) 
Y2 == Y 
```



## GPU计算

##### 对复杂的神经⽹络和⼤规模的数据来说，使⽤CPU来计算可能不够⾼效。MXNet可以指定⽤来存储和计算的设备，如使⽤内存的CPU或者使⽤显存的GPU。

```python
import mxnet as mx
from mxnet import nd
from mxnet.gluon import nn

mx.cpu(), mx.gpu(), mx.gpu(1)
a = nd.array([1, 2, 3], ctx=mx.gpu())
b = nd.random.uniform(shape=(2, 3), ctx=mx.gpu(1)) #当有多块gpu的时候可以指定gpu

net = nn.Sequential()
net.add(nn.Dense(1))
net.initialize(ctx=mx.gpu())

```

