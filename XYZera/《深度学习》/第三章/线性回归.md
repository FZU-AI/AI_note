# 线性回归



## 概念引入

> 在我们开始讨论技术原理之前，先看一些帮助我们直观理解的例子：
>
> 给你以下的X，Y值，(1,1), (2,2), (4,4), (100,100), (20, 20)，当X=5时，Y是多少？
>
> 答案是5，很简答，对吧？
>
> 现在，让我们看一个难一点的例子。给你以下的X，Y值，(1,1), (2,4), (4,16), (100,10000), (20, 400)，当X=5时，Y是多少？
>
> 答案是25.
>
> 然我们来理解一下上面的两个例子。当我们做第一个例子时，首先通过给定的数据，建立了X和Y之间的关系，那就是 ![[公式]](https://www.zhihu.com/equation?tex=Y+%3D+X) 。同样的在第二个例子中，关系是 ![[公式]](https://www.zhihu.com/equation?tex=Y+%3D+X%5E2)
>
> 在这两个例子中，我们可以很容易的通过给定的数据确定X和Y之间的关系。实际上，线性回归也采用同样的工作原理。
>
> 计算机会通过“理解”给定的数据，然后试着确定X和Y之间的“最合适”的关系。使用建立好的关系，就可以预测通过给定的X值预测未知的Y值。



## 重要指标

- ### 代价函数

  代价函数的作用是用来测量我们的模型的准确程度。

  以最简单的一个代价函数为例，也就是**均方误差（Mean squared error）**。

  假设我们的模型如下： ![[公式]](https://www.zhihu.com/equation?tex=h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29+%3D+%5Ctheta_0+%2B+%5Ctheta_1x%5E%7B%28i%29%7D)

  均方误差就是求预测值与真实值之间的差值的平方，即：

​                                       ![[公式]](https://www.zhihu.com/equation?tex=%28h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29+-+y%5E%7Bi%7D%29%5E2)

   如果对于训练集中的所有样本求均方误差，就是将每个样本的均方误差求和再求平均，即：

​                                      ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7D%5Csum%5E%7Bm%7D%7Bi%3D1%7D%28h%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29+-+y%5E%7Bi%7D%29%5E2)

   因此，如果我们使用大写的J来表示代价函数，那么它的定义如下(注意J是关于 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的函数)：

​                                     ![[公式]](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Cfrac%7B1%7D%7B2m%7D%5Csum%5E%7Bm%7D%7Bi%3D1%7D%28h%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29+-+y%5E%7Bi%7D%29%5E2)   

   由于代价函数越大，代表预测值与真实值之间的误差就越大，因此，问题的答案是使代价函数最小的参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)就是  

   最好的参数。



- ### 梯度下降

  既然代价函数是关于![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)的函数，我们就可以可视化该函数，见下图：

![img](https://pic4.zhimg.com/80/v2-71afaf9c18630b844f47ae9aa2926513_1440w.jpg)



   图中的蓝色区域是代价函数最小的点，因此，找到该点对应的![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)，即完成了任务。

   如何找到最低点对应的![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)？

   答案是对代价函数求偏导数：

![[公式]](https://www.zhihu.com/equation?tex=+%5Cbegin%7Bsplit%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta_j%7DJ%28%5Ctheta%29+%26%3D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta_j%7D+%5Cfrac%7B1%7D%7B2%7D%28h_%7B%5Ctheta%7D%28x%29+-+y%29%5E2+%5C++%26%3D+%5Cfrac%7B1%7D%7B2%7D%5Ctimes2%5Ctimes%28h_%7B%5Ctheta%7D%28x%29+-+y%29%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta_j%7D%28%5Csum%5En_%7Bi%3D0%7D%5Ctheta_ix_i+-+y%29+%5C++%26%3D+%28h_%7B%5Ctheta%7D%28x%29+-+y%29x_j+%5Cend%7Bsplit%7D+)

   上面的推导中1/2和2互消了，这就是为什么代价函数公式要乘以一个1/2.

   需要注意的是，上面的公式中，我们采用了一个更加通用的(考虑了多个自变量的情况)表示模型h的方式：

![[公式]](https://www.zhihu.com/equation?tex=h%28%5Ctheta%29+%3D+%5Csum%5En_%7Bi%3D0%7D%5Ctheta_ix_i+%3D+%5Ctheta_01+%2B+%5Ctheta_1x_1+%2B+%5Cdots+%2B+%5Ctheta_nx_n)

   对于我们的简单线性回归函数来说：

   注意，这里 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_0) 乘以一个常数1，也可以认为乘以了常量$x_0 = 1$

   所以代价函数对 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_0) 求偏导的结果就是：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta_0%7DJ%28%5Ctheta%29+%3D+%28h_%7B%5Ctheta%7D%28x%29+-+y%29x_0+%3D+%28h_%7B%5Ctheta%7D%28x%29+-+y%29)

   代价函数对 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1) 求偏导的结果就是：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta_1%7DJ%28%5Ctheta%29+%3D+%28h_%7B%5Ctheta%7D%28x%29+-+y%29x_1)

   求得的结果怎么使用？

   注意，对 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 求偏导数的意义是得到**这一点上的切线的斜率**，它将给我们一个向最小值移动的方向。

   因此，![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)减去偏导数就等于![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)向最小值的方向移动了一步。这一步的大小由一个参数$\alpha$决定，也称作学习   率。用公式表达如下：

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_j+%3A%3D+%5Ctheta_j+-+%5Calpha%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta_j%7DJ%28%5Ctheta%29)

   注意，上式中的符号 ![[公式]](https://www.zhihu.com/equation?tex=%3A%3D) 表示重复该过程，直到收敛。也就是我们的**梯度下降公式**。

- ### 学习率

  现在我们讨论一下学习率，当学习率比较小时，我们得到最优解的速度将很慢。



![img](https://pic1.zhimg.com/80/v2-3df13c7f294493e9a985b37b424edb50_1440w.jpg)



   当学习率比较大时，我们很容易无法得到全局最优解。



![img](https://pic1.zhimg.com/80/v2-7c6f5d5a8e92d570718da6dfc4a9da78_1440w.jpg)



   另外一种情况是，我们的学习率设置的比较小时，得到了局部最优解，而不是全局最优解的情况。



![img](https://pic4.zhimg.com/80/v2-3870e0b7c306899acfc61045c61f03ab_1440w.jpg)



   最后一种情况，对于我们使用的代价函数来说并不成立，因为我们使用的均方误差MSE是一个凸函数，也就是说 

   说，该函数任意两点的连线都不会与该函数交叉，所以，该函数不存在局部最优解。

  











