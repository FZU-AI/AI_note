# 第三章：深度学习基础

深度学习是机器学习的一类，通常使用神经网络实现

单层神经网络：线性回归和softmax回归

回归问题预测连续值，分类问题预测离散值

## 3.1线性回归

线性回归用于回归问题

### 3.1.1 线性回归的基本要素

1. 模型（model）代表输入和输出的关系

   模型中输入就是特征，样本的真实输出就是标签，特征的系数是权重，还有一个偏差参数

2. 模型训练

   需要训练数据，损失函数，优化函数三个部分

   1. 训练数据

      所收集的用于训练模型的数据称为训练数据集或训练集

      其中每条数据称为样本

      模型要预测的输出在真实数据中称为标签

      模型中的输入称为特征，用来预测输出

   2. 损失函数

      损失函数用来比较真实值和预测值的误差，是以模型参数为参数的函数（模型输出的预测函数的参数是特征），损失函数输出为一个非负值，数值越小表⽰误差越小

   3. 优化算法

      - 方程解可以直接用公式表示的为解析解

      - 只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值所求解称为数值解

      - 求数值解的优化算法，梯度下降算法十分常用，先选取⼀组模型参数的初始值，可随机选取，然后朝算是函数梯度最大的方向进行迭代，每次迭代计算所有样本的损失函数平均值中关于模型参数的梯度，乘以学习率后用来更新模型参数
      - 小批量随机梯度下降与梯度下降算法相比，就是每次迭代不是计算所有样本，而是随机均匀采样⼀个由固定数⽬训练数据样本所组成的小批量
      - 学习率是用于设置参数每次迭代变化的大小
      - 超参数（hyperparameter）指不是通过模型训练学出的参数，通常所说的“调参”指的正是调节超参数，一般就是通过试验多个值来选取一个合适的值，如小批量随机梯度下降中批量的大小，学习率

3. 模型预测

   模型训练完后，就可以通过输入特征来预测该情况下的输出

模型训练就是通过已知数据来得到模型参数的值，使模型输出与实际输出的值误差近可能小，即求所有样本的平均误差最小时的模型参数

### 3.1.2 线性回归的表⽰⽅法

**神经网络图**

线性回归用神经网络图表示，分为输入层和输出层

线性回归作为一个单层神经网络模型，虽然有输入层和输出层，但是只有输出层涉及计算，所以是单层神经网络模型。

输出层中的神经元和输⼊层中各个输⼊完全连接。因此，这⾥的输出层⼜叫全连接层（fully-connected layer）或稠密层（dense layer）。

**⽮量计算表达式**

矢量计算用于处理多个样本，比用逐个计算更节省时间

## 3.2 线性回归的从零开始实现

只利⽤NDArray和autograd来实现⼀个线性回归的训练



%matplotlib inline
from IPython import display
from matplotlib import pyplot as plt
from mxnet import autograd, nd
import random



num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += nd.random.normal(scale=0.01, shape=labels.shape)



## 3.3 线性回归的简洁实现

使⽤MXNet提供的Gluon接口更⽅便地实现线性回归的训练

### 3.3.1 ⽣成数据集

`from mxnet import autograd, nd`
`num_inputs = 2	#特征个数`
`num_examples = 1000	#样本个数`
`true_w = [2, -3.4]	#权重参数`
`true_b = 4.2	#偏差参数`
`features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))`
`labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b	#计算特征对应的标签`
`labels += nd.random.normal(scale=0.01, shape=labels.shape)	#为标签添加噪声`



### 3.3.2 读取数据

Gluon提供了data包来读取数据。

由于data常⽤作变量名，我们将导⼊的data模块⽤添加了Gluon⾸字⺟的假名gdata代替

`from mxnet.gluon import data as gdata`
`batch_size = 10	#批量大小`

`dataset = gdata.ArrayDataset(features, labels)`

`data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)`

读取并打印第一个小批量数据样本，X为特征，y为特征对应标签

`for X, y in data_iter:`
    `print(X, y)`
    `break`

### 3.3.3 定义模型

Gluon提供了⼤量预定义的层，这使我们只需关注使⽤哪些层来构造模型

导⼊nn模块。“nn”是neural networks（神经⽹络）的缩写，该模块定义了⼤量神经⽹络的层

`from mxnet.gluon import nn`

Sequential实例可以看作是⼀个串联各个层的容器，在构造模型时，我们在该容器中依次添加层

`net = nn.Sequential()`

线性回归的输出层是个全连接层，在Gluon中，全连接层是.个Dense实例。我们定义该层输出个数为1。

`net.add(nn.Dense(1))`

### 3.3.4 初始化模型参数

在Gluon中我们⽆须指定每⼀层输⼊的形状，例如线性回归的输⼊个数。当模型得到数据时，例如后⾯执⾏net(X)时，模型将⾃动推断出每⼀层的输⼊个数

MXNet的init模块，提供了模型参数初始化的各种⽅法

这里init是initializer的缩写形式。我们通过init.Normal(sigma=0.01)指定权重参数每个元素将在初始化时随机采样于均值为0、标准差为0.01的正态分布。偏差参数默认会初始化为零。

`from mxnet import init`
`net.initialize(init.Normal(sigma=0.01))`

### 3.3.5 定义损失函数

在Gluon中，loss模块定义了各种损失函数

`from mxnet.gluon import loss as gloss`
`loss = gloss.L2Loss()	# 平⽅损失⼜称L2范数损失`

### 3.3.6 定义优化算法

Gluon中通过创建Trainer实例来指定优化算法和学习率

`from mxnet import gluon`
`trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})`

### 3.3.7 训练模型

调⽤Trainer实例的step函数来迭代模型参数

`num_epochs = 3`
`for epoch in range(1, num_epochs + 1):`
    `for X, y in data_iter:`
        `with autograd.record():`
            `l = loss(net(X), y)`
        `l.backward()`
        `trainer.step(batch_size)`
    `l = loss(net(features), labels)`
    `print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))`

`dense.weight.data()`获取学习到的权重参数

`dense.bias.data()`获取学习到的偏差参数

## 3.4 softmax回归

softmax回归是一个分类模型，用于分类问题预测离散值

和线性回归不同，softmax回归的输出单元从⼀个变成了多个，且引⼊了softmax运算使输出更适合离散值的预测和训练。

分类问题虽然也可也使用回归模型，但将连续值转为离散值会影响分类质量

softmax回归与线性回归都是单层神经网络，但是softmax回归与线性回归不同输出值个数等于标签⾥的类别数

分类问题需要离散的输出值，所以可以将输出值当作预测为对应类别的置信度，并将值最大的输出所对应的类作为预测输出

softmax运算符将输出值变换成值为正且和为1的概率分布，且不改变预测类别输出，能更好的判断输出值的意义，以及进行对比。其中softmax运算符对前面的输出使用指数运算是为了将输出都转换为正数。

对于分类问题使用平方损失函数过于严格，因为分类正确只要预测分类的概率最大就行，所以可以使用交叉熵（cross entropy），交叉熵只关心对正确类别的预测概率，因为只要其值够大，就可以确保分类结果正确。最小化交叉熵损失函数等价于最⼤化训练数据集所有标签类别的联合预测概率

## 3.5 图像分类数据集（Fashion-MNIST）

图像分类数据集中最常⽤的是⼿写数字识别数据集MNIST 

由于⼤部分模型在MNIST上的分类精度都超过了95%，所以使⽤⼀个图像内容更加复杂的数据集Fashion-MNIST

通过Gluon的data包来下载这个数据集

`mnist_train = gdata.vision.FashionMNIST(train=True)`
`mnist_test = gdata.vision.FashionMNIST(train=False)`

mnist_train获取的是训练集

mnist_test获取的是测试集

在训练数据集上训练模型，并将训练好的模型在测试数据集上评价模型的表现

直接创建Gluon的DataLoader实例来读取一个批量的数据

通过ToTensor实例将图像数据从uint8格式变换成32位浮点数格式，并除以255使得所有像素的数值均在0到1之间，ToTensor实例还将图像通道从最后⼀维移到最前⼀维来⽅便之后介绍的卷积神经⽹络计算

然后通过数据集的transform_first函数，我们将ToTensor的变换应⽤在每个数据样本（图像和标签）的第⼀个元素，即图像之上

## 3.6 softmax回归的从零开始实现

NDArray对象的sum方法

`X = nd.array([[1, 2, 3], [4, 5, 6]])`

`X.sum()	#计算所有元素的总和`

`X.sum(axis=0, keepdims=True)	#计算每一列元素相加总和`

`X.sum(axis=1, keepdims=True)	#计算每一行元素相加总和`

上面的X按列或按行相加后就会由二维矩阵变成一维向量，keepdims=True就是使得结果虽然变成一维向量，但是还是以二维矩阵的方式来访问

softmax函数就是前面的softmax运算符

net函数就是softmax回归模型，即计算预测输出的函数

## 3.8 多层感知机

多层感知机（multilayer perceptron，MLP）是一种多层神经网络模型

多层感知机在单层神经⽹络的基础上引⼊了⼀到多个隐藏层（hidden layer）到输入层和输出层之间

多层感知机中的隐藏层和输出层都是全连接层

如果将隐藏层的输出直接作为输出层的输⼊，那么不论添加再多的隐藏层式子，联立以后依然等价于⼀个单层神经⽹络

问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是⼀个仿射变换

可以引入⾮线性变换，例如对隐藏变量使⽤按元素运算的⾮线性函数进⾏变换，然后再作为下⼀个全连接层的输⼊，该非线性函数被称为激活函数（activation function）

### 激活函数

ReLU（rectified linear unit）函数：只保留正数元素，并将负数元素清零

sigmoid函数：将元素的值变换到0和1之间
$$
y = {1\over1+e^{(-x)}}
$$
tanh（双曲正切）函数
