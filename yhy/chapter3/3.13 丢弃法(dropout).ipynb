{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13.1 方法\n",
    "- 丢弃法的核心就是加入丢弃概率来舍弃丢掉某些隐藏层单元\n",
    "- 新的隐藏单元$h_i^{'}$\n",
    "    $$\n",
    "        h_{i}^{'} = \\frac{\\xi_i}{1-p}h_i\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13.2 从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import d2lzh as d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "\n",
    "# 参数是输入特征和丢弃概率\n",
    "def dropout(X, drop_prob):\n",
    "    # 检查丢弃概率是否合法\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    \n",
    "    keep_prob = 1 - drop_prob\n",
    "    \n",
    "    \n",
    "    # keep_prob = 0 说明全部的隐藏单元都被丢弃了\n",
    "    if keep_prob == 0:\n",
    "        return X.zeros_like()\n",
    "    \n",
    "    # uniform 是 uniform distribution 均匀分布（包括左0,不包括右1）\n",
    "    mask = nd.random.uniform(0, 1, X.shape) < keep_prob\n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
       " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.arange(16).reshape(2,8)\n",
    "dropout(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  0.  0.  0.  8.  0.  0.  0.]\n",
       " [16.  0.  0.  0.  0. 26. 28. 30.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
    "b1 = nd.zeros(num_hiddens1)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
    "b2 = nd.zeros(num_hiddens2)\n",
    "W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两层隐藏单元丢弃的概率\n",
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "def net(X):\n",
    "    # 转一维\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = (nd.dot(X,W1) + b1).relu()\n",
    "    \n",
    "    # 只在训练模型时使用丢弃法\n",
    "    if autograd.is_training():\n",
    "        H1 = dropout(H1, drop_prob1)\n",
    "    \n",
    "    H2 = (nd.dot(H1,W2) + b2).relu()\n",
    "    \n",
    "    # 只在训练模型时使用丢弃法\n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2, drop_prob2)\n",
    "    \n",
    "    return nd.dot(H2,W3) + b3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.4007, train acc 0.855, test acc 0.871\n",
      "epoch 2, loss 0.3878, train acc 0.860, test acc 0.868\n",
      "epoch 3, loss 0.3682, train acc 0.865, test acc 0.867\n",
      "epoch 4, loss 0.3599, train acc 0.868, test acc 0.874\n",
      "epoch 5, loss 0.3493, train acc 0.872, test acc 0.878\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 5 , 0.5 ,256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13.3 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(256, activation='relu'),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(10)\n",
    "       )\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1212, train acc 0.566, test acc 0.742\n",
      "epoch 2, loss 0.5743, train acc 0.787, test acc 0.835\n",
      "epoch 3, loss 0.4859, train acc 0.822, test acc 0.847\n",
      "epoch 4, loss 0.4408, train acc 0.839, test acc 0.859\n",
      "epoch 5, loss 0.4130, train acc 0.849, test acc 0.863\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "- 丢弃法可以应对过拟合\n",
    "- 丢弃法只有在训练模型时，才可以用\n",
    "\n",
    "### 练习\n",
    "- 如果把超参数丢弃概率对调，会出现什么结果？\n",
    "    - 从结果上看，模型在测试集上的表现变得更好了\n",
    "- 增大迭代周期，比较使用丢弃法和不使用丢弃法\n",
    "- 如果将模型该的更加复杂，使用丢弃法应多过拟合是否效果更加明显？\n",
    "- 以本节中的模型为例，比较使用丢弃法与权重衰减的效果。如果同时使用两种方法，效果如何？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1313, train acc 0.570, test acc 0.769\n",
      "epoch 2, loss 0.5564, train acc 0.790, test acc 0.837\n",
      "epoch 3, loss 0.4624, train acc 0.829, test acc 0.847\n",
      "epoch 4, loss 0.4565, train acc 0.834, test acc 0.852\n",
      "epoch 5, loss 0.3935, train acc 0.854, test acc 0.864\n",
      "epoch 6, loss 0.3755, train acc 0.862, test acc 0.868\n",
      "epoch 7, loss 0.3529, train acc 0.870, test acc 0.871\n",
      "epoch 8, loss 0.3380, train acc 0.873, test acc 0.874\n",
      "epoch 9, loss 0.3248, train acc 0.880, test acc 0.877\n",
      "epoch 10, loss 0.3158, train acc 0.882, test acc 0.878\n"
     ]
    }
   ],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
    "b1 = nd.zeros(num_hiddens1)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
    "b2 = nd.zeros(num_hiddens2)\n",
    "W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()\n",
    "    \n",
    "    \n",
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "def net(X):\n",
    "    # 转一维\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = (nd.dot(X,W1) + b1).relu()\n",
    "    \n",
    "    # 只在训练模型时使用丢弃法\n",
    "    #if autograd.is_training():\n",
    "    #    H1 = dropout(H1, drop_prob1)\n",
    "    \n",
    "    H2 = (nd.dot(H1,W2) + b2).relu()\n",
    "    \n",
    "    # 只在训练模型时使用丢弃法\n",
    "    #if autograd.is_training():\n",
    "    #    H2 = dropout(H2, drop_prob2)\n",
    "    \n",
    "    return nd.dot(H2,W3) + b3 \n",
    "\n",
    "num_epochs, lr, batch_size = 10 , 0.5 ,256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
