{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类问题\n",
    "- 如从图像中分辨动物等\n",
    "- softmax回归适用的是预测离散值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax回归模型\n",
    "- 公式o = w × X + b\n",
    "- 同Linear regression都是一层神经网络,输出层为全连接层\n",
    "- 与线性回归模型的区别是输出值是有限制的，如1,2,3,每个输出都只能是1,2,3中的一个\n",
    "- 如何做出离散的预测输出？\n",
    "    - 将输出Oi看作是类别i的置信度，比如O1代表狗的置信度，O2代表猫的置信度，O1 = 0.1,02=5 那我们就预测是狗\n",
    "- softmax运算符:yi = softmax(O1,O2,...,Oi,...On) = e^0i / (e^O1 + e^O2 + ... e^Oi + ... + e^On)\n",
    "- softmax运算，能将所有的输出值都映射到0-1之间，这样不会显得那么突兀，比如一个输出1000，另一个只有0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单样本分类的矢量计算表达式\n",
    "- 假设我们现在要将输入的像素为2×2的图片进行分类，分类的结果有3种\n",
    "- 则x(i) = [x1(i),x2(i),x3(i),x4(i)]，表示输入的第i个样本它的每个特征\n",
    "- 输入O(i) = [O1(i),O2(i),O3(i)]，表示第i个样本的三个类别的输出\n",
    "- y(i) = [y1(i),y2(i),y3(i)] 表示经过softmax运算后的概率\n",
    "- 则我们得到公式：O(i) = x(i) * W + b\n",
    "-              y(i) = softmax(o(i))\n",
    "![线性回归和softmax回归的神经网络模型比较](./image/3.4_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小批量样本分类的矢量计算表达式\n",
    "- 注意小批量(batch-size)能提升计算效率\n",
    "- 假设小批量的大小为n，输入的特征为d，输出的个数为q\n",
    "- 则X的规模是n*d，而W的规模是d * q ,b的规模是1 * q\n",
    "- 令O = XW + b，Y = softmax(O)\n",
    "- 其中加法用到了广播机制，XW的规模是n*q，而b的规模是1*q,因此b会被补成n*q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵损失函数\n",
    "- 交叉熵(cross-entropy)是衡量两个概率分布差异的测量函数，即衡量我们的预测值y*(i)与真实值y(i)之间的差异\n",
    "    ![交叉熵损失函数](./image/3.4_2.png)\n",
    "- 我们分析一下yj(i)只有某个值是1,其他所有值都是0,假设yj(i)的第k个为1,其他都为0,那么化简后为:\n",
    "  ![1](./image/3.4_3.png)\n",
    "- 自变量在（0,1）之间，因此log是小于0。整个结果越大，说明自变量越小，自变量越小，说明越不准，自变量表示判断正确离散值的概率\n",
    "  ![2](./image/3.4_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "- softmax回归适用于分类问题。softmax运算输出类别的概率分布\n",
    "- softmax回归是单层神经网络，输出个数是分类的个数\n",
    "- 交叉熵适合衡量两个概率分布的差异"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习\n",
    "### 最大似然估计\n",
    "- 概率和统计\n",
    "    - 概率是由模型推结果，比如：我们想种西瓜（模型），我们需要确定土地的种类，种哪种西瓜，在什么温度下种，这些都是参数，我们想要推测我们种的瓜甜不甜，这就是预测;而统计则相反，我们有很多的数据，我们通过这些数据反推出我们的参数。所以概率和统计是一个相反的过程\n",
    "    - 对于P(x|θ)，若θ已知，x未知，我们把这个函数叫做概率函数(probability funciton);如果x已知，θ未知，我们把这个函数叫做似然函数(likehood funciton)\n",
    "    - 最大似然估计是求参数θ, 使似然函数P(x|θ)最大\n",
    "- 最小化交叉熵损失函数，是衡量两个概率分布的差异;最大似然估计是求出当前事件发生可能性最大的参数。从机器学习来看，本质都是为了拟合出最优的参数theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
